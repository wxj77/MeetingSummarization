{
    "query": "<s> summarize the discussion about the results from the experiments",
    "answer": "phd a : mike . mike - one ? professor d : we 're on ? yes , please . i mean , we 're testing noise robustness but let 's not get silly . ok , so , uh , you 've got some , uh , xerox things to pass out ? phd a : yeah , professor d : that are yeah . phd a : yeah . yeah , i 'm sorry for the table , but as it grows in size , uh , it . professor d : uh , so for th the last column we use our imagination . ok . phd b : ah , yeah . phd a : uh , yeah . phd b : uh , do you want @ @ . professor d : this one 's nice , though . this has nice big font . phd a : yeah . grad c : let 's see . yeah . chop ! professor d : yeah . when you get older you have these different perspectives . i mean , lowering the word hour rate is fine , but having big font ! phd a : next time we will put colors or something . professor d : that 's what 's yeah . it 's mostly big font . ok . phd a : ok , s so there is kind of summary of what has been done professor d : uh go ahead . phd a : it 's this . summary of experiments since , well , since last week professor d : oh . ok . phd a : and also since the we 've started to run work on this . um . so since last week we 've started to fill the column with um uh features w with nets trained on plp with on - line normalization but with delta also , because the column was not completely professor d : mm - hmm . mm - hmm . phd a : well , it 's still not completely filled , but we have more results to compare with network using without plp and finally , hhh , { comment } um ehhh { comment } pl - uh delta seems very important . uh i do n't know . if you take um , let 's say , anyway aurora - two - b , so , the next t the second , uh , part of the table , professor d : mm - hmm . phd a : uh when we use the large training set using french , spanish , and english , you have one hundred and six without delta and eighty - nine with the delta . professor d : a and again all of these numbers are with a hundred percent being , uh , the baseline performance , phd a : yeah , on the baseline , yeah . so professor d : but with a mel cepstra system going straight into the htk ? phd a : yeah . yeah . so now we see that the gap between the different training set is much uh uh much smaller um grad c : it 's out of the way . phd a : but , actually , um , for english training on timit is still better than the other languages . and mmm , yeah . and f also for italian , actually . if you take the second set of experiment for italian , so , the mismatched condition , professor d : mm - hmm . phd a : um when we use the training on timit so , it 's multi - english , we have a ninety - one number , professor d : mm - hmm . phd a : and training with other languages is a little bit worse . professor d : um oh , i see . down near the bottom of this sheet . uh , { comment } yes . phd a : yeah . and , yeah , and here the gap is still more important between using delta and not using delta . if y if i take the training s the large training set , it 's we have one hundred and seventy - two , and one hundred and four when we use delta . professor d : yeah . phd a : uh . even if the contexts used is quite the same , professor d : mm - hmm . phd a : because without delta we use seventeenths seventeen frames . uh . yeah , um , so the second point is that we have no single cross - language experiments , uh , that we did not have last week . uh , so this is training the net on french only , or on english only , and testing on italian . professor d : mm - hmm . phd a : and training the net on french only and spanish only and testing on , uh ti - digits . professor d : mm - hmm . phd a : and , fff { comment } um , yeah . what we see is that these nets are not as good , except for the multi - english , which is always one of the best . yeah , then we started to work on a large dat database containing , uh , sentences from the french , from the spanish , from the timit , from spine , uh from { comment } uh english digits , and from italian digits . so this is the another line another set of lines in the table . uh , @ @ with spine professor d : ah , yes . mm - hmm . phd a : and uh , actually we did this before knowing the result of all the data , uh , so we have to to redo the uh the experiment training the net with , uh plp , but with delta . but professor d : mm - hmm . phd a : um this this net performed quite well . well , it 's it 's better than the net using french , spanish , and english only . uh . so , uh , yeah . we have also started feature combination experiments . uh many experiments using features and net outputs together . and this is the results are on the other document . uh , we can discuss this after , perhaps well , just , @ @ . yeah , so basically there are four four kind of systems . the first one , yeah , is combining , um , two feature streams , uh using and each feature stream has its own mpl . so it 's the kind of similar to the tandem that was proposed for the first . the multi - stream tandem for the first proposal . the second is using features and klt transformed mlp outputs . and the third one is to u use a single klt trans transform features as well as mlp outputs . um , yeah . mmm . you know you can you can comment these results , phd b : yes , i can s i would like to say that , for example , um , mmm , if we does n't use the delta - delta , uh we have an improve when we use s some combination . but when phd a : yeah , we ju just to be clear , the numbers here are uh recognition accuracy . phd b : w yeah , this yeah , this number recognition acc phd a : so it 's not the again we switch to another phd b : yes , and the baseline the baseline have i is eighty - two . professor d : baseline is eighty - two . phd a : so it 's experiment only on the italian mismatched for the moment for this . professor d : uh , this is italian mismatched . phd b : yeah , by the moment . phd a : mm - hmm . phd b : and first in the experiment - one i i do i i use different mlp , professor d : mm - hmm . phd b : and is obviously that the multi - english mlp is the better . um . for the ne rest of experiment i use multi - english , only multi - english . and i try to combine different type of feature , but the result is that the msg - three feature does n't work for the italian database because never help to increase the accuracy . phd a : yeah , eh , actually , if w we look at the table , the huge table , um , we see that for ti - digits msg perform as well as the plp , professor d : mm - hmm . phd a : but this is not the case for italian what where the error rate is c is almost uh twice the error rate of plp . professor d : mm - hmm . phd a : so , um uh , well , i do n't think this is a bug but this this is something in probably in the msg um process that uh i do n't know what exactly . perhaps the fact that the the there 's no low - pass filter , well , or no pre - emp pre - emphasis filter and that there is some dc offset in the italian , or , well , something simple like that . but that we need to sort out if want to uh get improvement by combining plp and msg professor d : mm - hmm . phd a : because for the moment msg do does n't bring much information . professor d : mm - hmm . phd a : and as carmen said , if we combine the two , we have the result , basically , of plp . professor d : i um , the uh , baseline system when you said the baseline system was uh , uh eighty - two percent , that was trained on what and tested on what ? that was , uh italian mismatched d uh , uh , digits , uh , is the testing , phd b : yeah . professor d : and the training is italian digits ? phd b : yeah . professor d : so the `` mismatch `` just refers to the noise and and , uh microphone and so forth , phd a : yeah . phd b : yeah . professor d : right ? so , um did we have so would that then correspond to the first line here of where the training is is the uh italian digits ? phd b : the train the training of the htk ? professor d : the phd b : yes . ah yes ! this h yes . th - yes . professor d : yes . training of the net , phd b : yeah . professor d : yeah . so , um so what that says is that in a matched condition , we end up with a fair amount worse putting in the uh plp . now w would do we have a number , i suppose for the matched i i do n't mean matched , but uh use of italian training in italian digits for plp only ? phd b : uh yes ? phd a : uh yeah , so this is basically this is in the table . uh so the number is fifty - two , phd b : another table . phd a : uh professor d : fifty - two percent . phd a : fift - so no , it 's it 's the professor d : no , fifty - two percent of eighty - two ? phd a : of of of uh eighteen phd b : eighty . phd a : of eighteen . phd b : eighty . phd a : so it 's it 's error rate , basically . phd b : it 's plus six . phd a : it 's er error rate ratio . so professor d : oh this is accuracy ! phd a : uh , so we have nine nine let 's say ninety percent . phd b : yeah . professor d : oy ! { comment } ok . ninety . phd a : yeah . um { comment } which is uh { comment } what we have also if use plp and msg together , professor d : yeah . phd a : eighty - nine point seven . professor d : ok , so even just plp , uh , it is not , in the matched condition um i wonder if it 's a difference between plp and mel cepstra , or whether it 's that the net half , for some reason , is not helping . phd a : uh . p - plp and mel cepstra give the same same results . professor d : same result pretty much ? phd a : well , we have these results . i do n't know . it 's not do you have this result with plp alone , { comment } j fee feeding htk ? professor d : so , s phd a : that that 's what you mean ? phd b : yeah , phd a : just plp at the input of htk . phd b : yeah yeah yeah yeah , at the first and the yeah . phd a : yeah . so , plp professor d : eighty - eight point six . phd a : yeah . professor d : um , so adding msg phd a : um professor d : um well , but that 's yeah , that 's without the neural net , phd a : yeah , that 's without the neural net professor d : right ? phd a : and that 's the result basically that ogi has also with the mfcc with on - line normalization . professor d : but she had said eighty - two . phd a : this is the w well , but this is without on - line normalization . professor d : right ? oh , this the eighty - two . phd a : yeah . eighty - two is the it 's the aurora baseline , so mfcc . then we can use well , ogi , they use mfcc th the baseline mfcc plus on - line normalization professor d : oh , i 'm sorry , i k i keep getting confused because this is accuracy . phd a : yeah , sorry . yeah . phd b : yeah . professor d : ok . alright . phd a : yeah . professor d : alright . so this is i was thinking all this was worse . ok so this is all better phd b : yes , better . professor d : because eighty - nine is bigger than eighty - two . phd a : mm - hmm . phd b : yeah . professor d : ok . i 'm i 'm all better now . ok , go ahead . phd a : so what happ what happens is that when we apply on - line normalization we jump to almost ninety percent . professor d : yeah . mm - hmm . phd a : uh , when we apply a neural network , is the same . we j jump to ninety percent . phd b : nnn , we do n't know exactly . professor d : yeah . phd a : and and um whatever the normalization , actually . if we use n neural network , even if the features are not correctly normalized , we jump to ninety percent . so professor d : so we go from eighty - si eighty - eight point six to to ninety , or something . phd a : well , ninety no , i i mean ninety it 's around eighty - nine , ninety , eighty - eight . professor d : eighty - nine . phd a : well , there are minor minor differences . phd b : yeah . professor d : and then adding the msg does nothing , basically . yeah . ok . phd a : uh for italian , yeah . professor d : for this case , right ? alright . so , um so actually , the answer for experiments with one is that adding msg , if you uh does not help in that case . phd a : mm - hmm . professor d : um phd a : but w yeah . professor d : the other ones , we 'd have to look at it , but and the multi - english , does uh so if we think of this in error rates , we start off with , uh eighteen percent error rate , roughly . phd a : mm - hmm . professor d : um and we uh almost , uh cut that in half by um putting in the on - line normalization and the neural net . and the msg does n't however particularly affect things . and we cut off , i guess about twenty - five percent of the error . uh no , not quite that , is it . uh , two point six out of eighteen . about , um sixteen percent or something of the error , um , if we use multi - english instead of the matching condition . phd a : mm - hmm . yeah . professor d : not matching condition , but uh , the uh , italian training . phd a : mm - hmm . phd b : yeah . we select these these these tasks because it 's the more difficult . professor d : yes , good . ok ? so then you 're assuming multi - english is closer to the kind of thing that you could use since you 're not gon na have matching , uh , data for the uh for the new for the other languages and so forth . um , one qu thing is that , uh i think i asked you this before , but i wan na double check . when you say `` me `` in these other tests , that 's the multi - english , phd a : that 's it 's a part it 's professor d : but it is not all of the multi - english , right ? it is some piece of part of it . phd a : or , one million frames . professor d : and the multi - english is how much ? phd b : you have here the information . phd a : it 's one million and a half . yeah . professor d : oh , so you used almost all you used two thirds of it , phd a : yeah . professor d : you think . so , it it 's still it hurts you seems to hurt you a fair amount to add in this french and spanish . phd b : yeah . professor d : i wonder why yeah . uh . grad c : well stephane was saying that they were n't hand - labeled , phd a : yeah , it 's phd b : yeah . phd a : yeah . grad c : the french and the spanish . phd b : the spanish . maybe for that . professor d : it 's still ok . alright , go ahead . and then then phd b : um . mmm , with the experiment type - two , i first i tried to to combine , nnn , some feature from the mlp and other feature another feature . professor d : mm - hmm . phd b : and we s we can first the feature are without delta and delta - delta , and we can see that in the situation , uh , the msg - three , the same help nothing . professor d : mm - hmm . phd b : and then i do the same but with the delta and delta - delta plp delta and delta - delta . and they all p but they all put off the mlp is it without delta and delta - delta . and we have a l little bit less result than the the the baseline plp with delta and delta - delta . professor d : mm - hmm . phd b : maybe if when we have the new the new neural network trained with plp delta and delta - delta , maybe the final result must be better . i do n't know . phd a : actually , just to be some more phd b : uh phd a : do this number , this eighty - seven point one number , has to be compared with the professor d : yes , yeah , i mean it ca n't be compared with the other phd a : which number ? professor d : cuz this is , uh with multi - english , uh , training . phd b : mm - hmm . professor d : so you have to compare it with the one over that you 've got in a box , which is that , uh the eighty - four point six . phd b : mm - hmm . professor d : right ? so phd a : yeah , but i mean in this case for the eighty - seven point one we used mlp outputs for the plp net professor d : yeah . phd a : and straight features with delta - delta . and straight features with delta - delta gives you what 's on the first sheet . phd b : mm - hmm . professor d : yeah . not t not phd a : it 's eight eighty - eight point six . professor d : tr no . no . no . not trained with multi - english . phd a : uh , yeah , but th this is the second configuration . phd b : no , but they they feature @ @ without phd a : so we use feature out uh , net outputs together with features . so yeah , this is not perhaps not clear here but in this table , the first column is for mlp and the second for the features . professor d : eh . { comment } oh , i see . ah . so you 're saying w so asking the question , `` what what has adding the mlp done to improve over the , phd a : so , just yeah so , actually it it it decreased the the accuracy . professor d : uh phd b : yeah . phd a : because we have eighty - eight point six . professor d : uh - huh . phd a : and even the mlp alone what gives the mlp alone ? multi - english plp . oh no , it gives eighty - three point six . so we have our eighty - three point six and now eighty - eighty point six , phd b : but phd a : that gives eighty - seven point one . professor d : mm - hmm . eighty - s i thought it was eighty oh , ok , eighty - three point six and eighty eighty - eight point six . phd a : eighty - three point six . eighty is th is that right ? yeah ? phd b : yeah . but i do n't know but maybe if we have the neural network trained with the plp delta and delta - delta , maybe tha this can help . phd a : perhaps , yeah . professor d : well , that 's that 's one thing , but see the other thing is that , um , i mean it 's good to take the difficult case , but let 's let 's consider what that means . what what we 're saying is that one o one of the things that i mean my interpretation of your your s original suggestion is something like this , as motivation . when we train on data that is in one sense or another , similar to the testing data , then we get a win by having discriminant training . phd a : mm - hmm . professor d : when we train on something that 's quite different , we have a potential to have some problems . phd a : mm - hmm . professor d : and , um , if we get something that helps us when it 's somewhat similar , and does n't hurt us too much when it when it 's quite different , that 's maybe not so bad . phd a : yeah . mmm . professor d : so the question is , if you took the same combination , and you tried it out on , uh on say digits , phd a : on ti - digits ? ok . professor d : you know , d was that experiment done ? phd a : no , not yet . professor d : yeah , ok . uh , then does that , eh you know maybe with similar noise conditions and so forth , { comment } does it does it then look much better ? phd a : mm - hmm . professor d : and so what is the range over these different kinds of uh of tests ? so , an anyway . ok , go ahead . phd a : yeah . phd b : and , with this type of configuration which i do on experiment using the new neural net with name broad klatt s twenty - seven , uh , d i have found more or less the same result . professor d : mm - hmm . phd a : so , it 's slightly better , phd b : little bit better ? phd a : yeah . professor d : slightly better . phd a : yeah . phd b : slightly bet better . yes , is better . professor d : and and you know again maybe if you use the , uh , delta there , uh , you would bring it up to where it was , uh you know at least about the same for a difficult case . phd b : yeah , maybe . maybe . maybe . phd a : yeah . phd b : oh , yeah . phd a : yeah . well , so perhaps let 's let 's jump at the last experiment . phd b : oh , yeah . phd a : it 's either less information from the neural network if we use only the silence output . professor d : mm - hmm . phd a : it 's again better . so it 's eighty - nine point point one . phd b : yeah , professor d : mm - hmm . phd b : and we have only forty forty feature because in this situation we have one hundred and three feature . professor d : yeah . phd b : yeah . and then w with the first configuration , i f i am found that work , uh , does n't work professor d : yeah ."
}