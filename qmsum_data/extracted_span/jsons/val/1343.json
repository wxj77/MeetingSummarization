{
    "query": "<s> how did phd b approach the back-end of the aurora recognizer ?",
    "answer": "phd b : yes , i 'm playing . um so i wanted to do this experiment to see um uh what happens if we try to uh improve the performance of the back - end recognizer for the aurora task and see how that affects things . and so i had this um i think i sent around last week a this plan i had for an experiment , this matrix where i would take the um the original um the original system . so there 's the original system trained on the mel cepstral features and then com and then uh optimize the b htk system and run that again . so look at the difference there and then uh do the same thing for the icsi - ogi front - end . professor a : what which test set was this ? phd b : this is that i looked at ? professor a : mm - hmm . phd b : uh i 'm looking at the italian right now . professor a : mm - hmm . phd b : so as far as i 've gotten is i 've uh been able to go through from beginning to end the um full htk system for the italian data and got the same results that um that uh stephane had . so um i started looking to and now i 'm i 'm sort of lookin at the point where i wan na know what should i change in the htk back - end in order to try to uh to improve it . so . one of the first things i thought of was the fact that they use the same number of states for all of the models professor a : mm - hmm . phd b : and so i went on - line and i uh found a pronunciation dictionary for italian digits professor a : mm - hmm . phd b : and just looked at , you know , the number of phones in each one of the digits . um you know , sort of the canonical way of setting up a an hmm system is that you use um three states per phone and um so then the the total number of states for a word would just be , you know , the number of phones times three . and so when i did that for the italian digits , i got a number of states , ranging on the low end from nine to the high end , eighteen . um . now you have to really add two to that because in htk there 's an initial null and a final null so when they use uh models that have eighteen states , there 're really sixteen states . they 've got those initial and final null states . and so um their guess of eighteen states seems to be pretty well matched to the two longest words of the italian digits , the four and five which um , according to my , you know , sort of off the cuff calculation , should have eighteen states each . professor a : mm - hmm . phd b : and so they had sixteen . so that 's pretty close . um but for the most of the words are sh much shorter . so the majority of them wan na have nine states . and so theirs are s sort of twice as long . so my guess uh and then if you i i printed out a confusion matrix um uh for the well - matched case , and it turns out that the longest words are actually the ones that do the best . so my guess about what 's happening is that you know , if you assume a fixed the same amount of training data for each of these digits and a fixed length model for all of them but the actual words for some of them are half as long you really um have , you know , half as much training data for those models . because if you have a long word and you 're training it to eighteen states , uh you 've got you know , you 've got the same number of gaussians , you 've got ta train in each case , professor a : mm - hmm ."
}