{
    "query": "<s> summarize the discussion on improving the back recognizer for the aurora task",
    "answer": "professor a : oh . you 're playing ? phd b : i um huh ? professor a : you 're playing ? phd b : yes , i 'm playing . um so i wanted to do this experiment to see um uh what happens if we try to uh improve the performance of the back - end recognizer for the aurora task and see how that affects things . and so i had this um i think i sent around last week a this plan i had for an experiment , this matrix where i would take the um the original um the original system . so there 's the original system trained on the mel cepstral features and then com and then uh optimize the b htk system and run that again . so look at the difference there and then uh do the same thing for the icsi - ogi front - end . professor a : what which test set was this ? phd b : this is that i looked at ? professor a : mm - hmm . phd b : uh i 'm looking at the italian right now . professor a : mm - hmm . phd b : so as far as i 've gotten is i 've uh been able to go through from beginning to end the um full htk system for the italian data and got the same results that um that uh stephane had . so um i started looking to and now i 'm i 'm sort of lookin at the point where i wan na know what should i change in the htk back - end in order to try to uh to improve it . so . one of the first things i thought of was the fact that they use the same number of states for all of the models professor a : mm - hmm . phd b : and so i went on - line and i uh found a pronunciation dictionary for italian digits professor a : mm - hmm . phd b : and just looked at , you know , the number of phones in each one of the digits . um you know , sort of the canonical way of setting up a an hmm system is that you use um three states per phone and um so then the the total number of states for a word would just be , you know , the number of phones times three . and so when i did that for the italian digits , i got a number of states , ranging on the low end from nine to the high end , eighteen . um . now you have to really add two to that because in htk there 's an initial null and a final null so when they use uh models that have eighteen states , there 're really sixteen states . they 've got those initial and final null states . and so um their guess of eighteen states seems to be pretty well matched to the two longest words of the italian digits , the four and five which um , according to my , you know , sort of off the cuff calculation , should have eighteen states each . professor a : mm - hmm . phd b : and so they had sixteen . so that 's pretty close . um but for the most of the words are sh much shorter . so the majority of them wan na have nine states . and so theirs are s sort of twice as long . so my guess uh and then if you i i printed out a confusion matrix um uh for the well - matched case , and it turns out that the longest words are actually the ones that do the best . so my guess about what 's happening is that you know , if you assume a fixed the same amount of training data for each of these digits and a fixed length model for all of them but the actual words for some of them are half as long you really um have , you know , half as much training data for those models . because if you have a long word and you 're training it to eighteen states , uh you 've got you know , you 've got the same number of gaussians , you 've got ta train in each case , professor a : mm - hmm . phd b : but for the shorter words , you know , the total number of frames is actually half as many . professor a : mm - hmm . phd b : so it could be that , you know , for the short words there 's because you have so many states , you just do n't have enough data to train all those gaussians . so um i 'm going to try to um create more word - specific um uh prototype h m ms to start training from . professor a : yeah , i mean , it 's not at all uncommon you do worse on long word on short words than long words anyway just because you 're accumulating more evidence for the for the longer word , phd b : mm - hmm . yeah so i 'll i 'll , the next experiment i 'm gon na try is to just um you know create uh models that seem to be more w matched to my guess about how long they should be . professor a : mm - hmm . phd b : and as part of that um i wanted to see sort of how the um how these models were coming out , you know , what w when we train up uh th you know , the model for `` one `` , which wants to have nine states , you know , what is the uh what do the transition probabilities look like in the self - loops , { comment } look like in in those models ? and so i talked to andreas and he explained to me how you can calculate the expected duration of an hmm just by looking at the transition matrix professor a : mm - hmm . phd b : and so i wrote a little matlab script that calculates that and so i 'm gon na sort of print those out for each of the words to see what 's happening , you know , how these models are training up , professor a : mm - hmm . mm - hmm . phd b : you know , the long ones versus the short ones . i d i did quickly , i did the silence model and and um that 's coming out with about one point two seconds as its average duration and the silence model 's the one that 's used at the beginning and the end of each of the string of digits . professor a : wow . lots of silence . phd b : yeah , yeah . and so the s p model , which is what they put in between digits , i i have n't calculated that for that one yet , but um . so they basically their their model for a whole digit string is silence digit , sp , digit , sp blah - blah - blah and then silence at the end . and so . professor a : are the sp 's optional ? i mean skip them ? phd b : i have to look at that , but i 'm not sure that they are . now the one thing about the s p model is really it only has a single s emitting state to it . professor a : mm - hmm . phd b : so if it 's not optional , you know , it 's it 's not gon na hurt a whole lot professor a : i see . phd b : and it 's tied to the center state of the silence model so it 's not its own um it does n't require its own training data , professor a : mm - hmm . phd b : it just shares that state . professor a : mm - hmm . phd b : so it , i mean , it 's pretty good the way that they have it set up , but um i so i wan na play with that a little bit more . i 'm curious about looking at , you know how these models have trained and looking at the expected durations of the models and i wan na compare that in the the well - matched case f to the unmatched case , and see if you can get an idea of just from looking at the durations of these models , you know , what what 's happening . professor a : yeah , i mean , i think that uh , as much as you can , it 's good to d sort of not do anything really tricky . phd b : mm - hmm . professor a : not do anything that 's really finely tuned , but just sort of eh you know you t you i z phd b : yeah . professor a : the premise is kind of you have a a good person look at this for a few weeks and what do you come up with ? phd b : mm - hmm . mm - hmm . professor a : and uh phd b : and hynek , when i wa told him about this , he had an interesting point , and that was th um the the final models that they end up training up have i think probably something on the order of six gaussians per state . so they 're fairly , you know , hefty models . and hynek was saying that well , probably in a real application , you would n't have enough compute to handle models that are very big or complicated . so in fact what we may want are simpler models . professor a : could be . phd b : and compare how they perform to that . but you know , it depends on what the actual application is and it 's really hard to know what your limits are in terms of how many gaussians you can have . professor a : right . and that , i mean , at the moment that 's not the limitation , so . phd b : mm - hmm . professor a : i mean , i i i what i thought you were gon na say i but which i was thinking was um where did six come from ? probably came from the same place eighteen came from . you know , so . phd b : yeah . right . professor a : uh that 's another parameter , right ? that that maybe , you know , uh you really want three or nine or phd b : yeah , yeah . well one thing i mean , if i if if i start um reducing the number of states for some of these shorter models that 's gon na reduce the total number of gaussians . professor a : right . phd b : so in a sense it 'll be a simpler system . professor a : yeah . yeah . but i think right now again the idea is doing just very simple things phd b : yeah . professor a : how much better can you make it ? and um since they 're only simple things there 's nothing that you 're gon na do that is going to blow up the amount of computation phd b : mm - hmm . right . right . professor a : if you found that nine was better than six that would be o k , i think , actually . phd b : mm - hmm . professor a : does n't have to go down . phd b : yeah . i really was n't even gon na play with that part of the system yet , professor a : mm - hmm , ok . phd b : i was just gon na change the the t professor a : yeah , just work with the models , yeah . phd b : yeah , just look at the length of the models and just see what happens ."
}