{
    "query": "<s> summarize the meeting",
    "answer": "professor b : i think for two years we were two months , uh , away from being done . phd a : and what was that , morgan ? what project ? professor b : uh , the , uh , torrent chip . yeah . we were two we were phd c : yeah . professor b : uh , uh , we went through it jim and i went through old emails at one point and and for two years there was this thing saying , yeah , we 're we 're two months away from being done . it was very very believable schedules , too . i mean , we went through and with the schedules and we phd a : it was true for two years . professor b : yeah . oh , yeah . it was very true . phd a : so , should we just do the same kind of deal where we go around and do , uh , status report kind of things ? ok . and i guess when sunil gets here he can do his last or something . so . professor b : yeah . so we probably should wait for him to come before we do his . phd c : mm - hmm . phd a : ok . that 's a good idea . professor b : yeah . yeah . phd a : any objection ? do y ok , m professor b : all in favor phd a : do you want to start , morgan ? do you have anything , or ? professor b : uh , i do n't do anything . i no , i mean , i i 'm involved in discussions with with people about what they 're doing , but i think they 're since they 're here , they can talk about it themselves . grad f : ok . so should i go so that , uh , phd a : yeah . why do n't you go ahead , barry ? grad f : you 're gon na talk about aurora stuff , per se ? ok . um . well , this past week i 've just been , uh , getting down and dirty into writing my my proposal . so , um mmm . i just finished a section on , uh on talking about these intermediate categories that i want to classify , um , as a as a middle step . and , um , i hope to hope to get this , um a full rough draft done by , uh , monday so i can give it to morgan . phd a : when is your , uh , meeting ? grad f : um , my meeting phd a : yeah . grad f : with , uh ? oh , oh , you mean the the quals . phd a : the quals . yeah . grad f : uh , the quals are happening in july twenty - fifth . phd a : oh . soon . grad f : yeah . phd a : uh - huh . grad f : d - day . phd a : yeah . grad f : uh - huh . phd a : so , is the idea you 're going to do this paper and then you pass it out to everybody ahead of time and ? grad f : right , right . so , y you write up a proposal , and give it to people ahead of time , and you have a short presentation . and , um , and then , um then everybody asks you questions . yeah . phd a : i remember now . grad f : yep . so , um . phd a : have you d ? i was just gon na ask , do you want to say any a little bit about it , or ? mmm . grad f : oh . uh , a little bit about ? phd a : wh - what you 're what you 're gon na you said you were talking about the , uh , particular features that you were looking at , grad f : oh , the the phd a : or grad f : right . well , i was , um , i think one of the perplexing problems is , um , for a while i was thinking that i had to come up with a complete set of intermediate features in intermediate categories to to classify right away . but what i 'm thinking now is , i would start with with a reasonable set . something something like , um , um like , uh , re regular phonetic features , just to just to start off that way . and do some phone recognition . um , build a system that , uh , classifies these , um these feat uh , these intermediate categories using , uh , multi - band techniques . combine them and do phon phoneme recognition . look at then i would look at the errors produced in the phoneme recognition and say , ok , well , i could probably reduce the errors if i included this extra feature or this extra intermediate category . that would that would reduce certain confusions over other confusions . and then and then reiterate . um , build the intermediate classifiers . uh , do phoneme recognition . look at the errors . and then postulate new or remove , um , intermediate categories . and then do it again . phd a : so you 're gon na use timit ? grad f : um , for that for that part of the the process , yeah , i would use timit . phd a : mm - hmm . grad f : and , um , then after after , uh , um , doing timit . right ? phd a : mm - hmm . grad f : um , that 's that 's , um that 's just the ph the phone recognition task . phd a : yeah . grad f : uh , i wanted to take a look at , um , things that i could model within word . so , i would mov i would then shift the focus to , um , something like schw - switchboard , uh , where i 'd i would be able to , um to model , um , intermediate categories that span across phonemes , phd a : mm - hmm . grad f : not just within the phonemes , themselves , um , and then do the same process there , um , on on a large vocabulary task like switchboard . uh , and for that for that part i would i 'd use the sri recognizer since it 's already set up for for switchboard . and i 'd run some some sort of tandem - style processing with , uh , my intermediate classifiers . phd a : oh . so that 's why you were interested in getting your own features into the sri files . grad f : yeah . that 's why i i was asking about that . phd a : yeah . yeah . grad f : yeah . um , and i guess that 's that 's it . any any questions ? phd a : sounds good . so you just have a few more weeks , huh ? grad f : um , yeah . a few more . phd a : it 's about a month from now ? grad f : it 's a it 's a month and and a week . phd a : yeah . grad f : yeah . phd a : so , uh , you want to go next , dave ? and we 'll do grad e : oh . ok , sure . so , um , last week i finally got results from the sri system about this mean subtraction approach . and , um , we we got an improvement , uh , in word error rate , training on the ti - digits data set and testing on meeting recorder digits of , um , six percent to four point five percent , um , on the n on the far - mike data using pzm f , but , um , the near - mike performance worsened , um , from one point two percent to two point four percent . and , um , wh why would that be , um , considering that we actually got an improvement in near - mike performance using htk ? and so , uh , with some input from , uh , andreas , i have a theory in two parts . um , first of all htk sorry , sr - the sri system is doing channel adaptation , and so htk was n't . um , so this , um this mean subtraction approach will do a kind of channel normalization and so that might have given the htk use of it a boost that would n't have been applied in the sri case . and also , um , the andreas pointed out the sri system is using more parameters . it 's got finer - grained acoustic models . so those finer - grained acoustic models could be more sensitive to the artifacts in the re - synthesized audio . um . and me and barry were listening to the re - synthesized audio and sometimes it seems like you get of a bit of an echo of speech in the background . and so that seems like it could be difficult for training , cuz you could have different phones lined up with a different foreground phone , um , depending on the timing of the echo . so , um , i 'm gon na try training on a larger data set , and then , eh , the system will have seen more examples o of these artifacts and hopefully will be more robust to them . so i 'm planning to use the macrophone set of , um , read speech , and , um hmm . professor b : i had another thought just now , which is , uh , remember we were talking before about we were talking in our meeting about , uh , this stuff that some of the other stuff that avendano did , where they were , um , getting rid of low - energy sections ? um , uh , if you if you did a high - pass filtering , as hirsch did in late eighties to reduce some of the effects of reverberation , uh , uh , avendano and hermansky were arguing that , uh , perhaps one of the reasons for that working was ma may not have even been the filtering so much but the fact that when you filter a an all - positive power spectrum you get some negative values , and you got ta figure out what to do with them if you 're gon na continue treating this as a power spectrum . so , what what hirsch did was , uh , set them to zero set the negative values to zero . so if you imagine a a waveform that 's all positive , which is the time trajectory of energy , um , and , uh , shifting it downwards , and then getting rid of the negative parts , that 's essentially throwing away the low - energy things . and it 's the low - energy parts of the speech where the reverberation is most audible . you know , you have the reverberation from higher - energy things showing up in so in this case you have some artificially imposed reverberation - like thing . i mean , you 're getting rid of some of the other effects of reverberation , but because you have these non - causal windows , you 're getting these funny things coming in , uh , at n and , um , what if you did ? i mean , there 's nothing to say that the the processing for this re - synthesis has to be restricted to trying to get it back to the original , according to some equation . i mean , you also could , uh , just try to make it nicer . grad e : uh - huh . professor b : and one of the things you could do is , you could do some sort of vad - like thing grad e : mm - hmm . professor b : and you actually could take very low - energy sections and set them to some some , uh , very low or or near zero value . i mean , uh , i 'm just saying if in fact it turns out that that these echoes that you 're hearing are , uh grad e : uh - huh . professor b : or pre - echoes , whichever they are are are , uh , part of what 's causing the problem , you actually could get rid of them . grad e : uh - huh . professor b : be pretty simple . i mean , you do it in a pretty conservative way so that if you made a mistake you were more likely to keep in an echo than to throw out speech . phd g : um , what is the reverberation time like there ? grad e : in thi in this room ? uh phd g : on , uh , the the one what the s in the speech that you are you are using like ? grad e : y yeah . i i i i do n't know . professor b : so , it 's this room . phd g : it 's , uh professor b : it 's it 's this room . phd g : oh , this room ? professor b : so so it 's these are just microphone this micro close microphone and a distant microphone , he 's doing these different tests on . uh , we should do a measurement in here . i g think we never have . i think it 's i would guess , uh , point seven , point eight seconds f uh , r t something like that ? but it 's you know , it 's this room . phd g : mm - hmm . ok . mm - hmm . professor b : uh . but the other thing is , he 's putting in w i was using the word `` reverberation `` in two ways . he 's also putting in , uh , a he 's taking out some reverberation , but he 's putting in something , because he has averages over multiple windows stretching out to twelve seconds , which are then being subtracted from the speech . and since , you know , what you subtract , sometimes you 'll be you 'll be subtracting from some larger number and sometimes you wo n't . and phd g : mm - hmm . mm - hmm . professor b : so you can end up with some components in it that are affected by things that are seconds away . uh , and if it 's a low energy compo portion , you might actually hear some funny things . phd g : yeah . grad e : o o one thing , um , i noticed is that , um , the mean subtraction seems to make the pzm signals louder after they 've been re - synthesized . so i was wondering , is it possible that one reason it helped with the aurora baseline system is just as a kind of gain control ? cuz some of the pzm signals sound pretty quiet if you do n't amplify them . phd c : mm - hmm . i do n't see why why your signal is louder after processing , because yo grad e : yeah . i do n't know why - y , uh , either . phd c : yeah . professor b : i do n't think just multiplying the signal by two would have any effect . phd c : mm - hmm . grad e : oh , ok . professor b : yeah . i mean , i think if you really have louder signals , what you mean is that you have better signal - to - noise ratio . phd c : well , well professor b : so if what you 're doing is improving the signal - to - noise ratio , then it would be better . phd c : mm - hmm . professor b : but just it being bigger if with the same signal - to - noise ratio grad e : it w i i it would n't affect things . phd c : yeah . well , the system is use the absolute energy , so it 's a little bit dependent on on the signal level . but , not so much , i guess . professor b : well , yeah . but it 's trained and tested on the same thing . so if the if the if you change in both training and test , the absolute level by a factor of two , it will n have no effect . phd c : mm - hmm . yeah . phd a : did you add this data to the training set , for the aurora ? or you just tested on this ? grad e : uh um . did i w what ? phd a : well , morgan was just saying that , uh , as long as you do it in both training and testing , it should n't have any effect . grad e : sorry ? yeah . phd a : but i i was sort of under the impression that you just tested with this data . grad e : i i b phd a : you did n't train it also . grad e : i right . i trained on clean ti - digits . i i did the mean subtraction on clean ti - digits . but i did n't i 'm not sure if it made the clean ti ti - digits any louder . professor b : oh , i see . grad e : i only remember noticing it made the , um , pzm signal louder . professor b : ok . well , i do n't understand then . yeah . grad e : huh . i do n't know . if it 's if it 's like , if it 's trying to find a a reverberation filter , it could be that this reverberation filter is making things quieter . and then if you take it out that taking it out makes things louder . i mean . professor b : uh , no . i mean , uh , there 's there 's nothing inherent about removing if you 're really removing , grad e : nuh - huh . professor b : uh , r uh , then i do n't see how that would make it louder . grad e : the mean . ok . yeah , i see . professor b : so it might be just some grad e : yeah . ok . so i should maybe listen to that stuff again . professor b : yeah . it might just be some artifact of the processing that that , uh , if you 're uh , yeah . i do n't know . grad e : oh . ok . phd a : i wonder if there could be something like , uh for s for the pzm data , uh , you know , if occasionally , uh , somebody hits the table or something , you could get a spike . uh . i 'm just wondering if there 's something about the , um you know , doing the mean normalization where , uh , it it could cause you to have better signal - to - noise ratio . um . professor b : well , you know , there is this . wait a minute . it it i maybe i if , um subtracting the the mean log spectrum is is is like dividing by the spectrum . so , depending what you divide by , if your if s your estimate is off and sometimes you 're you 're you 're getting a small number , you could make it bigger . phd a : mm - hmm . grad e : mm - hmm . professor b : so , it 's it 's just a a question of there 's it it could be that there 's some normalization that 's missing , or something to make it grad e : mm - hmm . professor b : uh , y you 'd think it should n't be larger , but maybe in practice it is . that 's something to think about . i do n't know . phd c : i had a question about the system the sri system . so , you trained it on ti - digits ? but except this , it 's exactly the same system as the one that was tested before and that was trained on macrophone . right ? so on ti - digits it gives you one point two percent error rate and on macrophone it 's still o point eight . uh , but is it exactly the same system ? grad e : uh . i think so . if you 're talking about the macrophone results that andreas had about , um , a week and a half ago , i think it 's the same system . phd c : mm - hmm . so you use vtl - uh , vocal tract length normalization and , um , like mllr transformations also , grad e : mm - hmm . phd c : and professor b : i 'm sorry , was his point eight percent , er , a a result on testing on macrophone or or training ? phd c : all that stuff . grad e : that 's phd c : it was training on macrophone and testing yeah , on on meeting digits . professor b : oh . so that was done already . so we were uh , and it 's point eight ? ok . phd c : mm - hmm . yeah . i i 've just been text { comment } testing the new aurora front - end with well , aurora system actually so front - end and htk , um , acoustic models on the meeting digits and it 's a little bit better than the previous system . we have i have two point seven percent error rate . and before with the system that was proposed , it 's what ? it was three point nine . so . professor b : oh , that 's a lot better . phd c : we are getting better . professor b : so , what w ? phd c : and phd g : with the with the htk back - end ? what we have for aurora ? phd c : yeah . two point seven . phd g : i know in the meeting , like phd c : on the meeting we have two point seven . phd g : right . oh . grad f : that 's with the new iir filters ? phd c : uh . yeah , yeah . so , yeah , we have the new lda filters , and i think , maybe i did n't look , but one thing that makes a difference is this dc offset compensation . uh , eh do y did you have a look at at the meet uh , meeting digits , if they have a dc component , or ? grad e : i i did n't . no . phd g : no . the dc component could be negligible . i mean , if you are recording it through a mike . i mean , any all of the mikes have the dc removal some capacitor sitting right in that bias it . professor b : yeah . but this uh , uh , uh , no . because , uh , there 's a sample and hold in the a - tod . and these period these typically do have a dc offset . phd g : oh , ok . professor b : and and they can be surprisingly large . it depends on the electronics . phd g : oh , so it is the digital ok . it 's the a - tod that introduces the dc in . professor b : yeah . the microphone is n't gon na pass any dc . phd g : yeah . yeah . yeah . professor b : but but , typi you know , unless actually , there are instrumentation mikes that that do pass go down to dc . but but , phd g : mm - hmm . professor b : uh , no , it 's the electronics . and they and phd g : mm - hmm . professor b : then there 's amplification afterwards . and you can get , i think it was i think it was in the wall street journal data that that i ca n't remember , one of the darpa things . there was this big dc - dc offset phd a : mm - hmm . professor b : we did n't we did n't know about for a while , while we were messing with it . and we were getting these terrible results . and then we were talking to somebody and they said , `` oh , yeah . did n't you know ? everybody knows that . there 's all this dc offset in th `` so , yes . you can have dc offset in the data . phd g : oh , ok . professor b : yeah . phd a : so was that was that everything , dave ? grad e : oh . and i also , um , did some experiments about normalizing the phase . um . so i c i came up with a web page that people can take a look at . and , um , the interesting thing that i tried was , um , adam and morgan had this idea , um , since my original attempts to , um , take the mean of the phase spectra over time and normalize using that , by subtracting that off , did n't work . um , so , well , that we thought that might be due to , um , problems with , um , the arithmetic of phases . they they add in this modulo two pi way and , um , there 's reason to believe that that approach of taking the mean of the phase spectrum was n't really mathematically correct . so , what i did instead is i took the mean of the fft spectrum without taking the log or anything , and then i took the phase of that , and i subtracted that phase off to normalize . but that , um , did n't work either . professor b : see , we have a different interpretation of this . he says it does n't work . i said , i think it works magnificently , but just not for the task we intended . uh , it gets rid of the speech . phd a : what does it leave ? grad f : uh , gets rid of the speech . professor b : uh , it leaves you know , it leaves the junk . i mean , i i think it 's it 's tremendous . grad f : oh , wow . professor b : you see , all he has to do is go back and reverse what he did before , and he 's really got something . phd a : well , could you take what was left over and then subtract that ? professor b : ex - exactly . yeah , you got it . grad f : yeah . phd g : yeah . professor b : so , it 's it 's a general rule . phd g : oh , it 's professor b : just listen very carefully to what i say and do the opposite . including what i just said . grad e : and , yeah , that 's everything . phd a : all set ? do you want to go , stephane ? phd c : um . yeah . maybe , concerning these d still , these meeting digits . i 'm more interested in trying to figure out what 's still the difference between the sri system and the aurora system . and um . yeah . so , i think i will maybe train , like , gender - dependent models , because this is also one big difference between the two systems . um , the other differences were the fact that maybe the acoustic models of the sri are more sri system are more complex . but , uh , chuck , you did some experiments with this and phd a : it did n't seem to help in the htk system . phd c : it was hard t to to have some exper some improvement with this . um . professor b : well , it sounds like they also have he he 's saying they have all these , uh , uh , different kinds of adaptation . phd c : mm - hmm . professor b : you know , they have channel adaptation . they have speaker adaptation . phd c : yeah . right . phd a : well , there 's also the normalization . professor b : yeah . yeah . phd c : yeah . grad f : yeah . phd a : like they do , um i 'm not sure how they would do it when they 're working with the digits , phd c : the vocal tr phd a : but , like , in the switchboard data , there 's , um conversation - side normalization for the non - c - zero components , phd c : yeah . yeah . this is another difference . their normalization works like on on the utterance levels . phd a : mm - hmm . phd c : but we have to do it we have a system that does it on - line . phd a : right . phd c : so , it might be it might be better with it might be worse if the channel is constant , phd a : yeah . phd c : or nnn . phd g : and the acoustic models are like - k triphone models or or is it the whole word ? phd c : sri it 's it 's tr phd g : yeah . phd c : yeah . i guess it 's triphones . phd g : it 's triphone . professor b : i think it 's probably more than that . i mean , so they they have i i thin think they use these , uh , uh , genone things . so there 's there 's these kind of , uh , uh , pooled models and and they can go out to all sorts of dependencies . phd g : oh . it 's like the tied state . phd a : mm - hmm . professor b : they have tied states and i think i i i do n't real i 'm talk i 'm just guessing here . but i think i think they they do n't just have triphones . i think they have a range of of , uh , dependencies . phd c : mm - hmm . phd g : mm - hmm . phd c : mm - hmm . and yeah . well . um . well , the first thing i that i want to do is just maybe these gender things . uh . and maybe see with andreas if well , i i do n't know how much it helps , what 's the model . phd a : so so the n stuff on the numbers you got , the two point seven , is that using the same training data that the sri system used and got one point two ? phd c : that 's right . so it 's the clean ti - digits training set . phd a : so exact same training data ? phd c : right . mm - hmm . i guess you used the clean training set . grad e : right . phd c : mm - hmm . grad e : for with the sri system phd c : well . grad e : you know , the the aurora baseline is set up with these , um this version of the clean training set that 's been filtered with this g - seven - one - two filter , and , um , to train the sri system on digits s - andreas used the original ti - digits , um , under u doctor - speech data ti - digits , which do n't have this filter . but i do n't think there 's any other difference . phd c : mm - hmm . mm - hmm . yeah . professor b : so is that ? uh , are are these results comparable ? so you you were getting with the , uh , aurora baseline something like two point four percent on clean ti - digits , when , uh , training the sri system with clean tr digits { comment } ti - digits . right ? and grad e : um . uh - huh . professor b : yeah . and , so , is your two point seven comparable , where you 're , uh , uh , using , uh , the submitted system ? phd c : yeah . i think so . yeah . professor b : so it 's about the same , phd c : mm - hmm . professor b : maybe a little worse . grad e : w w it was one one point two with the sri system , professor b : i 'm sorry . phd c : yeah . grad e : i phd c : the complete sri system is one point two . professor b : you you were htk . phd c : yeah . professor b : right ? ok . that 's right . so phd c : mm - hmm . professor b : ok , so the comparable number then , uh for what you were talking about then , since it was htk , would be the um , two point f phd c : it was four point something . right ? the htk system with , uh , b professor b : oh , right , right , right , right . phd c : mfcc features grad e : do you mean the b ? the baseline aurora - two system , trained on ti - digits , tested on meeting recorder near , i think we saw in it today , and it was about six point six percent . professor b : right . right , right , right . ok . alright . so he 's doing some different things . phd c : so yeah . the only difference is the features , right now , between this and professor b : yes . ok , good . so they are helping . phd c : mm - hmm . professor b : that 's good to hear . yeah . phd c : they are helping . yeah . um . yeah . and another thing i i maybe would like to do is to just test the sri system that 's trained on macrophone test it on , uh , the noisy ti - digits , professor b : yeah . phd c : cuz i 'm still wondering where this improvement comes from . when you train on macrophone , it seems better on meeting digits . but i wonder if it 's just because maybe macrophone is acoustically closer to the meeting digits than than ti - digit is , which is ti - digits are very clean recorded digits professor b : mm - hmm . phd c : and phd a : you know , it would also be interesting to see , uh to do the regular aurora test , phd c : uh , f s phd a : um , but use the sri system instead of htk . phd c : that 's yeah . that 's what i wanted , just , uh yeah . so , just using the sri system , test it on and test it on aurora ti - digits . right . phd a : why not the full aurora , uh , test ? phd c : um . yeah . there is this problem of multilinguality yet . phd a : mm - hmm . phd c : so we do n't professor b : you 'd have to train the sri system with with all the different languages . phd a : right . phd c : we would have to train on phd a : yeah . that 's what i mean . phd c : yeah . phd a : so , like , comple professor b : it 'd be a lot of work . that 's the only thing . phd c : yeah . it 's phd a : well , i mean , uh , uh , i guess the work would be into getting the the files in the right formats , or something . right ? i mean phd c : mm - hmm . phd a : because when you train up the aurora system , you 're , uh you 're also training on all the data . phd c : that 's right . phd a : i mean , it 's phd c : yeah . yeah . i see . oh , so , ok . right . i see what you mean . professor b : that 's true , but i think that also when we 've had these meetings week after week , oftentimes people have not done the full arrange of things phd a : mm - hmm . professor b : because on on whatever it is they 're trying , because it 's a lot of work , even just with the htk . phd a : mm - hmm . professor b : so , it 's it 's a good idea , but it seems like it makes sense to do some pruning phd a : mm - hmm . professor b : first with a a test or two that makes sense for you , phd a : yeah . professor b : and then take the likely candidates and go further . phd a : yeah . phd c : mm - hmm . yeah . but , just testing on ti - digits would already give us some information about what 's going on . and mm - hmm . uh , yeah . ok . uh , the next thing is this this vad problem that , um , um so , i 'm just talking about the the curves that i i sent i sent you so , whi that shows that when the snr decrease , uh , the current vad approach does n't drop much frames for some particular noises , uh , which might be then noises that are closer to speech , uh , acoustically . professor b : i i just to clarify something for me . i they were supp supposedly , in the next evaluation , they 're going to be supplying us with boundaries . phd c : mm - hmm . professor b : so does any of this matter ? i mean , other than our interest in it . uh phd c : uh well . first of all , the boundaries might be , uh like we would have t two hundred milliseconds or before and after speech . uh . so removing more than that might still make a difference in the results . professor b : do we ? i mean , is there some reason that we think that 's the case ? phd c : and no . because we do n't did n't looked that much at that . professor b : yeah . phd c : but , still , i think it 's an interesting problem . professor b : oh , yeah . phd c : and um . yeah . professor b : but maybe we 'll get some insight on that when when , uh , the gang gets back from crete . because there 's lots of interesting problems , of course . phd c : mm - hmm . professor b : and then the thing is if if they really are going to have some means of giving us fairly tight , uh , boundaries , then that wo n't be so much the issue . phd c : yeah , yeah . mm - hmm . mm - hmm . professor b : um but i do n't know . phd g : because w we were wondering whether that vad is going to be , like , a realistic one or is it going to be some manual segmentation . and then , like , if if that vad is going to be a realistic one , then we can actually use their markers to shift the point around , i mean , the way we want professor b : mm - hmm . phd g : to find a i mean , rather than keeping the twenty frames , we can actually move the marker to a point which we find more suitable for us . professor b : right . phd g : but if that is going to be something like a manual , uh , segmenter , then we ca n't use that information anymore , phd c : mm - hmm . phd g : because that 's not going to be the one that is used in the final evaluation . professor b : right . phd g : so . we do n't know what is the type of vad which they 're going to provide . professor b : yeah . phd c : yeah . and actually there 's yeah . there 's an uh , i think it 's still for even for the evaluation , uh , it might still be interesting to work on this because the boundaries apparently that they would provide is just , um , starting of speech and end of speech uh , at the utterance level . and um . phd g : with some some gap . phd c : so phd g : i mean , with some pauses in the center , provided they meet that whatever the hang - over time which they are talking . phd c : yeah . but when you have like , uh , five or six frames , both phd g : yeah . then the they will just fill fill it up . phd c : it it with phd g : i mean , th yeah . phd c : yeah . professor b : so if you could get at some of that , uh phd c : so professor b : although that 'd be hard . phd c : yeah . it might be useful for , like , noise estimation , and a lot of other things that we want to work on . professor b : but but yeah . phd g : yeah . professor b : right . ok . phd c : but mmm . yeah . so i did i just started to test putting together two vad which was was not much work actually . um , i im re - implemented a vad that 's very close to the , um , energy - based vad that , uh , the other aurora guys use . um . so , which is just putting a threshold on the noise energy , professor b : mm - hmm . phd c : and , detect detecting the first group of four frames that have a energy that 's above this threshold , and , uh , from this point , uh , tagging the frames there as speech . so it removes the first silent portion portion of each utterance . and it really removes it , um , still o on the noises where our mlp vad does n't work a lot . professor b : cuz i would have thought that having some kind of spectral information , phd c : and professor b : uh uh , you know , in the old days people would use energy and zero crossings , for instance uh , would give you some better performance . right ? cuz you might have low - energy fricatives or or , uh stop consonants , or something like that . phd c : mm - hmm . yeah . so , your point is will be to u use whatever professor b : oh , that if you d if you use purely energy and do n't look at anything spectral , then you do n't have a good way of distinguishing between low - energy speech components and nonspeech . and , um , phd c : mm - hmm . professor b : just as a gross generalization , most nonsp many nonspeech noises have a low - pass kind of characteristic , some sort of slope . and and most , um , low - energy speech components that are unvoiced have a a high - pass kind of characteristic phd c : mm - hmm . professor b : an upward slope . so having some kind of a phd c : yeah . professor b : uh , you know , at the beginning of a of a of an s sound for instance , just starting in , it might be pretty low - energy , phd c : mm - hmm . professor b : but it will tend to have this high - frequency component . whereas , a a lot of rumble , and background noises , and so forth will be predominantly low - frequency . uh , you know , by itself it 's not enough to tell you , but it plus energy is sort of phd c : yeah . professor b : it plus energy plus timing information is sort of phd c : mm - hmm . professor b : i mean , if you look up in rabiner and schafer from like twenty - five years ago or something , that 's sort of what they were using then . phd c : mm - hmm . professor b : so it 's it 's not a phd c : mm - hmm . so , yeah . it it might be that what i did is so , removes like low , um , uh low - energy , uh , speech frames . because the way i do it is i just i just combine the two decisions so , the one from the mlp and the one from the energy - based with the with the and operator . so , i only keep the frames where the two agree that it 's speech . so if the energy - based dropped dropped low - energy speech , mmm , they they are they are lost . mmm . professor b : mm - hmm . phd c : but s still , the way it 's done right now it it helps on on the noises where it seems to help on the noises where our vad was not very good . professor b : well , i guess i mean , one could imagine combining them in different ways . but but , i guess what you 're saying is that the the mlp - based one has the spectral information . so . phd c : yeah . but yeah . but the way it 's combined wi is maybe done well , yeah . professor b : well , you can imagine phd c : the way i use a an a `` and `` operator is so , it i , uh professor b : is ? phd c : the frames that are dropped by the energy - based system are are , uh , dropped , even if the , um , mlp decides to keep them . professor b : right . right . and that might not be optimal , phd c : but , yeah . professor b : but phd c : mm - hmm . professor b : but i mean , i guess in principle what you 'd want to do is have a uh , a probability estimated by each one and and put them together . phd c : yeah . mmm . m yeah . phd a : something that that i 've used in the past is , um when just looking at the energy , is to look at the derivative . and you make your decision when the derivative is increasing for so many frames . then you say that 's beginning of speech . phd c : uh - huh . phd a : but , i 'm i 'm trying to remember if that requires that you keep some amount of speech in a buffer . i guess it depends on how you do it . but i mean , that 's that 's been a useful thing . professor b : yeah . phd c : mm - hmm . grad f : mm - hmm . phd g : yeah . well , every everywhere has a delay associated with it . i mean , you still have to k always keep a buffer , phd a : mm - hmm . phd g : then only make a decision because you still need to smooth the decision further . phd a : right . right . phd g : so that 's always there . phd a : yeah . ok . phd c : well , actually if i do n't maybe do n't want to work too much of on it right now . i just wanted to to see if it 's what i observed was the re was caused by this this vad problem . professor b : mm - hmm . phd c : and it seems to be the case . um . uh , the second thing is the this spectral subtraction . um . um , which i 've just started yesterday to launch a bunch of , uh , twenty - five experiments , uh , with different , uh , values for the parameters that are used . so , it 's the makhoul - type spectral subtraction which use an over - estimation factor . so , we substr i subtract more , um , noise than the noise spectra that is estimated on the noise portion of the s uh , the utterances . so i tried several , uh , over - estimation factors . and after subtraction , i also add a constant noise , and i also try different , uh , noise , uh , values and we 'll see what happen . professor b : hmm . ok . phd c : mm - hmm . mm - hmm . but st still when we look at the , um well , it depends on the parameters that you use , but for moderate over - estimation factors and moderate noise level that you add , you st have a lot of musical noise . um . on the other hand , when you subtract more and when you add more noise , you get rid of this musical noise but maybe you distort a lot of speech . so . well . mmm . well , it until now , it does n't seem to help . but we 'll see . so the next thing , maybe i what i will try to to do is just to try to smooth mmm , the , um to smooth the d the result of the subtraction , to get rid of the musical noise , using some kind of filter , or phd g : can smooth the snr estimate , also . phd c : yeah . right . mmm . phd g : your filter is a function of snr . hmm ? phd c : yeah . so , to get something that 's would be closer to what you tried to do with wiener filtering . phd g : yeah . phd c : and mm - hmm . yeah . phd g : actually , it 's , uh uh . i do n't know , it 's go ahead . phd c : it phd g : and it 's phd c : maybe you can phd g : go ahead . phd c : i think it 's that 's it for me . phd g : ok . so , uh u th i 've been playing with this wiener filter , like . and there are there were some bugs in the program , so i was p initially trying to clear them up . because one of the bug was i was assuming that always the vad uh , the initial frames were silence . it always started in the silence state , but it was n't for some utterances . so the it was n't estimating the noise initially , and then it never estimated , because i assumed that it was always silence . phd c : mm - hmm . so this is on speechdat - car italian ? phd g : yeah . phd c : so , in some cases s there are also phd g : speechdat - car italian . yeah . there 're a few cases , actually , which i found later , that there are . phd c : o uh - huh . phd g : so that was one of the bugs that was there in estimating the noise . and , uh , so once it was cleared , uh , i ran a few experiments with different ways of smoothing the estimated clean speech and how t estimated the noise and , eh , smoothing the snr also . and so the the trend seems to be like , uh , smoothing the current estimate of the clean speech for deriving the snr , which is like deriving the wiener filter , seems to be helping . then updating it quite fast using a very small time constant . so we 'll have , like , a few results where the estimating the the more smoothing is helping . but still it 's like it 's still comparable to the baseline . i have n't got anything beyond the baseline . but that 's , like , not using any wiener filter . and , uh , so i 'm i 'm trying a few more experiments with different time constants for smoothing the noise spectrum , and smoothing the clean speech , and smoothing snr . so there are three time constants that i have . so , i 'm just playing around . so , one is fixed in the line , like smoothing the clean speech is is helping , so i 'm not going to change it that much . but , the way i 'm estimating the noise and the way i 'm estimating the snr , i 'm just trying trying a little bit . so , that h and the other thing is , like , putting a floor on the , uh , snr , because that if some in some cases the clean speech is , like when it 's estimated , it goes to very low values , so the snr is , like , very low . and so that actually creates a lot of variance in the low - energy region of the speech . so , i 'm thinking of , like , putting a floor also for the snr so that it does n't vary a lot in the low - energy regions . and , uh . so . the results are , like so far i 've been testing only with the baseline , which is which does n't have any lda filtering and on - line normalization . i just want to separate the the contributions out . so it 's just vad , plus the wiener filter , plus the baseline system , which is , uh , just the spectral i mean , the mel sp mel , uh , frequency coefficients . um . and the other thing that i tried was but i just took of those , uh , carlos filters , which hynek had , to see whether it really h helps or not . i mean , it was just a a run to see whether it really degrades or it helps . and it 's it seems to be like it 's not hurting a lot by just blindly picking up one filter which is nothing but a four hertz a band - pass m m filter on the cubic root of the power spectrum . so , that was the filter that hy - uh , carlos had . and so yeah . just just to see whether it really it 's it 's is it worth trying or not . so , it does n't seems to be degrading a lot on that . so there must be something that i can that can be done with that type of noise compensation also , which i guess i would ask carlos about that . i mean , how how he derived those filters and and where d if he has any filters which are derived on ogi stories , added with some type of noise which what we are using currently , or something like that . so maybe i 'll professor b : this is cubic root of power spectra ? phd g : yeah . cubic root of power spectrum . professor b : so , if you have this band - pass filter , you probably get n you get negative values . right ? phd g : yeah . and i 'm , like , floating it to z zeros right now . so it has , like the spectrogram has , like uh , it actually , uh , enhances the onset and offset of i mean , the the begin and the end of the speech . so it 's there seems to be , like , deep valleys in the begin and the end of , like , high - energy regions , professor b : mm - hmm . phd g : because the filter has , like , a sort of mexican - hat type structure . professor b : mm - hmm . phd g : so , those are the regions where there are , like when i look at the spectrogram , there are those deep valleys on the begin and the end of the speech . but the rest of it seems to be , like , pretty nice . professor b : mm - hmm . phd g : so . that 's something i observe using that filter . and yeah . there are a few very not a lot of because the filter does n't have a really a deep negative portion , so that it 's not really creating a lot of negative values in the cubic root . so , i 'll i 'll s may continue with that for some w i 'll i 'll maybe i 'll ask carlos a little more about how to play with those filters , and but while making this wiener filter better . so . yeah . that that 's it , morgan . professor b : uh , last week you were also talking about building up the subspace stuff ? phd g : yeah . i i i would actually m m did n't get enough time to work on the subspace last week . it was mostly about finding those bugs and th you know , things , and i did n't work much on that . phd a : how about you , carmen ? phd d : well , i am still working with , eh , vts . and , one of the things that last week , eh , say here is that maybe the problem was with the diff because the signal have different level of energy . and , maybe , talking with stephane and with sunil , we decide that maybe it was interesting to to apply on - line normalization before applying vts . but then we decided that that 's it does n't work absolutely , because we modified also the noise . and well , thinking about that , we we then we decide that maybe is a good idea . we do n't know . i do n't hav i do n't this is i did n't do the experiment yet to apply vts in cepstral domain . professor b : the other thing is so so , in i i and not and c - zero would be a different so you could do a different normalization for c - zero than for other things anyway . i mean , the other thing i was gon na suggest is that you could have two kinds of normalization with with , uh , different time constants . so , uh , you could do some normalization s uh , before the vts , and then do some other normalization after . i do n't know . but but c - zero certainly acts differently than the others do , so that 's phd c : mm - hmm . phd d : well , we s decide to m to to obtain the new expression if we work in the cepstral domain . and well . i am working in that now , professor b : uh - huh . phd d : but i 'm not sure if that will be usefu useful . i do n't know . it 's k it 's k it 's quite a lot it 's a lot of work . professor b : uh - huh . phd d : well , it 's not too much , but this it 's work . professor b : yeah . phd d : and i want to know if if we have some feeling that the result i i would like to know if i do n't have any feeling if this will work better than apply vts aft in cepstral domain will work better than apply in m mel in filter bank domain . i r i 'm not sure . i do n't i do n't know absolutely nothing . phd c : mm - hmm . professor b : yeah . well , you 're i think you 're the first one here to work with vts , so , uh , maybe we could call someone else up who has , ask them their opinion . uh , phd c : mm - hmm . professor b : i do n't i do n't have a good feeling for it . um . phd g : pratibha . phd c : actually , the vts that you tested before was in the log domain and so the codebook is e e kind of dependent on the level of the speech signal . phd d : yeah ? phd c : and so i expect it if if you have something that 's independent of this , i expect it to it to , uh , be a better model of speech . phd d : to have better phd c : and . well . professor b : you you would n't even need to switch to cepstra . right ? i mean , you can just sort of normalize the phd c : no . we could normali norm i mean , remove the median . professor b : yeah . yeah . and then you have one number which is very dependent on the level cuz it is the level , phd d : mm - hmm . professor b : and the other which is n't . phd c : mm - hmm . yeah . but here also we would have to be careful about removing the mean of speech and not of noise . because it 's like first doing general normalization and then noise removal , which is phd d : yeah . we i was thinking to to to estimate the noise with the first frames and then apply the vad , professor b : mm - hmm . phd c : mm - hmm . phd d : before the on - line normalization . phd c : mm - hmm . phd d : we we see well , i am thinking about that and working about that , professor b : yeah . phd d : but i do n't have result this week . professor b : sure . i mean , one of the things we 've talked about maybe it might be star time to start thinking about pretty soon , is as we look at the pros and cons of these different methods , how do they fit in with one another ? because we 've talked about potentially doing some combination of a couple of them . maybe maybe pretty soon we 'll have some sense of what their characteristics are , phd d : mm - hmm . professor b : so we can see what should be combined . phd c : mm - hmm . phd a : is that it ? ok ? professor b : ok . why do n't we read some digits ? phd a : yep . want to go ahead , morgan ? professor b : sure ."
}