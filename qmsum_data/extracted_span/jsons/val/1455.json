{
    "query": "<s> what did phd a think about the results ?",
    "answer": "phd a : and , fff { comment } um , yeah . what we see is that these nets are not as good , except for the multi - english , which is always one of the best . yeah , then we started to work on a large dat database containing , uh , sentences from the french , from the spanish , from the timit , from spine , uh from { comment } uh english digits , and from italian digits . so this is the another line another set of lines in the table . uh , @ @ with spine professor d : ah , yes . mm - hmm . phd a : and uh , actually we did this before knowing the result of all the data , uh , so we have to to redo the uh the experiment training the net with , uh plp , but with delta . but professor d : mm - hmm . phd a : um this this net performed quite well . well , it 's it 's better than the net using french , spanish , and english only . uh . so , uh , yeah . we have also started feature combination experiments . uh many experiments using features and net outputs together . and this is the results are on the other document . uh , we can discuss this after , perhaps well , just , @ @ . yeah , so basically there are four four kind of systems . the first one , yeah , is combining , um , two feature streams , uh using and each feature stream has its own mpl . so it 's the kind of similar to the tandem that was proposed for the first . the multi - stream tandem for the first proposal . the second is using features and klt transformed mlp outputs . and the third one is to u use a single klt trans transform features as well as mlp outputs . um , yeah . mmm . you know you can you can comment these results , phd b : yes , i can s i would like to say that , for example , um , mmm , if we does n't use the delta - delta , uh we have an improve when we use s some combination . but when phd a : yeah , we ju just to be clear , the numbers here are uh recognition accuracy . phd b : w yeah , this yeah , this number recognition acc phd a : so it 's not the again we switch to another phd b : yes , and the baseline the baseline have i is eighty - two . professor d : baseline is eighty - two . phd a : so it 's experiment only on the italian mismatched for the moment for this . professor d : uh , this is italian mismatched . phd b : yeah , by the moment . phd a : mm - hmm . phd b : and first in the experiment - one i i do i i use different mlp , professor d : mm - hmm . phd b : and is obviously that the multi - english mlp is the better . um . for the ne rest of experiment i use multi - english , only multi - english . and i try to combine different type of feature , but the result is that the msg - three feature does n't work for the italian database because never help to increase the accuracy . phd a : yeah , eh , actually , if w we look at the table , the huge table , um , we see that for ti - digits msg perform as well as the plp , professor d : mm - hmm . phd a : but this is not the case for italian what where the error rate is c is almost uh twice the error rate of plp . professor d : mm - hmm . phd a : so , um uh , well , i do n't think this is a bug but this this is something in probably in the msg um process that uh i do n't know what exactly . perhaps the fact that the the there 's no low - pass filter , well , or no pre - emp pre - emphasis filter and that there is some dc offset in the italian , or , well , something simple like that . but that we need to sort out if want to uh get improvement by combining plp and msg professor d : mm - hmm . phd a : because for the moment msg do does n't bring much information . professor d : mm - hmm . phd a : and as carmen said , if we combine the two , we have the result , basically , of plp . professor d : i um , the uh , baseline system when you said the baseline system was uh , uh eighty - two percent , that was trained on what and tested on what ? that was , uh italian mismatched d uh , uh , digits , uh , is the testing , phd b : yeah . professor d : and the training is italian digits ? phd b : yeah . professor d : so the `` mismatch `` just refers to the noise and and , uh microphone and so forth , phd a : yeah . phd b : yeah . professor d : right ? so , um did we have so would that then correspond to the first line here of where the training is is the uh italian digits ? phd b : the train the training of the htk ? professor d : the phd b : yes . ah yes ! this h yes . th - yes . professor d : yes . training of the net , phd b : yeah . professor d : yeah . so , um so what that says is that in a matched condition , we end up with a fair amount worse putting in the uh plp . now w would do we have a number , i suppose for the matched i i do n't mean matched , but uh use of italian training in italian digits for plp only ? phd b : uh yes ? phd a : uh yeah , so this is basically this is in the table . uh so the number is fifty - two , phd b : another table . phd a : uh professor d : fifty - two percent . phd a : fift - so no , it 's it 's the professor d : no , fifty - two percent of eighty - two ? phd a : of of of uh eighteen phd b : eighty . phd a : of eighteen . phd b : eighty . phd a : so it 's it 's error rate , basically . phd b : it 's plus six . phd a : it 's er error rate ratio . so professor d : oh this is accuracy ! phd a : uh , so we have nine nine let 's say ninety percent . phd b : yeah . professor d : oy ! { comment } ok . ninety . phd a : yeah . um { comment } which is uh { comment } what we have also if use plp and msg together , professor d : yeah . phd a : eighty - nine point seven . professor d : ok , so even just plp , uh , it is not , in the matched condition um i wonder if it 's a difference between plp and mel cepstra , or whether it 's that the net half , for some reason , is not helping . phd a : uh . p - plp and mel cepstra give the same same results . professor d : same result pretty much ? phd a : well , we have these results . i do n't know . it 's not do you have this result with plp alone , { comment } j fee feeding htk ? professor d : so , s phd a : that that 's what you mean ? phd b : yeah , phd a : just plp at the input of htk . phd b : yeah yeah yeah yeah , at the first and the yeah . phd a : yeah . so , plp professor d : eighty - eight point six . phd a : yeah . professor d : um , so adding msg phd a : um professor d : um well , but that 's yeah , that 's without the neural net , phd a : yeah , that 's without the neural net professor d : right ? phd a : and that 's the result basically that ogi has also with the mfcc with on - line normalization . professor d : but she had said eighty - two . phd a : this is the w well , but this is without on - line normalization . professor d : right ? oh , this the eighty - two . phd a : yeah . eighty - two is the it 's the aurora baseline , so mfcc . then we can use well , ogi , they use mfcc th the baseline mfcc plus on - line normalization professor d : oh , i 'm sorry , i k i keep getting confused because this is accuracy . phd a : yeah , sorry . yeah . phd b : yeah . professor d : ok . alright . phd a : yeah . professor d : alright . so this is i was thinking all this was worse . ok so this is all better phd b : yes , better . professor d : because eighty - nine is bigger than eighty - two . phd a : mm - hmm . phd b : yeah . professor d : ok . i 'm i 'm all better now . ok , go ahead . phd a : so what happ what happens is that when we apply on - line normalization we jump to almost ninety percent . professor d : yeah . mm - hmm . phd a : uh , when we apply a neural network , is the same . we j jump to ninety percent . phd b : nnn , we do n't know exactly . professor d : yeah . phd a : and and um whatever the normalization , actually . if we use n neural network , even if the features are not correctly normalized , we jump to ninety percent . so professor d : so we go from eighty - si eighty - eight point six to to ninety , or something ."
}