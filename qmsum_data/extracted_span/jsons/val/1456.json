{
    "query": "<s> summarize the discussion on future directions",
    "answer": "phd a : yeah . f but , yeah . professor d : uh . but it 's another thing to try . phd a : so , this is w w i wa wa this is one thing , this this could be could help could help perhaps to reduce language dependency and for the noise part um we could combine this with other approaches , like , well , the kleinschmidt approach . so the d the idea of putting all the noise that we can find inside a database . i think kleinschmidt was using more than fifty different noises to train his network , phd b : yeah . professor d : mm - hmm . phd a : and so this is one approach and the other is multi - band uh , that i think is more robust to the noisy changes . professor d : mm - hmm . mm - hmm . phd a : so perhaps , i think something like multi - band trained on a lot of noises with uh , features - based targets could could could help . professor d : yeah , if you i i it 's interesting thought maybe if you just trained up i mean w yeah , one one fantasy would be you have something like articulatory targets and you have um some reasonable database , um but then which is um copied over many times with a range of different noises , phd a : mm - hmm . professor d : and uh if cuz what you 're trying to do is come up with a a core , reasonable feature set which is then gon na be used uh , by the the uh hmm system . phd a : mm - hmm . professor d : so . yeah , ok . phd a : so , um , yeah . the future work is , well , try to connect to the to make to plug in the system to the ogi system . um , there are still open questions there , where to put the mlp basically . professor d : mm - hmm . and i guess , you know , the the the real open question , i mean , e u there 's lots of open questions , but one of the core quote { comment } `` open questions `` for that is um , um , if we take the uh you know , the best ones here , maybe not just the best one , but the best few or something you want the most promising group from these other experiments . um , how well do they do over a range of these different tests , not just the italian ? um . and y phd a : yeah , yeah . professor d : y right ? and then um then see , again , how we know that there 's a mis there 's a uh a a loss in performance when the neural net is trained on conditions that are different than than , uh we 're gon na test on , but well , if you look over a range of these different tests um , how well do these different ways of combining the straight features with the mlp features , uh stand up over that range ? phd b : mm - hmm . professor d : that 's that that seems like the the the real question . and if you know that so if you just take plp with uh , the double - deltas . assume that 's the p the feature . look at these different ways of combining it . and uh , take let 's say , just take uh multi - english cause that works pretty well for the training . phd a : mm - hmm . professor d : and just look take that case and then look over all the different things . how does that how does that compare between the phd a : so all the all the test sets you mean , yeah . phd b : yeah . professor d : all the different test sets , phd a : and professor d : and for and for the couple different ways that you have of of of combining them . phd a : yeah . professor d : um . how well do they stand up , over the phd a : mmm . and perhaps doing this for cha changing the variance of the streams and so on getting different scaling phd b : mm - hmm . professor d : that 's another possibility if you have time , yeah . yeah . phd a : um . yeah , so thi this sh would be more working on the mlp as an additional path instead of an insert to the to their diagram . cuz yeah . perhaps the insert idea is kind of strange because nnn , they they make lda and then we will again add a network does discriminate anal nnn , that discriminates , professor d : yeah . it 's a little strange phd a : or ? mmm ? professor d : but on the other hand they did it before . phd a : mmm . and and and professor d : um the phd a : yeah . and because also perhaps we know that the when we have very good features the mlp does n't help . so . i do n't know . professor d : um , the other thing , though , is that um so . uh , we we wan na get their path running here , right ? if so , we can add this other stuff . as an additional path right ? phd a : yeah , the the way we want to do professor d : cuz they 're doing lda rasta . phd a : the d what ? professor d : they 're doing lda rasta , phd a : yeah , the way we want to do it perhaps is to just to get the vad labels and the final features . professor d : yeah ? phd a : so they will send us the well , provide us with the feature files , professor d : i see . i see . phd a : and with vad uh , binary labels so that we can uh , get our mlp features and filter them with the vad and then combine them with their f feature stream . so . professor d : i see . so we so . first thing of course we 'd wan na do there is to make sure that when we get those labels of final features is that we get the same results as them . without putting in a second path . phd a : uh . you mean oh , yeah ! just re re retraining r retraining the htk ? professor d : yeah just th w i i just to make sure that we have we understand properly what things are , our very first thing to do is to is to double check that we get the exact same results as them on htk . phd a : oh yeah . yeah , ok . mmm . phd b : yeah . professor d : uh , i mean , i do n't know that we need to r phd a : yeah . professor d : um do we need to retrain i mean we can just take the re their training files also . but . but , uh just for the testing , jus just make sure that we get the same results so we can duplicate it before we add in another phd a : mmm . ok . professor d : cuz otherwise , you know , we wo n't know what things mean . phd a : oh , yeah . ok . and um . yeah , so fff , lograsta , i do n't know if we want to we can try networks with lograsta filtered features . professor d : maybe . phd a : mmm . i 'm sorry ? yeah . well yeah . but professor d : oh ! you know , the other thing is when you say comb i 'm i 'm sorry , i 'm interrupting . { comment } that u um , uh , when you 're talking about combining multiple features , um suppose we said , `` ok , we 've got these different features and so forth , but plp seems pretty good . `` if we take the approach that mike did and have phd a : mm - hmm . professor d : i mean , one of the situations we have is we have these different conditions . we have different languages , we have different different noises , um if we have some drastically different conditions and we just train up different m l ps with them . and put put them together . what what what mike found , for the reverberation case at least , i mean i mean , who knows if it 'll work for these other ones . that you did have nice interpolative effects . that is , that yes , if you knew what the reverberation condition was gon na be and you trained for that , then you got the best results . but if you had , say , a heavily - reverberation ca heavy - reverberation case and a no - reverberation case , uh , and then you fed the thing , uh something that was a modest amount of reverberation then you 'd get some result in between the two . so it was sort of behaved reasonably . is tha that a fair yeah . phd a : yeah . so you you think it 's perhaps better to have several m l yeah but professor d : it works better if what ? i see . well , see , i oc you were doing some something that was so maybe the analogy is n't quite right . you were doing something that was in way a little better behaved . you had reverb for a single variable which was re uh , uh , reverberation . here the problem seems to be is that we do n't have a hug a really huge net with a really huge amount of training data . but we have s f for this kind of task , i would think , sort of a modest amount . i mean , a million frames actually is n't that much . we have a modest amount of of uh training data from a couple different conditions , and then uh in yeah , that and the real situation is that there 's enormous variability that we anticipate in the test set in terms of language , and noise type uh , and uh , uh , channel characteristic , sort of all over the map . a bunch of different dimensions . and so , i 'm just concerned that we do n't really have um , the data to train up i mean one of the things that we were seeing is that when we added in we still do n't have a good explanation for this , but we are seeing that we 're adding in uh , a fe few different databases and uh the performance is getting worse and uh , when we just take one of those databases that 's a pretty good one , it actually is is is is is better . and uh that says to me , yes , that , you know , there might be some problems with the pronunciation models that some of the databases we 're adding in or something like that . but one way or another we do n't have uh , seemingly , the ability to represent , in the neural net of the size that we have , um , all of the variability that we 're gon na be covering . so that i 'm i 'm i 'm hoping that um , this is another take on the efficiency argument you 're making , which is i 'm hoping that with moderate size neural nets , uh , that uh if we if they look at more constrained conditions they they 'll have enough parameters to really represent them . mm - hmm . mm - hmm . mm - hmm . yeah . phd a : so doing both is is not is not right , you mean , or ? yeah . professor d : yeah . i i just sort of have a feeling phd a : but yeah . mm - hmm . professor d : yeah . i mean i i e the um i think it 's true that the ogi folk found that using lda rasta , which is a kind of lograsta , it 's just that they have the i mean it 's done in the log domain , as i recall , and it 's it uh it 's just that they d it 's trained up , right ? that that um benefitted from on - line normalization . so they did at least in their case , it did seem to be somewhat complimentary . so will it be in our case , where we 're using the neural net ? i mean they they were not not using the neural net . uh i do n't know . ok , so the other things you have here are uh , trying to improve results from a single yeah . make stuff better . ok . uh . yeah . and cpu memory issues . yeah . we 've been sort of ignoring that , have n't we ? phd a : yeah , so i do n't know . professor d : but phd a : but we have to address the problem of cpu and memory we professor d : yeah , but i li well , i think my impression you you folks have been looking at this more than me . but my impression was that uh , there was a a a a strict constraint on the delay , phd b : yeah . professor d : but beyond that it was kind of that uh using less memory was better , and using less cpu was better . something like that , phd a : yeah , but professor d : right ? phd a : yeah . so , yeah , but we 've i do n't know . we have to get some reference point to where we well , what 's a reasonable number ? perhaps be because if it 's if it 's too large or large or @ @ professor d : um , well i do n't think we 're um completely off the wall . i mean i think that if we if we have uh , i mean the ultimate fall back that we could do if we find uh i mean we may find that we we 're not really gon na worry about the m l you know , if the mlp ultimately , after all is said and done , does n't really help then we wo n't have it in . if the mlp does , we find , help us enough in some conditions , uh , we might even have more than one mlp . we could simply say that is uh , done on the uh , server . and it 's uh we do the other manipulations that we 're doing before that . so , i i i think i think that 's that 's ok . phd a : and yeah . professor d : so i think the key thing was um , this plug into ogi . um , what what are they what are they gon na be working do we know what they 're gon na be working on while we take their features , phd a : they 're they 're starting to wor work on some kind of multi - band . professor d : and ? phd a : so . um this that was pratibha . sunil , what was he doing , do you remember ? phd b : sunil ? phd a : yeah . he was doing something new or ? phd b : i i do n't re i did n't remember . maybe he 's working with neural network . phd a : i do n't think so . trying to tune wha networks ? phd b : yeah , i think so . phd a : i think they were also mainly , well , working a little bit of new things , like networks and multi - band , but mainly trying to tune their their system as it is now to just trying to get the best from this this architecture . phd b : yeah . professor d : ok . so i guess the way it would work is that you 'd get there 'd be some point where you say , `` ok , this is their version - one `` or whatever , and we get these vad labels and features and so forth for all these test sets from them , phd a : mm - hmm . professor d : and then um , uh , that 's what we work with . we have a certain level we try to improve it with this other path and then um , uh , when it gets to be uh , january some point uh , we say , `` ok we we have shown that we can improve this , in this way . so now uh um what 's your newest version ? `` and then maybe they 'll have something that 's better and then we we 'd combine it . this is always hard . i mean i i i used to work with uh folks who were trying to improve a good uh , hmm system with uh with a neural net system and uh , it was a common problem that you 'd oh , and this actually , this is true not just for neural nets but just for in general if people were working with uh , rescoring uh , n - best lists or lattices that come came from uh , a mainstream recognizer . uh , you get something from the the other site at one point and you work really hard on making it better with rescoring . but they 're working really hard , too . so by the time you have uh , improved their score , they have also improved their score and now there is n't any difference , phd a : yeah . professor d : because the other phd b : yeah . professor d : so , um , i guess at some point we 'll have to phd a : so it 's professor d : uh { comment } uh , i i do n't know . i think we 're we 're integrated a little more tightly than happens in a lot of those cases . i think at the moment they they say that they have a better thing we can we e e what takes all the time here is that th we 're trying so many things , presumably uh , in a in a day we could turn around uh , taking a new set of things from them and and rescoring it , phd a : mmm . yeah . yeah , perhaps we could . professor d : right ? so . yeah . well , ok . no , this is i think this is good . i think that the most wide open thing is the issues about the uh , you know , different trainings . you know , da training targets and noises and so forth . phd a : mmm . so we we can for we c we can forget combining multiple features and mlg perhaps , professor d : that 's sort of wide open . phd a : or focus more on the targets and on the training data and ? professor d : yeah , i think for right now um , i th i i really liked msg . and i think that , you know , one of the things i liked about it is has such different temporal properties . and um , i think that there is ultimately a really good uh , potential for , you know , bringing in things with different temporal properties . um , but um , uh , we only have limited time and there 's a lot of other things we have to look at . and it seems like much more core questions are issues about the training set and the training targets , and fitting in uh what we 're doing with what they 're doing , and , you know , with limited time . yeah . i think we have to start cutting down . so uh i think so , yeah . and then , you know , once we um , having gone through this process and trying many different things , i would imagine that certain things uh , come up that you are curious about uh , that you 'd not getting to and so when the dust settles from the evaluation uh , i think that would time to go back and take whatever intrigued you most , you know , got you most interested uh and uh and and work with it , you know , for the next round . uh , as you can tell from these numbers uh , nothing that any of us is gon na do is actually gon na completely solve the problem . so . so , { comment } there 'll still be plenty to do . barry , you 've been pretty quiet . grad c : just listening . professor d : well i figured that , but that what what what were you involved in in this primarily ? grad c : um , helping out uh , preparing well , they 've been kind of running all the experiments and stuff and i 've been uh , uh w doing some work on the on the preparing all all the data for them to to um , train and to test on . um yeah . right now , i 'm i 'm focusing mainly on this final project i 'm working on in jordan 's class . yeah . professor d : i see . right . what 's what 's that ? grad c : um , i 'm trying to um so there was a paper in icslp about um this this multi - band um , belief - net structure . { comment } this guy did professor d : mm - hmm . grad c : uh basically it was two h m ms with with a with a dependency arrow between the two h m professor d : uh - huh . grad c : and so i wan na try try coupling them instead of t having an arrow that that flows from one sub - band to another sub - band . i wan na try having the arrows go both ways . and um , i 'm just gon na see if if that that better models um , uh asynchrony in any way or um yeah . professor d : oh ! ok . well , that sounds interesting . grad c : yeah . professor d : ok . alright . anything to you wanted to no . ok . silent partner in the in the meeting . oh , we got a laugh out of him , that 's good . ok , everyone h must contribute to the our our sound sound files here . ok , so speaking of which , if we do n't have anything else that we need you happy with where we are ? know know wher know where we 're going ? uh phd a : i think so , yeah . professor d : yeah , yeah . you you happy ? you 're happy . ok everyone should be happy . ok . you do n't have to be happy . you 're almost done . yeah , yeah . ok . grad e : al - actually i should mention so if { comment } um , about the linux machine `` swede . `` professor d : yeah . grad e : so it looks like the um , neural net tools are installed there . and um dan ellis { comment } i believe knows something about using that machine so if people are interested in in getting jobs running on that maybe i could help with that . phd a : yeah , but i do n't know if we really need now a lot of machines . well . we could start computing another huge table but yeah , we professor d : well . yeah , i think we want a different table , at least phd a : yeah , sure . professor d : right ? i mean there 's there 's some different things that we 're trying to get at now . phd a : but professor d : but phd a : yeah . mmm . professor d : so . yeah , as far as you can tell , you 're actually ok on c - on cpu uh , for training and so on ? yeah . phd a : ah yeah . i think so . well , more is always better , but mmm , i do n't think we have to train a lot of networks , now that we know we just select what works fine professor d : ok . ok . phd a : and try to improve this phd b : yeah . to work professor d : and we 're ok on and we 're ok on disk ? phd a : and it 's ok , yeah . well sometimes we have some problems . phd b : some problems with the professor d : but they 're correctable , uh problems . phd a : yeah , restarting the script basically phd b : you know . phd a : and professor d : yes . yeah , i 'm familiar with that one , ok . alright , so uh , { comment } since uh , we did n't ha get a channel on for you , { comment } you do n't have to read any digits but the rest of us will . uh , is it on ? well . we did n't uh i think i wo n't touch anything cuz i 'm afraid of making the driver crash which it seems to do , pretty easily . ok , thanks . ok , so we 'll uh i 'll start off the uh um connect the phd a : my battery is low . professor d : well , let 's hope it works . maybe you should go first and see so that you 're ok . phd b : batteries ? grad c : yeah , your battery 's going down too . professor d : transcript uh two grad c : carmen 's battery is d going down too . professor d : oh , ok . yeah . why do n't you go next then . ok . guess we 're done . ok , uh so . just finished digits . yeah , so . uh well , it 's good . i think i guess we can turn off our microphones now ."
}