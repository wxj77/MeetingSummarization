{
    "query": "<s> what did the team discuss about lda filters ?",
    "answer": "grad c : and and th the third thing , um , uh , is , um , barry explained lda filtering to me yesterday . and so , um , mike shire in his thesis um , did a a series of experiments , um , training lda filters in d on different conditions . and you were interested in having me repeat this for for this mean subtraction approach ? is is that right ? or for these long analysis windows , i guess , is the right way to put it . professor d : i guess , the the the issue i was the general issue i was bringing up was that if you 're have a moving moving window , uh , a wa a a set of weights times things that , uh , move along , shift along in time , that you have in fact a linear time invariant filter . and you just happened to have picked a particular one by setting all the weights to be equal . and so the issue is what are some other filters that you could use , uh , in that sense of `` filter `` ? grad c : mm - hmm . professor d : and , um , as i was saying , i think the simplest thing to do is not to train anything , but just to do some sort of , uh , uh , hamming or hanning , uh , kind of window , kind of thing , grad c : right . mm - hmm . professor d : just sort of to de - emphasize the jarring . so i think that would sort of be the first thing to do . but then , yeah , the lda i uh , is interesting because it would sort of say well , suppose you actually trained this up to do the best you could by some criterion , what would the filter look like then ? grad c : uh - huh . professor d : uh , and , um , that 's sort of what we 're doing in this aur - aurora stuff . and , uh , it 's still not clear to me in the long run whether the best thing to do would be to do that or to have some stylized version of the filter that looks like these things you 've trained up , because you always have the problem that it 's trained up for one condition and it is n't quite right for another . so . uh that 's that 's why that 's why rasta filter has actually ended up lasting a long time , people still using it quite a bit , because y you do n't change it . so does n't get any worse . uh , anyway . grad c : o ok . so , um , a actually i was just thinking about what i was asking about earlier , wi which is about having less than say twelve seconds in the smartkom system to do the mean subtraction . you said in systems where you use cepstral mean subtraction , they concatenate utterances and , do you know how they address this issue of , um , testing versus training ? can professor d : go ahead . professor g : i think what they do is they do it always on - line , i mean , that you just take what you have from the past , that you calculate the mean of this and subtract the mean ."
}