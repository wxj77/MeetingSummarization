{
    "query": "<s> summarize the discussion on moving between data formats and the logistics of training models",
    "answer": "phd e : d i begin to work with the italian database to nnn , to with the f front - end and with the htk program and the @ @ . and i trained eh , with the spanish two neural network with plp and with lograsta plp . i do n't know exactly what is better if if lograsta or jrasta . professor f : well , um , jrasta has the potential to do better , but it does n't always . it 's i i jrasta is more complicated . it 's it 's , uh instead of doing rasta with a log , you 're doing rasta with a log - like function that varies depending on a j parameter , uh , which is supposed to be sensitive to the amount of noise there is . so , it 's sort of like the right transformation to do the filtering in , is dependent on how much noise there is . phd e : hm - hmm . professor f : and so in jrasta you attempt to do that . it 's a little complicated because once you do that , you end up in some funny domain and you end up having to do a transformation afterwards , which requires some tables . and , uh , phd e : hm - hmm . professor f : so it 's it 's it 's a little messier , uh , there 's more ways that it can go wrong , uh , but if if if you 're careful with it , it can do better . phd e : it 's a bit i 'll do better . professor f : so , it 's so . phd e : um , and i think to to to recognize the italian digits with the neural netw spanish neural network , and also to train another neural network with the spanish digits , the database of spanish digits . and i working that . professor f : yeah . phd e : but prepa to prepare the the database are difficult . was for me , n it was a difficult work last week with the labels because the the program with the label obtained that i have , the albayzin , is different w to the label to train the neural network . and that is another work that we must to do , to to change . professor f : i i did n't understand . phd e : uh , for example albayzin database was labeled automatically with htk . it 's not hand it 's not labels by hand . professor f : oh , `` l labeled `` . phd e : labels . professor f : i 'm sorry , phd e : i 'm sorry , professor f : i have a p i had a problem with the pronunciation . phd e : i 'm sorry . the labels . i 'm sorry . the labels . professor f : yeah , ok . phd e : oh , also that professor f : so , ok , so let 's start over . so , ti timi timit 's hand - labeled , and and you 're saying about the spanish ? phd e : the spanish labels ? that was in different format , that the format for the em the program to train the neural network . professor f : oh , i see . phd e : i necessary to convert . and someti well phd a : so you 're just having a problem converting the labels . phd e : it 's it 's yeah . yeah , but n yes , because they have one program , feacalc , but no , l labecut , l labecut , but do n't does n't , eh , include the htk format to convert . professor f : mm - hmm . phd e : and , i do n't know what . i ask e even i ask to dan ellis what i can do that , and h they he say me that h he does does n't any any s any form to to do that . and at the end , i think that with labecut i can transfer to ascii format , and htk is an ascii format . and i m do another , uh , one program to put ascii format of htk to ase ay ac ascii format to exceed professor f : mm - hmm . phd e : and they used labcut to to pass . professor f : ok , yeah . phd e : actually that was complicated , professor f : so you phd e : but well , i know how we can did that do that . professor f : sure . so it 's just usual kind of uh sometimes say housekeeping , right ? to get these get these things sorted out . phd e : yeah . professor f : so it seems like there 's there 's some peculiarities of the , uh of each of these dimensions that are getting sorted out . and then , um , if if you work on getting the , uh , assembly lines together , and then the the pieces sort of get ready to go into the assembly line and gradually can start , you know , start turning the crank , more or less . and , uh , uh , we have a lot more computational capability here than they do at ogi , so i think that i if what 's what 's great about this is it sets it up in a very systematic way , so that , uh , once these all of these , you know , mundane but real problems get sorted out , we can just start turning the crank phd e : mm - hmm . professor f : and and push all of us through , and then finally figure out what 's best . grad c : yeah . um , i i was thinking two things . uh , the first thing was , um we we actually had thought of this as sort of like , um not not in stages , { comment } but more along the the time axis . just kind of like one stream at a time , professor f : mm - hmm . grad c : je - je - je - je - je { comment } check out the results and and go that way . professor f : oh , yeah , yeah , sure . no , i 'm just saying , i 'm just thinking of it like loops , grad c : uh - huh . professor f : right ? and so , y y y if you had three nested loops , that you have a choice for this , a choice for this , and a choice for that , grad c : yeah . mm - hmm . professor f : right ? and you 're going through them all . that that 's what i meant . grad c : right , right . professor f : and , uh , the thing is that once you get a better handle on how much you can realistically do , uh , um , concurrently on different machines , different sperts , and so forth , uh , and you see how long it takes on what machine and so forth , you can stand back from it and say , `` ok , if we look at all these combinations we 're talking about , and combinations of combinations , and so forth , `` you 'll probably find you ca n't do it all . grad c : mm - hmm . ok . professor f : ok , so then at that point , uh , we should sort out which ones do we throw away . grad c : mm - hmm . professor f : which of the combinations across you know , what are the most likely ones , and and , uh , i still think we could do a lot of them . i mean , it would n't surprise me if we could do a hundred of them or something . but , probably when you include all the combinations , you 're actually talking about a thousand of them or something , and that 's probably more than we can do . uh , but a hundred is a lot . and and , uh , um yeah . grad c : yeah , and the the second thing was about scratch space . and i think you sent an email about , um , e scratch space for for people to work on . and i know that , uh , stephane 's working from an nt machine , so his his home directory exists somewhere else . professor f : his his stuff is somewhere else , yeah . yeah , i mean , my point i i want to yeah , thanks for bring it back to that . my th i want to clarify my point about that that that chuck repeated in his note . um . we 're over the next year or two , we 're gon na be upgrading the networks in this place , grad c : mm - hmm . professor f : but right now they 're still all te pretty much all ten megabit lines . and we have reached the this the machines are getting faster and faster . so , it actually has reached the point where it 's a significant drag on the time for something to move the data from one place to another . grad c : mm - hmm . professor f : so , you you do n't w especially in something with repetitive computation where you 're going over it multiple times , you do do n't want to have the the data that you 're working on distant from where it 's being where the computation 's being done if you can help it . grad c : mm - hmm . professor f : uh . now , we are getting more disk for the central file server , which , since it 's not a computational server , would seem to be a contradiction to what i just said . but the idea is that , uh , suppose you 're working with , uh , this big bunch of multi multilingual databases . um , you put them all in the central ser at the cen central file server . grad c : mm - hmm . professor f : then , when you 're working with something and accessing it many times , you copy the piece of it that you 're working with over to some place that 's close to where the computation is and then do all the work there . and then that way you you wo n't have the the network you wo n't be clogging the network for yourself and others . that 's the idea . so , uh , it 's gon na take us it may be too late for this , uh , p precise crunch we 're in now , but , uh , we 're , uh it 's gon na take us a couple weeks at least to get the , uh , uh , the amount of disk we 're gon na be getting . we 're actually gon na get , uh , i think four more , uh , thirty - six gigabyte drives and , uh , put them on another another disk rack . we ran out of space on the disk rack that we had , so we 're getting another disk rack and four more drives to share between , uh primarily between this project and the meetings meetings project . um . but , uh , we 've put another i guess there 's another eighteen gigabytes that 's that 's in there now to help us with the immediate crunch . but , uh , are you saying so i do n't know where you 're stephane , where you 're doing your computations . if i so , you 're on an nt machine , so you 're using some external machine phd g : yeah , it , uh well , to it 's nutmeg and mustard , i think , professor f : do you know these yet ? phd g : i do n't know what kind . phd a : nuh - uh . professor f : yeah , ok . uh , are these are these , uh , computational servers , or something ? i 'm i 've been kind of out of it . phd g : yeah , i think , yeah . i think so . professor f : unfortunately , these days my idea of running comput of computa doing computation is running a spread sheet . uh , have n't been have n't been doing much computing personally , so . um . yeah , so those are computational servers . so i guess the other question is what disk there i space there is there on the computational servers . phd a : right . yeah , i 'm not sure what 's available on is it you said nutmeg and what was the other one ? phd g : mustard . phd a : mustard . ok . professor f : yeah , well , you 're the you 're the disk czar now . phd a : right , right . well , i 'll check on that . professor f : yeah . yeah , so basically , uh , chuck will be the one who will be sorting out what disk needs to be where , and so on , and i 'll be the one who says , `` ok , spend the money . `` so . which , i mean , n these days , uh , if you 're talking about scratch space , it does n't increase the , uh , need for backup , and , uh , i think it 's not that big a d and the the disks themselves are not that expensive . right now it 's phd a : what you can do , when you 're on that machine , is , uh , just go to the slash - scratch directory , and do a df minus k , and it 'll tell you if there 's space available . phd g : yeah . phd a : uh , and if there is then , uh professor f : but was n't it , uh i think dave was saying that he preferred that people did n't put stuff in slash - scratch . it 's more putting in d s xa or xb or , phd a : well , there 's different there , um , there 's professor f : right ? phd a : right . so there 's the slash - x - whatever disks , and then there 's slash - scratch . and both of those two kinds are not backed up . and if it 's called `` slash - scratch `` , it means it 's probably an internal disk to the machine . um . and so that 's the kind of thing where , like if um , ok , if you do n't have an nt , but you have a a a unix workstation , and they attach an external disk , { comment } it 'll be called `` slash - x - something `` uh , if it 's not backed up and it 'll be `` slash - d - something `` if it is backed up . and if it 's inside the machine on the desk , it 's called `` slash - scratch `` . but the problem is , if you ever get a new machine , they take your machine away . it 's easy to unhook the external disks , put them back on the new machine , but then your slash - scratch is gone . so , you do n't wan na put anything in slash - scratch that you wan na keep around for a long period of time . but if it 's a copy of , say , some data that 's on a server , you can put it on slash - scratch because , um , first of all it 's not backed up , and second it does n't matter if that machine disappears and you get a new machine because you just recopy it to slash - scratch . so tha that 's why i was saying you could check slash - scratch on those on on , um , mustard and and nutmeg to see if if there 's space that you could use there . professor f : i see . phd a : you could also use slash - x - whatever disks on mustard and nutmeg . phd g : yeah , yeah . phd a : um . yeah , and we do have i mean , yeah , so so you yeah , it 's better to have things local if you 're gon na run over them lots of times so you do n't have to go to the network . professor f : right , so es so especially if you 're right , if you 're if you 're taking some piece of the training corpus , which usually resides in where chuck is putting it all on the on the , uh , file server , uh , then , yeah , it 's fine if it 's not backed up because if it g g gets wiped out or something , y i mean it is backed up on the other disk . so , phd a : mm - hmm . professor f : yeah , ok . phd a : yeah , so , one of the things that i need to i 've started looking at uh , is this the appropriate time to talk about the disk space stuff ? professor f : sure . phd a : i 've started looking at , um , disk space . dan david , um , put a new , um , drive onto abbott , that 's an x disk , which means it 's not backed up . so , um , i 've been going through and copying data that is , you know , some kind of corpus stuff usually , that that we 've got on a cd - rom or something , onto that new disk to free up space on other disks . and , um , so far , um , i 've copied a couple of carmen 's , um , databases over there . we have n't deleted them off of the slash - dc disk that they 're on right now in abbott , um , uh , but we i would like to go through sit down with you about some of these other ones and see if we can move them onto , um , this new disk also . there 's there 's a lot more space there , phd g : yeah , ok . phd a : and it 'll free up more space for doing the experiments and things . so , anything that that you do n't need backed up , we can put on this new disk . um , but if it 's experiments and you 're creating files and things that you 're gon na need , you probably wan na have those on a disk that 's backed up , just in case something { comment } goes wrong . so . um so far i 've i 've copied a couple of things , but i have n't deleted anything off of the old disk to make room yet . um , and i have n't looked at the any of the aurora stuff , except for the spanish . so i i guess i 'll need to get together with you and see what data we can move onto the new disk . phd g : yeah , ok . professor f : um , yeah , i i just an another question occurred to me is is what were you folks planning to do about normalization ? phd g : um . well , we were thinking about using this systematically for all the experiments . um . professor f : this being ? phd g : so , but uh . so that this could be another dimension , but we think perhaps we can use the the best , uh , um , uh , normalization scheme as ogi is using , so , with parameters that they use there , professor f : yeah , i think that 's a good idea . phd g : u u professor f : i mean it 's i i we we seem to have enough dimensions as it is . so probably if we sort of take their phd g : yeah , yeah , yeah . professor f : probably the on - line line normalization because then it { comment } it 's if we do anything else , we 're gon na end up having to do on - line normalization too , so we may as well just do on - line normalization . phd g : mm - hmm . professor f : so . um . so that it 's plausible for the final thing . good . um . so , i guess , yeah , th the other topic i maybe we 're already there , or almost there , is goals for the for next week 's meeting . uh . i i i it seems to me that we wan na do is flush out what you put on the board here . uh . you know , maybe , have it be somewhat visual , a little bit . grad c : ok . like a s like a slide ? professor f : uh , so w we can say what we 're doing , yeah . and , um , also , if you have sorted out , um , this information about how long i roughly how long it takes to do on what and , you know , what we can how many of these trainings , uh , uh , and testings and so forth that we can realistically do , uh , then one of the big goals of going there next week would be to to actually settle on which of them we 're gon na do . and , uh , when we come back we can charge in and do it . um . anything else that i a a actually started out this this field trip started off with with , uh , stephane talking to hynek , so you may have you may have had other goals , uh , for going up , and any anything else you can think of would be we should think about accomplishing ? i mean , i 'm just saying this because maybe there 's things we need to do in preparation . phd g : oh , i think basically , this is this is , uh , yeah . professor f : ok . ok . uh . alright . and uh and the other the the last topic i had here was , um , uh d dave 's fine offer to to , uh , do something on this . i mean he 's doing he 's working on other things , but to to do something on this project . so the question is , `` where where could we , uh , uh , most use dave 's help ? `` phd g : um , yeah , i was thinking perhaps if , um , additionally to all these experiments , which is not really research , well i mean it 's , uh , running programs professor f : yeah . phd g : and , um , trying to have a closer look at the perhaps the , um , speech , uh , noise detection or , uh , voiced - sound - unvoiced - sound detection and which could be important in i for noise noise phd a : i think that would be a i think that 's a big big deal . because the you know , the thing that sunil was talking about , uh , with the labels , uh , labeling the database when it got to the noisy stuff ? the that that really throws things off . you know , having the noise all of a sudden , your your , um , speech detector , i mean the the , um what was it ? what was happening with his thing ? he was running through these models very quickly . he was getting lots of , uh , uh insertions , is what it was , in his recognitions . professor f : the only problem i mean , maybe that 's the right thing the only problem i have with it is exactly the same reason why you thought it 'd be a good thing to do . um , i i think that let 's fall back to that . but i think the first responsibility is sort of to figure out if there 's something that , uh , an an additional uh , that 's a good thing you remove the mike . go ahead , good . uh , uh . what an additional clever person could help with when we 're really in a crunch for time . right ? cuz dave 's gon na be around for a long time , phd e : yeah . professor f : right ? he 's he 's gon na be here for years . and so , um , phd g : yeah . professor f : over years , if he 's if he 's interested in , you know , voiced - unvoiced - silence , he could do a lot . but if there if in fact there 's something else that he could be doing , that would help us when we 're we 're sort of uh strapped for time we have we we 've , you know , only , uh , another another month or two to you know , with the holidays in the middle of it , um , to to get a lot done . if we can think of something some piece of this that 's going to be the very fact that it is sort of just work , and i and it 's running programs and so forth , is exactly why it 's possible that it some piece of could be handed to someone to do , because it 's not uh , yeah , so that that 's the question . and we do n't have to solve it right this s second , but if we could think of some some piece that 's that 's well defined , that he could help with , he 's expressing a will willingness to do that . phd a : what about training up a , um , a multilingual net ? phd e : yes , maybe to , mmm , put together the the label the labels between timit and spanish or something like that . phd g : yeah . yeah , so defining the superset , and , uh , joining the data and mmm . phd e : yeah . phd g : yeah . professor f : uh . yeah , that 's something that needs to be done in any event . phd e : yeah . professor f : so what we were just saying is that that , um i was arguing for , if possible , coming up with something that that really was development and was n't research because we we 're we have a time crunch . and so , uh , if there 's something that would would save some time that someone else could do on some other piece , then we should think of that first . see the thing with voiced - unvoiced - silence is i really think that that it 's to do to do a a a a poor job is is pretty quick , uh , or , you know , a so - so job . you can you can you can throw in a couple fea we know what what kinds of features help with it . you can throw something in . you can do pretty well . but i remember , in fact , when you were working on that , and you worked on for few months , as i recall , and you got to , say ninety - three percent , and getting to ninety - four really really hard . phd a : mm - hmm . another year . professor f : yeah , yeah . so , um and th th the other tricky thing is , since we are , uh , even though we 're not we do n't have a strict prohibition on memory size , and and computational complexity , uh , clearly there 's some limitation to it . so if we have to if we say we have to have a pitch detector , say , if we if we 're trying to incorporate pitch information , or at least some kind of harmonic harmonicity , or something , this is another whole thing , take a while to develop . anyway , it 's a very very interesting topic . i mean , one i think one of the a lot of people would say , and i think dan would also , uh , that one of the things wrong with current speech recognition is that we we really do throw away all the harmonicity information . uh , we try to get spectral envelopes . reason for doing that is that most of the information about the phonetic identity is in the spectral envelopes are not in the harmonic detail . but the harmonic detail does tell you something . like the fact that there is harmonic detail is is real important . so . um . so , uh . so i think yeah . so wh that so the the other suggestion that just came up was , well what about having him work on the , uh , multilingual super f superset kind of thing . uh , coming up with that and then , you know , training it training a net on that , say , um , from from , uh from timit or something . is that or uh , for multiple databases . what what would you what would you think it would wh what would this task consist of ? phd g : yeah , it would consist in , uh , well , um , creating the the superset , and , uh , modifying the lab labels for matching the superset . uh . professor f : uh , creating a superset from looking at the multiple languages , phd g : well , creating the mappings , actually . professor f : and then creating i m changing labels on timit ? phd g : yeah . professor f : or on or on multiple language multiple languages ? phd e : no . the multiple language . phd g : yeah , yeah , with the @ @ three languages , phd e : maybe for the other language because timit have more phone . professor f : yeah . phd a : so you 'd have to create a mapping from each language to the superset . phd e : yeah . mm - hmm . phd g : from each language to the superset , phd e : yeah . phd g : yeah . grad c : there 's , um carmen was talking about this sampa thing , and it 's , um , it 's an effort by linguists to come up with , um , a machine readable ipa , um , sort of thing , right ? and , um , they they have a web site that stephane was showing us that has , um has all the english phonemes and their sampa correspondent , um , phoneme , professor f : yeah . grad c : and then , um , they have spanish , they have german , they have all all sorts of languages , um , mapping mapping to the sampa phonemes , which phd e : yeah , the tr the transcription , though , for albayzin is n the transcription are of sampa the same , uh , how you say , symbol that sampa appear . phd b : sampa ? what does `` sampa `` mean ? professor f : mm - hmm . hmm . phd e : but i do n't know if timit o how is timit . phd b : so , i mean professor f : what phd b : i 'm sorry . professor f : go ahead . phd b : i was gon na say , does that mean ipa is not really international ? grad c : no , it 's it 's saying phd a : it uses special diacritics and stuff , which you ca n't do with ascii characters . grad c : y ca n't print on ascii . phd e : yeah . phd a : so the sampa 's just mapping those . phd b : oh , i see . got it . professor f : what , uh has ogi done anything about this issue ? do they have do they have any kind of superset that they already have ? phd g : i do n't think so . well , they they they 're going actually the the other way , defining uh , phoneme clusters , apparently . well . professor f : aha . that 's right . uh , and that 's an interesting way to go too . phd a : so they just throw the speech from all different languages together , then cluster it into sixty or fifty or whatever clusters ? phd g : i think they 've not done it , uh , doing , uh , multiple language yet , but what they did is to training , uh , english nets with all the phonemes , and then training it in english nets with , uh , kind of seventeen , i think it was seventeen , uh , broad classes . phd a : automatically derived mm - hmm . automatically derived broad classes , or ? phd g : yeah . yeah , i think so . phd a : uh - huh . phd g : uh , and , yeah . and the result was that apparently , when testing on cross - language it was better . i think so . but hynek did n't add did n't have all the results when he showed me that , so , well . professor f : so that does make an interesting question , though . phd g : but professor f : is there 's some way that we should tie into that with this . um . right ? i mean , if if in fact that is a better thing to do , should we leverage that , rather than doing , um , our own . right ? so , if i if if they s i mean , we have i we have the the trainings with our own categories . and now we 're saying , `` well , how do we handle cross - language ? `` and one way is to come up with a superset , but they are als they 're trying coming up with clustered , and do we think there 's something wrong with that ? phd g : i think that there 's something wrong professor f : ok . what w phd g : or well , because well , for the moment we are testing on digits , and e i perhaps u using broad phoneme classes , it 's it 's ok for um , uh classifying the digits , but as soon as you will have more words , well , words can differ with only a single phoneme , and which could be the same , uh , class . professor f : i see . phd g : well . so . professor f : right . although , you are not using this for the phd g : so , i 'm professor f : you 're using this for the feature generation , though , not the phd g : yeah , but you will ask the net to put one for th th the phoneme class professor f : yeah . phd g : and so . phd a : so you 're saying that there may not be enough information coming out of the net to help you discriminate the words ? professor f : yeah . phd g : well . yeah , yeah . mmm . phd b : fact , most confusions are within the phone phone classes , right ? i think , uh , larry was saying like obstruents are only confused with other obstruents , et cetera , et cetera . professor f : yeah . yeah . yeah . phd g : yeah , this is another p yeah , another point . professor f : yeah . grad c : so so , maybe we could look at articulatory type stuff , professor f : but that 's what i thought they were gon na grad c : right ? professor f : did they not do that , or ? phd g : i do n't think so . well , professor f : so phd g : they were talking about , perhaps , but they d professor f : they 're talking about it , but that 's sort of a question whether they did phd g : w yeah . professor f : because that 's that 's the other route to go . grad c : mm - hmm . professor f : instead of this , you know grad c : superclass . professor f : instead of the the the the superclass thing , which is to take so suppose y you do n't really mark arti to really mark articulatory features , you really wan na look at the acoustics and and see where everything is , and we 're not gon na do that . so , uh , the second class way of doing it is to look at the , uh , phones that are labeled and translate them into acoustic uh , uh articulatory , uh , uh , features . so it wo n't really be right . you wo n't really have these overlapping things and so forth , phd a : so the targets of the net are these ? professor f : but phd a : articulatory features . professor f : articulatory feature . phd a : but that implies that you can have more than one on at a time ? professor f : right . that 's right . phd a : ah . ok . professor f : you either do that or you have multiple nets . phd a : i see . professor f : um . and , um i do n't know if our software this if the qu versions of the quicknet that we 're using allows for that . do you know ? grad c : allows for ? professor f : multiple targets being one ? grad c : oh , um , we have gotten soft targets to to work . professor f : ok . so that that 'll work , yeah . grad c : yeah . professor f : ok . so , um , that 's another thing that could be done is that we could we could , uh , just translate instead of translating to a superset , just translate to articulatory features , some set of articulatory features and train with that . now the fact even though it 's a smaller number , it 's still fine because you have the the , uh , combinations . so , in fact , it has every , you know it had has has every distinction in it that you would have the other way . phd g : yeah . professor f : but it should go across languages better . phd a : we could do an interesting cheating experiment with that too . we could i do n't know , if you had uh the phone labels , you could replace them by their articulatory features and then feed in a vector with those uh , things turned on based on what they 're supposed to be for each phone to see if it if you get a big win . do you know what i 'm saying ? so , um , i mean , if your net is gon na be outputting , uh , a vector of basically of well , it 's gon na have probabilities , but let 's say that they were ones and zeros , then y and you know for each , um , i do n't know if you know this for your testing data , but if you know for your test data , you know , what the string of phones is and and you have them aligned , then you can just instead of going through the net , just create the vector for each phone and feed that in to see if that data helps . eh , eh , what made me think about this is , i was talking with hynek and he said that there was a guy at a t - andt who spent eighteen months working on a single feature . and because they had done some cheating experiments professor f : this was the guy that we were just talking a that we saw on campus . so , this was larry saul who did this did this . phd a : oh , ok . professor f : he used sonorants . phd a : right , ok , professor f : was what he was doing . phd a : right . and they they had done a cheating experiment or something , right ? professor f : yeah . phd a : and determined that professor f : he he di he did n't mention that part . phd a : well , hynek said that that , i guess before they had him work on this , they had done some experiment where if they could get that one feature right , it dramatically improved the result . professor f : but . i see . ok . phd a : so i was thinking , you know it made me think about this , that if it 'd be an interesting experiment just to see , you know , if you did get all of those right . professor f : should be . because if you get all of them in there , that defines all of the phones . so that 's that 's equivalent to saying that you 've got got all the phones right . phd a : right . professor f : so , if that does n't help , there 's phd a : yeah . professor f : although , yeah , it would be make an interesting cheating experiment because we are using it in this funny way , phd a : yeah . professor f : where we 're converting it into features . phd a : and then you also do n't know what error they 've got on the htk side . you know ? it sort of gives you your the best you could hope for , kind of . professor f : yeah . grad c : mmm . mmm , i see . phd b : the soft training of the nets still requires the vector to sum to one , though , right ? grad c : to sum up to one . phd b : so you ca n't really feed it , like , two articulatory features that are on at the same time with ones cuz it 'll kind of normalize them down to one half or something like that , for instance . phd g : but perhaps you have the choice of the final nonl grad c : right . nonlinearity ? phd g : uh , nonlinearity , yeah . is it always softmax grad c : it 's sig no , it 's actually sigmoid - x phd g : or ? yeah . grad c : for the phd g : so if you choose sigmoid it 's o it 's ok ? grad c : you , um professor f : did we just run out of disk , grad c : i think i think apparently , the , uh professor f : or ? phd b : why do n't you just choose linear ? right ? grad c : what 's that ? phd b : linear outputs ? grad c : linear outputs ? phd b : is n't that what you 'll want ? if you 're gon na do a kl transform on it . grad c : right , right . right , but during the training , we would train on sigmoid - x phd b : oh , you yeah ? grad c : and then at the end just chop off the final nonlinearity ."
}