{
    "query": "<s> summarize the meeting",
    "answer": "professor b : is it starting now ? so what what from what grad a : hello ? professor b : whatever we say from now on , it can be held against us , right ? phd e : that 's right . professor b : and uh grad a : it 's your right to remain silent . professor b : yeah . so i i the the problem is that i actually do n't know how th these held meetings are held , if they are very informal and sort of just people are say what 's going on phd e : yeah . yeah , that 's usually what we do . we just sorta go around and people say what 's going on , what 's the latest uh professor b : yeah . ok . so i guess that what may be a reasonable is if i uh first make a report on what 's happening in aurora in general , at least what from my perspective . phd e : yeah . that would be great . professor b : and and uh so , i i think that carmen and stephane reported on uh amsterdam meeting , which was kind of interesting because it was for the first time we realized we are not friends really , but we are competitors . cuz until then it was sort of like everything was like wonderful and phd e : yeah . it seemed like there were still some issues , professor b : yeah . phd e : right ? that they were trying to decide ? professor b : there is a plenty of there 're plenty of issues . phd e : like the voice activity detector , professor b : well and what happened was that they realized that if two leading proposals , which was french telecom alcatel , and us both had uh voice activity detector . and i said `` well big surprise , i mean we could have told you that n n n four months ago , except we did n't because nobody else was bringing it up `` . phd e : right . professor b : obviously french telecom did n't volunteer this information either , cuz we were working on mainly on voice activity detector for past uh several months phd e : right . professor b : because that 's buying us the most uh thing . and everybody said `` well but this is not fair . we did n't know that . `` and of course uh the it 's not working on features really . and be i agreed . phd e : right . professor b : i said `` well yeah , you are absolutely right , i mean if i wish that you provided better end point at speech because uh or at least that if we could modify the recognizer , uh to account for these long silences , because otherwise uh that that th that was n't a correct thing . `` and so then ev ev everybody else says `` well we should we need to do a new eval evaluation without voice activity detector , or we have to do something about it `` . phd e : right . professor b : and in principle i uh i we agreed . phd e : mm - hmm . professor b : we said uh `` yeah `` . because uh but in that case , uh we would like to change the uh the algorithm because uh if we are working on different data , we probably will use a different set of tricks . phd e : right . professor b : but unfortunately nobody ever officially can somehow acknowledge that this can be done , because french telecom was saying `` no , no , no , now everybody has access to our code , so everybody is going to copy what we did . `` yeah well our argument was everybody ha has access to our code , and everybody always had access to our code . we never uh uh denied that . we thought that people are honest , that if you copy something and if it is protected protected by patent then you negotiate , or something , phd e : yeah . right . professor b : right ? i mean , if you find our technique useful , we are very happy . phd e : right . professor b : but and french telecom was saying `` no , no , no , phd e : mm - hmm . professor b : there is a lot of little tricks which uh sort of uh can not be protected and you guys will take them , `` which probably is also true . i mean , you know , it might be that people will take uh uh th the algorithms apart and use the blocks from that . but i somehow think that it would n't be so bad , as long as people are happy abou uh uh uh honest about it . phd e : yeah . professor b : and i think they have to be honest in the long run , because winning proposal again uh what will be available th is will be a code . so the uh the people can go to code and say `` well listen this is what you stole from me `` phd e : mm - hmm . professor b : you know ? phd e : right . professor b : `` so let 's deal with that `` . phd e : right . professor b : so i do n't see the problem . the biggest problem of course is that f that alcatel french telecom cl claims `` well we fulfilled the conditions . we are the best . uh . we are the standard . `` and e and other people do n't feel that , because they so they now decided that that is the whole thing will be done on well - endpointed data , essentially that somebody will endpoint the data based on clean speech , because most of this the speechdat - car has the also close speaking mike and endpoints will be provided . phd e : mm - hmm . ah . professor b : and uh we will run again still not clear if we are going to run the if we are allowed to run uh uh new algorithms , but i assume so . because uh we would fight for that , really . uh but since uh u u n u at least our experience is that only endpointing a a mel cepstrum gets uh gets you twenty - one percent improvement overall and twenty - seven improvement on speechdat - car then obvious the database uh i mean the the the uh the baseline will go up . and nobody can then achieve fifty percent improvement . phd e : right . professor b : so they agreed that uh there will be a twenty - five percent improvement required on on uh h u m bad mis badly mismatched phd e : but wait a minute , i thought the endpointing really only helped in the noisy cases . professor b : it uh phd e : oh , but you still have that with the mfcc . professor b : y yeah . yeah but you have the same prob i mean mfcc basically has an enormous number of uh insertions . phd e : yeah . right . yeah . yeah . yeah . professor b : and so , so now they want to say `` we we will require fifty percent improvement only for well matched condition , and only twenty - five percent for the serial cases . `` and uh and they almost agreed on that except that it was n't a hundred percent agreed . and so last time uh during the meeting , i just uh brought up the issue , i said `` well you know uh quite frankly i 'm surprised how lightly you are making these decisions because this is a major decision . for two years we are fighting for fifty percent improvement and suddenly you are saying `` oh no we we will do something less `` , but maybe we should discuss that . and everybody said `` oh we discussed that and you were not a mee there `` and i said `` well a lot of other people were not there because not everybody participates at these teleconferencing c things . `` then they said `` oh no no no because uh everybody is invited . `` however , there is only ten or fifteen lines , so people ca n't even con you know participate . so eh they agreed , and so they said `` ok , we will discuss that . `` immediately nokia uh raised the question and they said `` oh yeah we agree this is not good to to uh dissolve the uh uh the uh the criterion . `` phd e : mm - hmm . professor b : so now officially , nokia is uh uh complaining and said they they are looking for support , uh i think qualcomm is uh saying , too `` we should n't abandon the fifty percent yet . we should at least try once again , one more round . `` phd e : mm - hmm . professor b : so this is where we are . phd e : mm - hmm . professor b : i hope that i hope that this is going to be a adopted . next wednesday we are going to have uh another uh teleconferencing call , so we 'll see what uh where it goes . phd e : so what about the issue of um the weights on the for the different systems , the well - matched , and medium - mismatched and professor b : yeah , that 's what that 's a g very good uh point , because david says `` well you know we ca we can manipulate this number by choosing the right weights anyways . `` so while you are right but uh you know but phd e : mm - hmm . professor b : uh yeah , if of course if you put a zero uh weight zero on a mismatched condition , or highly mismatched then then you are done . phd e : mm - hmm . professor b : but weights were also deter already decided uh half a year ago . so phd e : and they 're the staying the same ? professor b : well , of course people will not like it . now what is happening now is that i th i think that people try to match the criterion to solution . phd e : mm - hmm . professor b : they have solution . now they want to make sure their criterion is phd e : right . professor b : and i think that this is not the right way . phd e : yeah . professor b : uh it may be that that eventually it may ha may ha it may have to happen . but it 's should happen at a point where everybody feels comfortable that we did all what we could . phd e : mm - hmm . professor b : and i do n't think we did . phd e : mm - hmm . professor b : basically , i think that that this test was a little bit bogus because of the data and uh essentially there were these arbitrary decisions made , and and everything . so , so so this is so this is where it is . so what we are doing at ogi now is uh uh uh working basically on our parts which we i think a little bit neglected , like noise separation . uh so we are looking in ways is in uh which uh with which we can provide better initial estimate of the mel spectrum basically , which would be a l uh , f more robust to noise , and so far not much uh success . we tried uh things which uh a long time ago bill byrne suggested , instead of using fourier spectrum , from fourier transform , use the spectrum from lpc model . their argument there was the lpc model fits the peaks of the spectrum , so it may be m naturally more robust in noise . and i thought `` well , that makes sense , `` but so far we ca n't get much much out of it . uh we may try some standard techniques like spectral subtraction and phd e : you have n't tried that yet ? professor b : not not not much . or even i was thinking about uh looking back into these totally ad - hoc techniques like for instance uh dennis klatt was suggesting uh the one way to uh deal with noisy speech is to add noise to everything . so . { comment } i mean , uh uh add moderate amount of noise to all data . so that makes uh th any additive noise less addi less a a effective , phd e : i see . professor b : right ? because you already uh had the noise uh in a phd e : right . professor b : and it was working at the time . it was kind of like one of these things , you know , but if you think about it , it 's actually pretty ingenious . so well , you know , just take a take a spectrum and and and add of the constant , c , to every every value . phd e : well you 're you 're basically y yeah . so you 're making all your training data more uniform . professor b : exactly . and if if then if this data becomes noisy , it b it becomes eff effectively becomes less noisy basically . but of course you can not add too much noise because then you 'll s then you 're clean recognition goes down , but i mean it 's yet to be seen how much , it 's a very simple technique . phd e : mm - hmm . professor b : yes indeed it 's a very simple technique , you just take your spectrum and and use whatever is coming from fft , add constant , you know ? on onto power spectrum . that that or the other thing is of course if you have a spectrum , what you can s start doing , you can leave start leaving out the p the parts which are uh uh low in energy and then perhaps uh one could try to find a a all - pole model to such a spectrum . because a all - pole model will still try to to to put the the continuation basically of the of the model into these parts where the issue set to zero . that 's what we want to try . i have a visitor from brno . he 's a kind of like young faculty . pretty hard - working so he so he 's so he 's looking into that . and then most of the effort is uh now also aimed at this e e trap recognition . this uh this is this recognition from temporal patterns . phd e : hmm ! what is that ? professor b : ah , you do n't know about traps ! phd e : the traps sound familiar , i but i do n't professor b : yeah i mean tha this is familiar like sort of because we gave you the name , but , what it is , is that normally what you do is that you recognize uh speech based on a shortened spectrum . phd e : mm - hmm . mm - hmm . professor b : essentially l p - lpc , mel cepstrum , uh , everything starts with a spectral slice . uh so if you s so , given the spectrogram you essentially are sliding sliding the spectrogram along the uh f frequency axis phd e : mm - hmm . professor b : and you keep shifting this thing , and you have a spectrogram . phd e : mm - hmm . professor b : so you can say `` well you can also take the time trajectory of the energy at a given frequency `` , and what you get is then , that you get a p vector . phd e : mm - hmm . professor b : and this vector can be a a s assigned to s some phoneme . namely you can say i it i will i will say that this vector will eh will will describe the phoneme which is in the center of the vector . and you can try to classify based on that . and you so you classi so it 's a very different vector , very different properties , we do n't know much about it , but the truth is phd e : hmm . but you have many of those vectors per phoneme , professor b : well , so you get many decisions . phd e : right ? uh - huh . professor b : and then you can start dec thinking about how to combine these decisions . exactly , that 's what yeah , that 's what it is . phd e : hmm . hmm . professor b : because if you run this uh recognition , you get you still get about twenty percent error uh twenty percent correct . you know , on on like for the frame by frame basis , so uh uh so it 's much better than chance . phd e : how wide are the uh frequency bands ? professor b : that 's another thing . well c currently we start i mean we start always with critical band spectrum . for various reasons . but uh the latest uh observation uh is that you you you are you can get quite a big advantage of using two critical bands at the same time . grad a : are they adjacent , or are they s professor b : adjacent , adjacent . and the reasons there are some reasons for that . because there are some reasons i can i could talk about , will have to tell you about things like masking experiments which uh uh uh uh yield critical bands , and also experiments with release of masking , which actually tell you that something is happening across critical bands , across bands . and phd e : well how do you how do you uh convert this uh energy over time in a particular frequency band into a vector of numbers ? professor b : it 's uh uh uh i mean time t - zero is one number , time t phd e : yeah but what 's the number ? is it just the professor b : it 's a spectral energy , logarithmic spectral energy , phd e : it 's just the amount of energy in that band from f in that time interval . professor b : yeah . yes , yes . yes , yes . and that 's what that 's what i 'm saying then , so this is a this is a starting vector . it 's just like shortened f spectrum , or something . but now we are trying to understand what this vector actually represents , phd e : mm - hmm . professor b : for instance a question is like `` how correlated are the elements of this vector ? `` turns out they are quite correlated , because i mean , especially the neighboring ones , right ? they they represent the same almost the same configuration of the vocal tract . phd e : yeah . yeah . mm - hmm . professor b : so there 's a very high correlation . so the classifiers which use the diagonal covariance matrix do n't like it . so we 're thinking about de - correlating them . then the question is uh `` can you describe elements of this vector by gaussian distributions `` , or to what extent ? because uh and and and so on and so on . so we are learning quite a lot about that . and then another issue is how many vectors we should be using , i mean the so the minimum is one . phd e : mm - hmm . professor b : but i mean is the is the critical band the right uh uh dimension ? so we somehow made arbitrary decision , `` yes `` . then but then now we are thinking a lot how to uh how to use at least the neighboring band because that seems to be happening this i somehow start to believe that 's what 's happening in recognition . cuz a lot of experiments point to the fact that people can split the signal into critical bands , but then oh uh uh so you can you are quite capable of processing a signal in uh uh independently in individual critical bands . that 's what masking experiments tell you . but at the same time you most likely pay attention to at least neighboring bands when you are making any decisions , you compare what 's happening in in this band to what 's happening to the band to to to the to the neighboring bands . and that 's how you make uh decisions . that 's why the articulatory events , which uh f f fletcher talks about , they are about two critical bands . you need at least two , basically . you need some relative , relative relation . absolute number does n't tell you the right thing . you need to you need to compare it to something else , what 's happening but it 's what 's happening in the in the close neighborhood . so if you are making decision what 's happening at one kilohertz , you want to know what 's happening at nine hundred hertz and it and maybe at eleven hundred hertz , but you do n't much care what 's happening at three kilohertz . phd e : so it 's really w it 's sort of like saying that what 's happening at one kilohertz depends on what 's happening around it . it 's sort of relative to it . professor b : to some extent , it that is also true . yeah . but it 's but for but for instance , th uh uh what what uh humans are very much capable of doing is that if th if they are exactly the same thing happening in two neighboring critical bands , recognition can discard it . phd e : mm - hmm . professor b : is what 's happening hey ! ok , we need us another another voice here . phd e : hey stephane . professor b : yeah , i think so . yeah ? phd e : yep . sure . go ahead . professor b : and so so so for instance if you d if you a if you add the noise that normally masks masks the uh the the signal right ? phd e : mm - hmm . professor b : and you can show that in that if the if you add the noise outside the critical band , that does n't affect the the decisions you 're making about a signal within a critical band . unless this noise is modulated . if the noise is modulated , with the same modulation frequency as the noise in a critical band , the amount of masking is less . the moment you moment you provide the noise in n neighboring critical bands . so the s m masking curve , normally it looks like sort of i start from from here , so you { comment } you have uh no noise then you you you are expanding the critical band , so the amount of maching is increasing . and when you e hit a certain point , which is a critical band , then the amount of masking is the same . so that 's the famous experiment of fletcher , a long time ago . like that 's where people started thinking `` wow this is interesting ! `` so . phd e : yeah . professor b : but , if you if you if you modulate the noise , the masking goes up and the moment you start hitting the another critical band , the masking goes down . so essentially essentially that 's a very clear indication that that that cognition can take uh uh into consideration what 's happening in the neighboring bands . but if you go too far in a in a if you if the noise is very broad , you are not increasing much more , so so if you if you are far away from the signal uh from the signal f uh the frequency at which the signal is , then the m even the when the noise is co - modulated it it 's not helping you much . phd e : mm - hmm . yeah . mm - hmm . professor b : so . so things like this we are kind of playing with with with the hope that perhaps we could eventually u use this in a in a real recognizer . grad a : mm - hmm . professor b : like uh partially of course we promised to do this under the the the aurora uh program . phd e : but you probably wo n't have anything before the next time we have to evaluate , professor b : probably not . phd e : right ? professor b : well , maybe , most likely we will not have anything which c would comply with the rules . phd e : yeah . ah . professor b : like because uh uh phd e : latency and things . professor b : latency currently chops the require uh significant uh latency amount of processing , phd e : mm - hmm . professor b : because uh we do n't know any better , yet , than to use the neural net classifiers , uh and uh and uh traps . phd e : yeah . grad a : mm - hmm . professor b : though the the work which uh everybody is looking at now aims at s trying to find out what to do with these vectors , so that a g simple gaussian classifier would be happier with it . phd e : mm - hmm . professor b : or to what extent a gaussian classifier should be unhappy uh that , and how to gaussian - ize the vectors , and so this is uh what 's happening . then sunil is uh uh uh asked me f for one month 's vacation and since he did not take any vacation for two years , i had no i did n't have heart to tell him no . so he 's in india . and uh phd e : is he getting married or something ? professor b : uh well , he may be looking for a girl , for for i do n't i do n't i do n't ask . i know that naran - when last time narayanan did that he came back engaged . phd e : right . well , i mean , i 've known other friends who they they go to ind - they go back home to india for a month , they come back married , professor b : yeah . i know . i know , i know , phd e : you know , huh . professor b : and then of course then what happened with narayanan was that he start pushing me that he needs to get a phd because they would n't give him his wife . and she 's very pretty and he loves her and so so we had to really phd e : so he finally had some incentive to finish , professor b : oh yeah . we had well i had a incentive because he he always had this plan except he never told me . sort of figured that that was a uh that he uh he told me the day when we did very well at our nist evaluations of speaker recognition , the technology , and he was involved there . we were after presentation we were driving home and he told me . phd e : when he knew you were happy , professor b : yeah . so i i said `` well , yeah , ok `` so he took another another three quarter of the year but uh he was out . so i would n't surprise me if he has a plan like that , though though uh pratibha still needs to get out first . cuz pratibha is there a a year earlier . and s and satya needs to get out very first because he 's he already has uh four years served , though one year he was getting masters . so . so . phd e : so have the um when is the next uh evaluation ? june or something ? professor b : which ? speaker recognition ? phd e : no , for uh aurora ? professor b : uh there , we do n't know about evaluation , next meeting is in june . and uh uh but like getting get together . phd e : oh , ok . are people supposed to rerun their systems , professor b : nobody said that yet . phd e : or ? professor b : i assume so . uh yes , uh , but nobody even set up yet the date for uh delivering uh endpointed data . phd e : hmm . wow . professor b : and this uh that that sort of stuff . but i uh , yeah , what i think would be of course extremely useful , if we can come to our next meeting and say `` well you know we did get fifty percent improvement . if if you are interested we eventually can tell you how `` , but uh we can get fifty percent improvement . phd e : mm - hmm . professor b : because people will s will be saying it 's impossible . phd e : hmm . do you know what the new baseline is ? oh , i guess if you do n't have professor b : twenty - two t twenty twenty - two percent better than the old baseline . phd e : using your uh voice activity detector ? professor b : u yes . yes . but i assume that it will be similar , i do n't i i do n't see the reason why it should n't be . phd e : similar , yeah . professor b : i d i do n't see reason why it should be worse . phd e : mm - hmm . professor b : cuz if it is worse , then we will raise the objection , phd e : yeah . professor b : we say `` well you know how come ? `` because eh if we just use our voice activity detector , which we do n't claim even that it 's wonderful , it 's just like one of them . phd c : mm - hmm . phd e : yeah . professor b : we get this sort of improvement , how come that we do n't see it on on on on your endpointed data ? phd c : yeah . i guess it could be even better , professor b : i think so . phd c : because the voice activity detector that i choosed is something that cheating , it 's using the alignment of the speech recognition system , professor b : yeah . c yeah uh phd c : and only the alignment on the clean channel , and then mapped this alignment to the noisy channel . professor b : and on clean speech data . yeah . phd e : oh , ok . professor b : well david told me david told me yesterday or harry actually he told harry from qualcomm and harry uh brought up the suggestion we should still go for fifty percent he says are you aware that your system does only thirty percent uh comparing to to endpointed baselines ? so they must have run already something . phd c : yeah . professor b : so . and harry said `` yeah . but i mean we think that we we did n't say the last word yet , that we have other other things which we can try . `` so . so there 's a lot of discussion now about this uh new criterion . because nokia was objecting , with uh qualcomm 's we basically supported that , we said `` yes `` . phd c : mm - hmm . mm - hmm . professor b : now everybody else is saying `` well you guys might must be out of your mind . `` uh the guenter hirsch who d does n't speak for ericsson anymore because he is not with ericsson and ericsson may not may withdraw from the whole aurora activity because they have so many troubles now . ericsson 's laying off twenty percent of people . phd e : where 's uh guenter going ? professor b : well guenter is already he got the job uh already was working on it for past two years or three years phd e : mm - hmm . professor b : he got a job uh at some some fachschule , the technical college not too far from aachen . so it 's like professor u university professor phd e : mm - hmm . professor b : you know , not quite a university , not quite a sort of it 's not aachen university , but it 's a good school and he he 's happy . phd e : mm - hmm . hmm ! professor b : and he well , he was hoping to work uh with ericsson like on t uh like consulting basis , but right now he says says it does n't look like that anybody is even thinking about speech recognition . phd e : mm - hmm . professor b : they think about survival . yeah . so . so . but this is being now discussed right now , and it 's possible that uh that that it may get through , that we will still stick to fifty percent . phd c : mm - hmm . professor b : but that means that nobody will probably get this im this improvement . yet , wi with the current system . which event es essentially i think that we should be happy with because that that would mean that at least people may be forced to look into alternative solutions phd c : mm - hmm . professor b : and phd c : mm - hmm . but maybe i i mean we are not too far from from fifty percent , from the new baseline . professor b : uh , but not phd c : which would mean like sixty percent over the current baseline , which is professor b : yeah . yes . yes . we we getting we getting there , right . phd c : well . we are around fifty , fifty - five . professor b : yeah . yeah . phd c : mm - hmm . professor b : is it like sort of is how did you come up with this number ? if you improve twenty by twenty percent the c the f the all baselines , it 's just a quick c comp co computation ? phd c : yeah . i do n't know exactly if it 's professor b : uh - huh . i think it 's about right . phd c : yeah , because it de it depends on the weightings professor b : yeah , yeah . phd c : and yeah . but . mm - hmm . phd e : hmm . how 's your documentation or whatever it w what was it you guys were working on last week ? phd c : yeah , finally we we 've not finished with this . we stopped . phd d : more or less it 's finished . phd c : yeah . phd d : ma - nec to need a little more time to improve the english , and maybe s to fill in something some small detail , something like that , phd c : mm - hmm . phd d : but it 's more or less ready . phd c : yeah . well , we have a document that explain a big part of the experiments , phd d : necessary to to include the bi the bibliography . mm - hmm . phd c : it 's not , yeah , finished yet . mm - hmm . phd e : so have you been running some new experiments ? i i thought i saw some jobs of yours running on some of the machine phd c : yeah . right . we 've fff { comment } done some strange things like removing c - zero or c - one from the the vector of parameters , and we noticed that c - one is almost not useful at all . you can remove it from the vector , it does n't hurt . phd e : really ? ! that has no effect ? eh is this in the baseline ? or in uh phd c : in the no , in the proposal . phd e : in uh - huh , uh - huh . professor b : so we were just discussing , since you mentioned that , in it w phd c : mm - hmm . professor b : driving in the car with morgan this morning , we were discussing a good experiment for b for beginning graduate student who wants to run a lot of who wants to get a lot of numbers on something phd c : mm - hmm . professor b : which is , like , `` imagine that you will you will start putting every co any coefficient , which you are using in your vector , in some general power . phd e : in some what ? professor b : general pow power . like sort of you take a s power of two , or take a square root , or something . phd e : mm - hmm . phd c : mm - hmm . professor b : so suppose that you are working with a s c - zer c - one . phd e : mm - hmm . professor b : so if you put it in a s square root , that effectively makes your model half as efficient . because uh your uh gaussian mixture model , right ? computes the mean . phd e : mm - hmm . professor b : and and uh i i i but it 's the mean is an exponent of the whatever , the the this gaussian function . phd e : you 're compressing the range , professor b : so you 're compressing the range of this coefficient , so it 's becoming less efficient . phd e : right ? of that professor b : right ? phd e : mm - hmm . professor b : so . so . morgan was @ @ and he was he was saying well this might be the alternative way how to play with a with a fudge factor , you know , uh in the you know , just compress the whole vector . phd e : yeah . professor b : and i said `` well in that case why do n't we just start compressing individual elements , like when when because in old days we were doing when when people still were doing template matching and euclidean distances , we were doing this liftering of parameters , right ? phd e : uh - huh . professor b : because we observed that uh higher parameters were more important than lower for recognition . and basically the the c - ze c - one contributes mainly slope , phd e : right . professor b : and it 's highly affected by uh frequency response of the of the recording equipment and that sort of thing , phd c : mm - hmm . phd e : mm - hmm . professor b : so so we were coming with all these f various lifters . phd e : mm - hmm . professor b : uh bell labs had he this uh uh r raised cosine lifter which still i think is built into h htk for reasons n unknown to anybody , but but uh we had exponential lifter , or triangle lifter , basic number of lifters . and . but so they may be a way to to fiddle with the f with the f phd e : insertions . professor b : insertions , deletions , or the the giving a relative uh basically modifying relative importance of the various parameters . phd c : mm - hmm . professor b : the only of course problem is that there 's an infinite number of combinations and if the if you s if y phd e : oh . uh - huh . you need like a some kind of a professor b : yeah , you need a lot of graduate students , and a lot of computing power . phd e : you need to have a genetic algorithm , that basically tries random permutations of these things . professor b : i know . exactly . oh . if you were at bell labs or i d d i should n't be saying this in on on a mike , right ? or i uh ibm , that 's what maybe that 's what somebody would be doing . phd e : yeah . professor b : oh , i mean , i mean the places which have a lot of computing power , so because it is really it 's a p it 's a it 's it will be reasonable search phd e : mm - hmm . yeah . professor b : uh but i wonder if there is n't some way of doing this uh search like when we are searching say for best discriminants . phd e : you know actually , i do n't know that this would n't be all that bad . i mean you you compute the features once , professor b : yeah . yeah . phd e : right ? and then these exponents are just applied to that professor b : absolutely . and hev everything is fixed . everything is fixed . each each phd e : and is this something that you would adjust for training ? or only recognition ? professor b : for both , you would have to do . yeah . phd e : you would do it on both . professor b : you have to do bo both . phd e : so you 'd actually professor b : because essentially you are saying `` uh this feature is not important `` . phd e : mm - hmm . professor b : or less important , so that 's th that 's a that 's a painful one , yeah . phd e : so for each uh set of exponents that you would try , it would require a training and a recognition ? professor b : yeah . but but wait a minute . you may not need to re uh uh retrain the m model . you just may n may need to c uh give uh less weight to to uh a mod uh a component of the model which represents this particular feature . you do n't have to retrain it . phd e : oh . so if you instead of altering the feature vectors themselves , you you modify the the the gaussians in the models . professor b : you just multiply . yeah . yep . you modify the gaussian in the model , but in the in the test data you would have to put it in the power , but in a training what you c in a training uh in trained model , all you would have to do is to multiply a model by appropriate constant . phd e : uh - huh . but why if you 're if you 're multi if you 're altering the model , why w in the test data , why would you have to muck with the uh cepstral coefficients ? professor b : because in uh test in uh test data you ca do n't have a model . you have uh only data . but in a in a tr phd e : no . but you 're running your data through that same model . professor b : that is true , but w i mean , so what you want to do you want to say if uh obs you if you observe something like stephane observes , that c - one is not important , you can do two things . phd e : mm - hmm . mm - hmm . professor b : if you have a trained trained recognizer , in the model , you know the the the the component which i i mean di dimension wh phd e : mm - hmm . all of the all of the mean and variances that correspond to c - one , you put them to zero . professor b : to the s you you know it . but what i 'm proposing now , if it is important but not as important , you multiply it by point one in a model . phd e : yeah . professor b : but but but phd e : but what are you multiplying ? cuz those are means , right ? grad a : you 're multiplying the standard deviation ? phd e : i mean you 're grad a : so it 's professor b : i think that you multiply the i would i would have to look in the in the math , i mean how how does the model uh phd e : i think you professor b : yeah . phd e : yeah , i think you 'd have to modify the standard deviation or something , so that you make it wider or narrower . grad a : cuz yeah . professor b : yeah . phd c : yeah . professor b : effectively , that 's that that 's i exactly . that 's what you do . that 's what you do , you you you modify the standard deviation as it was trained . grad a : yeah . professor b : effectively you , you know y in f in front of the of the model , you put a constant . s yeah effectively what you 're doing is you is you are modifying the the the deviation . right ? grad a : the spread , right . phd e : sorry . professor b : yeah , the spread . grad a : it 's the same same mean , right ? professor b : and and and phd e : so by making th the standard deviation narrower , { comment } uh your scores get worse for professor b : yeah . phd e : unless it 's exactly right on the mean . professor b : your als no . by making it narrower , phd e : right ? professor b : uh y your phd e : i mean there 's you 're you 're allowing for less variance . grad a : mm - hmm . professor b : yes , so you making this particular dimension less important . because see what you are fitting is the multidimensional gaussian , right ? phd e : mm - hmm . professor b : it 's a it has it has uh thirty - nine dimensions , or thirteen dimensions if you g ignore deltas and double - deltas . phd e : mm - hmm . mm - hmm . professor b : so in order if you in order to make dimension which which stephane sees uh less important , uh uh i mean not not useful , less important , what you do is that this particular component in the model you can multiply by w you can you can basically de - weight it in the model . but you ca n't do it in a in a test data because you do n't have a model for th i mean uh when the test comes , but what you can do is that you put this particular component in and and you compress it . that becomes uh th gets less variance , subsequently becomes less important . phd e : could n't you just do that to the test data and not do anything with your training data ? professor b : that would be very bad , because uh your t your model was trained uh expecting uh , that would n't work . because your model was trained expecting a certain var variance on c - one . phd e : uh - huh . professor b : and because the model thinks c - one is important . after you train the model , you sort of y you could do you could do still what i was proposing initially , that during the training you you compress c - one that becomes then it becomes less important in a training . phd e : mm - hmm . professor b : but if you have if you want to run e ex extensive experiment without retraining the model , you do n't have to retrain the model . you train it on the original vector . but after , you wh when you are doing this parametric study of importance of c - one you will de - weight the c - one component in the model , and you will put in the you will compress the this component in a in the test data . s by the same amount . phd e : could you also if you wanted to if you wanted to try an experiment uh by leaving out say , c - one , could n't you , in your test data , uh modify the all of the c - one values to be um way outside of the normal range of the gaussian for c - one that was trained in the model ? so that effectively , the c - one never really contributes to the score ? phd c : mm - hmm . professor b : no , that would be a severe mismatch , phd e : do you know what i 'm say professor b : right ? what you are proposing ? n no you do n't want that . phd e : yeah , someth professor b : because that would then your model would be unlikely . your likelihood would be low , right ? because you would be providing severe mismatch . phd e : mm - hmm . but what if you set if to the mean of the model , then ? and it was a cons you set all c - ones coming in through your test data , you you change whatever value that was there to the mean that your model had . professor b : no that would be very good match , right ? phd e : yeah . professor b : that you would phd c : which well , yeah , but we have several means . so . professor b : i see what you are sa saying , phd c : right ? grad a : saying . professor b : but uh , no , no i do n't think that it would be the same . i mean , no , the if you set it to a mean , that would no , you ca n't do that . y you ca you ca ch - chuck , you ca n't do that . phd e : oh , that 's true , right , yeah , because you you have phd c : wait . which professor b : because that would be a really f fiddling with the data , phd e : yeah . professor b : you ca n't do that . phd e : mm - hmm . mm - hmm . professor b : but what you can do , i 'm confident you ca well , i 'm reasonably confident and i putting it on the record , right ? i mean y people will listen to it for for centuries now , is what you can do , is you train the model uh with the with the original data . grad a : mm - hmm . professor b : then you decide that you want to see how important c c - one is . so what you will do is that a component in the model for c - one , you will divide it by by two . and you will compress your test data by square root . phd e : mm - hmm . professor b : then you will still have a perfect m match . except that this component of c - one will be half as important in a in a overall score . phd e : mm - hmm . mm - hmm . professor b : then you divide it by four and you take a square , f fourth root . then if you think that some component is more is more important then th th th it then then uh uh i it is , based on training , then you uh multiply this particular component in the model by by by phd e : you 're talking about the standard deviation ? professor b : yeah . phd e : yeah . professor b : yeah , multiply this component uh i it by number b larger than one , phd e : mm - hmm . professor b : and you put your data in power higher than one . then it becomes more important . in the overall score , i believe . phd c : yeah , but , at the phd e : but do n't you have to do something to the mean , also ? grad a : yeah . phd c : but i think it 's uh the the variance is on on the denominator in the in the gaussian equation . so . i think it 's maybe it 's the contrary . if you want to decrease the importance of a c parameter , you have to increase it 's variance . professor b : yes . right . yes . phd d : multiply . professor b : exactly . yeah . so you so you may want to do it other way around , phd c : hmm . that 's right . ok . professor b : yeah . phd c : mm - hmm . grad a : right . phd e : but if your if your um original data for c - one had a mean of two . professor b : uh - huh . phd e : and now you 're you 're you 're changing that by squaring it . now your mean of your c - one original data has { comment } is four . but your model still has a mean of two . so even though you 've expended the range , your mean does n't match anymore . phd c : mm - hmm . professor b : let 's see . phd e : do you see what i mean ? phd c : i think what i see what could be done is you do n't change your features , which are computed once for all , professor b : uh - huh . phd c : but you just tune the model . so . you have your features . you train your your model on these features . phd e : mm - hmm . phd c : and then if you want to decrease the importance of c - one you just take the variance of the c - one component in the in the model and increase it if you want to decrease the importance of c - one or decrease it phd e : yeah . professor b : yeah . phd e : right . professor b : yeah . you would have to modify the mean in the model . i you i agree with you . yeah . yeah , but i mean , but it 's it 's i it 's do - able , phd c : well . phd e : yeah , so y professor b : right ? i mean , it 's predictable . uh . yeah . phd e : it 's predictable , yeah . professor b : yeah . yeah , it 's predictable . phd e : yeah . but as a simple thing , you could just just muck with the variance . phd c : just adjust the model , yeah . phd e : to get uh this uh this the effect i think that you 're talking about , professor b : mm - hmm . phd e : right ? professor b : it might be . phd e : could increase the variance to decrease the importance . professor b : mm - hmm . phd c : mm - hmm . phd e : yeah , because if you had a huge variance , you 're dividing by a large number , { comment } you get a very small contribution . grad a : does n't matter phd c : yeah , it becomes more flat grad a : right . phd c : and professor b : yeah . phd c : yeah . phd e : yeah . grad a : yeah , the sharper the variance , the more more important to get that one right . professor b : mm - hmm . phd e : yeah , you know actually , this reminds me of something that happened uh when i was at bbn . we were playing with putting um pitch into the mandarin recognizer . professor b : mm - hmm . phd e : and this particular pitch algorithm um when it did n't think there was any voicing , was spitting out zeros . so we were getting uh when we did clustering , we were getting groups uh of features professor b : p pretty new outliers , interesting outliers , right ? phd e : yeah , with with a mean of zero and basically zero variance . professor b : variance . phd e : so , when ener { comment } when anytime any one of those vectors came in that had a zero in it , we got a great score . i mean it was just , you know , incredibly high score , and so that was throwing everything off . phd c : mm - hmm . phd e : so if you have very small variance you get really good scores when you get something that matches . professor b : yeah . phd c : mm - hmm . phd e : so . so that 's a way , yeah , yeah that 's a way to increase the yeah , n that 's interesting . so in fact , that would be that does n't require any retraining . professor b : yeah . no . no . phd c : no , that 's right . so it 's phd e : so that means it 's just professor b : yeah . phd c : just tuning the models and testing , actually . phd e : recognitions . professor b : yeah . phd e : yeah . phd c : it would be quick . phd e : you you have a step where you you modify the models , make a d copy of your models with whatever variance modifications you make , and rerun recognition . professor b : yeah . phd c : mm - hmm . professor b : yeah . yeah . yeah . phd e : and then do a whole bunch of those . professor b : yeah . phd c : mm - hmm . phd e : that could be set up fairly easily i think , and you have a whole bunch of you know professor b : chuck is getting himself in trouble . phd e : that 's an interesting idea , actually . for testing the yeah . huh ! grad a : did n't you say you got these uh htk 's set up on the new linux boxes ? phd e : that 's right . grad a : yeah . phd e : in fact , and and they 're just t right now they 're installing uh increasing the memory on that uh the linux box . professor b : and chuck is sort of really fishing for how to keep his computer busy , grad a : right . professor b : right ? phd e : yeah . absinthe . professor b : well , you know , that 's phd e : absinthe . we 've got five processors on that . grad a : oh yeah . professor b : that 's yeah , that 's a good thing grad a : that 's right . professor b : because then y you just write the `` do `` - loops and then you pretend that you are working while you are sort of you c you can go fishing . phd e : and two gigs of memory . phd c : yeah . phd e : yeah . grad a : pretend , yeah . phd e : exactly . yeah . phd d : go fishing . phd e : see how many cycles we used ? professor b : yeah . then you are sort of in this mode like all of those arpa people are , right ? phd e : yeah . professor b : uh , since it is on the record , i ca n't say uh which company it was , but it was reported to me that uh somebody visited a company and during a d during a discussion , there was this guy who was always hitting the carriage returns uh on a computer . phd e : uh - huh . professor b : so after two hours uh the visitor said `` wh why are you hitting this carriage return ? `` and he said `` well you know , we are being paid by a computer ty i mean we are we have a government contract . and they pay us by by amount of computer time we use . `` it was in old days when there were uh of pdp - eights and that sort of thing . phd e : oh , my gosh ! so he had to make it look like professor b : because so they had a they literally had to c monitor at the time at the time on a computer how much time is being spent i i i or on on this particular project . phd e : yeah . how idle time . grad a : yeah . phd e : yeah . professor b : nobody was looking even at what was coming out . phd e : have you ever seen those little um it 's it 's this thing that 's the shape of a bird and it has a red ball and its beak dips into the water ? professor b : yeah , i know , right . phd e : so if you could hook that up so it hit the keyboard professor b : yeah . yeah . yeah . yeah . phd e : that 's an interesting experiment . professor b : it would be similar similar to i knew some people who were uh that was in old communist uh czechoslovakia , right ? so we were watching for american airplanes , coming to spy on on uh on us at the time , phd e : mm - hmm . mm - hmm . professor b : so there were three guys uh uh stationed in the middle of the woods on one l lonely uh watching tower , pretty much spending a year and a half there because there was this service right ? and so they very quickly they made friends with local girls and local people in the village and phd e : yeah . professor b : and so but they there was one plane flying over s always uh uh above , and so that was the only work which they had . they like four in the afternoon they had to report there was a plane from prague to brno basically f flying there , phd e : yeah . professor b : so they f very q f first thing was that they would always run back and and at four o ' clock and and quickly make a call , `` this plane is uh uh passing `` then a second thing was that they they took the line from this u u post to uh uh a local pub . and they were calling from the pub . and they but third thing which they made , and when they screwed up , they finally they had to p the the p the pub owner to make these phone calls because they did n't even bother to be there anymore . and one day there was there was no plane . at least they were sort of smart enough that they looked if the plane is flying there , right ? and the pub owner says `` oh my four o ' clock , ok , quickly p pick up the phone , call that there 's a plane flying . `` phd e : yeah . professor b : there was no plane for some reason , phd e : and there was n't ? professor b : it was downed , or and so they got in trouble . but . but uh . phd e : huh ! well that 's that 's a really i professor b : so . so . yeah . phd e : that would n't be too difficult to try . professor b : yeah . phd e : maybe i could set that up . professor b : yeah . phd e : and we 'll just professor b : well , at least go test the s test the uh assumption about c - c - one i mean to begin with . but then of course one can then think about some predictable result to change all of them . phd c : mm - hmm . professor b : it 's just like we used to do these uh these uh um the the uh distance measures . it might be that uh phd e : yeah , so the first set of uh variance weighting vectors would be just you know one modifying one and leaving the others the same . professor b : yeah . yeah . yeah . yeah . yeah . phd c : yeah . maybe . phd e : and and do that for each one . professor b : because you see , i mean , what is happening here in a in a in a in such a model is that it 's tells you yeah what has a low variance uh is uh is uh is more reliable , phd e : that would be one set of experiment professor b : right ? how do we phd e : wh - yeah , when the data matches that , then you get really professor b : yeah . yeah . yeah . yeah . yeah . phd e : yeah . right . professor b : how do we know , especially when it comes to noise ? phd e : but there could just naturally be low variance . professor b : yeah ? phd e : because i like , i 've noticed in the higher cepstral coefficients , the numbers seem to get smaller , right ? so d phd c : they t phd e : i mean , just naturally . phd c : yeah . professor b : yeah , th that 's phd c : they have smaller means , also . uh . phd e : yeah . exactly . and so it seems like they 're already sort of compressed . phd c : uh - huh . phd e : the range of values . professor b : yeah that 's why uh people used these lifters were inverse variance weighting lifters basically that makes uh uh euclidean distance more like uh mahalanobis distance with a diagonal covariance when you knew what all the variances were over the old data . phd e : mm - hmm . mm - hmm . hmm . professor b : what they would do is that they would weight each coefficient by inverse of the variance . turns out that uh the variance decreases at least at fast , i believe , as the index of the cepstral coefficients . i think you can show that uh uh analytically . phd e : mm - hmm . professor b : so typically what happens is that you you need to weight the uh weight the higher coefficients more than uh the lower coefficients . phd e : hmm . mm - hmm . hmm . professor b : when yeah . when we talked about aurora still i wanted to m make a plea uh encourage for uh more communication between between uh uh different uh parts of the distributed uh uh center . uh even when there is absolutely nothing to to s to say but the weather is good in ore - in in berkeley . i 'm sure that it 's being appreciated in oregon and maybe it will generate similar responses down here , like , uh phd c : we can set up a webcam maybe . professor b : yeah . grad a : yeah . professor b : what you know , nowadays , yeah . it 's actually do - able , almost . phd e : is the um if we mail to `` aurora - inhouse `` , does that go up to you guys also ? professor b : i do n't think so . no . so we should do that . phd e : so i what is it grad a : yeah . professor b : we should definitely set up phd e : yeah we sh do we have a mailing list that includes uh the ogi people ? professor b : yeah . phd c : uh no . we do n't have . professor b : uh - huh . phd e : oh ! maybe we should set that up . that would make it much easier . professor b : yeah . yeah . yeah , that would make it easier . phd e : so maybe just call it `` aurora `` or something that would professor b : yeah . yeah . and then we also can send the the dis to the same address right , and it goes to everybody phd e : mm - hmm . mm - hmm . phd c : yeah . phd e : ok . maybe we can set that up . professor b : because what 's happening naturally in research , i know , is that people essentially start working on something and they do n't want to be much bothered , right ? but what the the then the danger is in a group like this , is that two people are working on the same thing and i c of course both of them come with the s very good solution , but it could have been done somehow in half of the effort or something . phd e : mm - hmm . professor b : oh , there 's another thing which i wanted to uh uh report . lucash , i think , uh wrote the software for this aurora - two system . reasonably uh good one , because he 's doing it for intel , but i trust that we have uh rights to uh use it uh or distribute it and everything . cuz intel 's intentions originally was to distribute it free of charge anyways . u s and so so uh we we will make sure that at least you can see the software and if if if if it is of any use . just uh phd c : mm - hmm . professor b : it might be a reasonable point for p perhaps uh start converging . phd c : mm - hmm . professor b : because morgan 's point is that he is an experienced guy . he says `` well you know it 's very difficult to collaborate if you are working with supposedly the same thing , in quotes , except which is not s is not the same . phd e : mm - hmm . professor b : which which uh uh one is using that set of hurdles , another one set is using another set of hurdles . so . and and then it 's difficult to c compare . phd c : what about harry ? uh . we received a mail last week and you are starting to to do some experiments . professor b : he got the he got the software . yeah . they sent the release . phd c : and use this intel version . professor b : yeah . yeah . yeah . yeah . yeah because intel paid us uh should i say on a microphone ? uh some amount of money , not much . not much i can say on a microphone . much less then we should have gotten for this amount of work . and they wanted uh to to have software so that they can also play with it , which means that it has to be in a certain environment they use actu actually some intel libraries , but in the process , lucash just rewrote the whole thing because he figured rather than trying to f make sense uh of uh including icsi software uh not for training on the nets but i think he rewrote the the the or so maybe somehow reused over the parts of the thing so that so that the whole thing , including mlp , trained mlp is one piece of uh software . phd e : mm - hmm . wow ! professor b : is it useful ? grad a : ye - yeah . professor b : yeah ? grad a : i mean , i remember when we were trying to put together all the icsi software for the submission . professor b : or that 's what he was saying , right . he said that it was like it was like just so many libraries and nobody knew what was used when , and and so that 's where he started and that 's where he realized that it needs to be needs to be uh uh at least cleaned up , grad a : yeah . phd c : mm - hmm . professor b : and so i think it this is available . so phd c : yeah . well , the the only thing i would check is if he does he use intel math libraries , professor b : uh e ev phd c : because if it 's the case , it 's maybe not so easy to use it on another architecture . professor b : n not maybe maybe not in a first maybe not in a first ap approximation because i think he started first just with a plain c c or c - plus - plus or something before phd c : ah yeah . mm - hmm . professor b : i i can check on that . yeah . phd c : yeah . phd e : hmm . professor b : and uh in otherwise the intel libraries , i think they are available free of f freely . but they may be running only on on uh on uh windows . phd c : yeah . professor b : or on on the phd c : on intel architecture maybe . professor b : yeah , on intel architecture , may not run in sun . phd c : i 'm yeah . professor b : yeah . phd c : yeah . professor b : that is p that is that is possible . that 's why intel of course is distributing it , phd c : well . professor b : right ? or that 's phd c : yeah . well there are at least there are optimized version for their architecture . professor b : yeah . phd c : i do n't know . i never checked carefully these sorts of professor b : i know there was some issues that initially of course we d do all the development on linux but we use we do n't have we have only three uh uh uh uh s suns and we have them only because they have a spert board in . otherwise otherwise we t almost exclusively are working with uh pc 's now , with intel . in that way intel succeeded with us , because they gave us too many good machines for very little money or nothing . phd e : yeah . professor b : so . so . so we run everything on intel . and phd e : hmm . does anybody have anything else ? to shall we read some digits ? phd c : yeah . professor b : yes . i have to take my glasses phd e : so . hynek , i do n't know if you 've ever done this . the way that it works is each person goes around in turn , { comment } and uh you say the transcript number and then you read the digits , the the strings of numbers as individual digits . professor b : mm - hmm . phd e : so you do n't say `` eight hundred and fifty `` , you say `` eight five oh `` , and so forth . professor b : ok . ok . so can maybe can i t maybe start then ?"
}