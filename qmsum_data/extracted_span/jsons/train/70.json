{
    "query": "<s> summarize the discussion on support vector machines to map mfcc to phonological features",
    "answer": "grad a : so for my class project i 'm um i 'm tinkering with uh support vector machines ? something that we learned in class , and uh um basically just another method for doing classification . and so i 'm gon na apply that to um compare it with the results by um king and taylor who did um these um using recurrent neural nets , they recognized um a set of phonological features um and made a mapping from the mfcc 's to these phonological features , so i 'm gon na do a similar thing with with support vector machines and see if phd e : so what 's the advantage of support vector machines ? what grad a : um . so , support vector machines are are good with dealing with a less amount of data and um so if you if you give it less data it still does a reasonable job in learning the the patterns . um and um professor c : i guess it yeah , they 're sort of succinct , and and they uh grad a : yeah . phd e : does there some kind of a distance metric that they use or how do they for cla what do they do for classification ? grad a : um . right . so , the the simple idea behind a support vector machine is um , you have you have this feature space , right ? and then it finds the optimal separating plane , um between these two different um classes , phd e : mm - hmm . mm - hmm . mm - hmm . grad a : and um and so um , what it i at the end of the day , what it actually does is it picks those examples of the features that are closest to the separating boundary , and remembers those phd e : mm - hmm . grad a : and and uses them to recreate the boundary for the test set . so , given these um these features , or or these these examples , um , critical examples , which they call support f support vectors , then um given a new example , if the new example falls um away from the boundary in one direction then it 's classified as being a part of this particular class and otherwise it 's the other class . phd e : so why save the examples ? why not just save what the boundary itself is ? grad a : mm - hmm . um . hmm . let 's see . uh . yeah , that 's a good question . i yeah . professor c : that 's another way of doing it . right ? so so it i mean i i guess it 's phd e : mmm . sort of an equivalent . professor c : you know , it it goes back to nearest - neighbor sort of thing , phd e : mm - hmm . professor c : right ? um , i i if is it eh w when is nearest - neighbor good ? well , nearest - neighbor good is good if you have lots and lots of examples . um but of course if you have lots and lots of examples , then it can take a while to to use nearest - neighbor . there 's lots of look ups . so a long time ago people talked about things where you would have uh a condensed nearest - neighbor , where you would you would you would pick out uh some representative examples which would uh be sufficient to represent to to correctly classify everything that came in . phd e : oh . mm - hmm . professor c : i i think s i think support vector stuff sort of goes back to to that kind of thing . um . phd e : i see . so rather than doing nearest neighbor where you compare to every single one , you just pick a few critical ones , and"
}