{
    "query": "<s> what \u2019 s the summary of the meeting ?",
    "answer": "grad b : so i guess this is more or less now just to get you up to date , johno . this is what , uh , grad c : this is a meeting for me . grad b : um , eva , bhaskara , and i did . grad d : did you add more stuff to it ? later ? grad b : um . why ? grad d : um . i do n't know . there were , like , the you know , @ @ and all that stuff . but . i thought you you said you were adding stuff grad b : uh , no . grad d : but i do n't know . grad b : this is um , ha ! very nice . um , so we thought that , we can write up uh , an element , and for each of the situation nodes that we observed in the bayes - net ? so . what 's the situation like at the entity that is mentioned ? if we know anything about it ? is it under construction ? or is it on fire or something happening to it ? or is it stable ? and so forth , going all the way um , f through parking , location , hotel , car , restroom , @ @ { comment } riots , fairs , strikes , or disasters . grad c : so is this is a situation are is all the things which can be happening right now ? or , what is the situation type ? grad b : that 's basically just specifying the the input for the w what 's grad c : oh , i see y why are you specifying it in xml ? grad b : um . just because it forces us to be specific about the values here ? and , also , i mean , this is a what the input is going to be . right ? so , we will , uh this is a schema . this is grad c : well , yeah . i just do n't know if this is th l what the does this is what java bayes takes ? as a bayes - net spec ? grad b : no , because i mean if we i mean we 're sure gon na interface to we 're gon na get an xml document from somewhere . right ? and that xml document will say `` we are able to we were able to observe that w the element , um , @ @ { comment } of the location that the car is near . `` so that 's gon na be { comment } um . grad c : so this is the situational context , everything in it . is that what situation is short for , shi situational context ? grad b : so this is just , again , a an xml schemata which defines a set of possible , uh , permissible xml structures , which we view as input into the bayes - net . right ? grad c : and then we can r uh possibly run one of them uh transformations ? that put it into the format that the bayes n or java bayes or whatever wants ? grad b : yea - are you talking are you talking about the the structure ? grad c : well it grad b : i mean when you observe a node . grad c : when you when you say the input to the v java bayes , { comment } it takes a certain format , grad b : um - hmm . grad c : right ? which i do n't think is this . although i do n't know . grad b : no , it 's certainly not this . nuh . grad c : so you could just could n't you just run a grad b : xsl . { comment } yeah . grad c : yeah . to convert it into the java bayes for format ? grad b : that 's that 's no problem , but i even think that , um i mean , once once you have this sort of as running as a module right ? what you want is you wan na say , `` ok , give me the posterior probabilities of the go - there node , when this is happening . `` right ? when the person said this , the car is there , it 's raining , and this is happening . and with this you can specify the what 's happening in the situation , and what 's happening with the user . so we get after we are done , through the situation we get the user vector . so , this is a grad c : so this is just a specification of all the possible inputs ? grad b : yep . and , all the possible outputs , too . so , we have , um , for example , the , uh , go - there decision node which has two elements , going - there and its posterior probability , and not - going - there and its posterior probability , because the output is always gon na be all the decision nodes and all the the a all the posterior probabilities for all the values . grad c : and then we would just look at the , eh , struct that we wan na look at in terms of if if we 're only asking about one of the so like , if i 'm just interested in the going - there node , i would just pull that information out of the struct that gets return that would that java bayes would output ? grad b : um , pretty much , yes , but i think it 's a little bit more complex . as , if i understand it correctly , it always gives you all the posterior probabilities for all the values of all decision nodes . so , when we input something , we always get the , uh , posterior probabilities for all of these . right ? so there is no way of telling it t not to tell us about the eva values . grad c : yeah , wait i agree , that 's yeah , use oh , uh yeah , ok . grad b : so so we get this whole list of of , um , things , and the question is what to do with it , what to hand on , how to interpret it , in a sense . so y you said if you `` i 'm only interested in whether he wants to go there or not `` , then i just look at that node , look which one grad c : look at that struct in the output , right ? grad b : look at that struct in the the output , even though i would n't call it a `` struct `` . but . grad c : well i well , it 's an xml structure that 's being res returned , grad b : oh . mm - hmm . grad c : right ? grad b : so every part of a structure is a `` struct `` . yeah . grad c : yeah , i just uh i just was abbreviated it to struct in my head , and started going with that . grad b : that element or object , i would say . grad c : not a c struct . that 's not what i was trying to k grad b : yeah . grad c : though yeah . grad b : ok . and , um , the reason is why i think it 's a little bit more complex or why why we can even think about it as an interesting problem in and of itself is um . so . the , uh let 's look at an example . grad c : well , w would n't we just take the structure that 's outputted and then run another transformation on it , that would just dump the one that we wanted out ? grad b : yeah . w we 'd need to prune . right ? throw things away . grad c : well , actually , you do n't even need to do that with xml . d ca n't you just look at one specific grad b : yeah , exactly . the @ @ { comment } xerxes allows you to say , u `` just give me the value of that , and that , and that . `` but , we do n't really know what we 're interested in before we look at the complete at at the overall result . so the person said , um , `` where is x ? `` and so , we want to know , um , is does he want info ? o on this ? or know the location ? or does he want to go there ? let 's assume this is our our question . grad c : sure . grad b : nuh ? so . um . do this in perl . so we get ok . let 's assume this is the output . so . we should con be able to conclude from that that i mean . it 's always gon na give us a value of how likely we think i it is that he wants to go there and does n't want to go there , or how likely it is that he wants to get information . but , maybe w we should just reverse this to make it a little bit more delicate . so , does he wan na know where it is ? or does he wan na go there ? grad c : he wants to know where it is . grad b : right . i i i tend to agree . and if it 's if grad c : well now , y i mean , you could grad b : and i if there 's sort of a clear winner here , and , um and this is pretty , uh indifferent , then we then we might conclude that he actually wants to just know where , uh t uh , he does want to go there . grad c : uh , out of curiosity , is there a reason why we would n't combine these three nodes ? into one smaller subnet ? that would just basically be the question for we have `` where is x ? `` is the question , right ? that would just be info - on or location ? based upon grad b : or go - there . a lot of people ask that , if they actually just wan na go there . people come up to you on campus and say , `` where 's the library ? `` you 're gon na say y you 're gon na say , g `` go down that way . `` you 're not gon na say `` it 's it 's five hundred yards away from you `` or `` it 's north of you `` , or `` it 's located `` grad c : well , i mean but the there 's so you just have three decisions for the final node , that would link thes these three nodes in the net together . grad b : um . i do n't know whether i understand what you mean . but . again , in this given this input , we , also in some situations , may wan na postulate an opinion whether that person wants to go there now the nicest way , use a cab , or so s wants to know it wants to know where it is because he wants something fixed there , because he wants to visit t it or whatever . so , it n i mean a all i 'm saying is , whatever our input is , we 're always gon na get the full output . and some some things will always be sort of too not significant enough . grad c : wha or i or i it 'll be tight . you wo n't it 'll be hard to decide . but i mean , i guess i guess the thing is , uh , this is another , smaller , case of reasoning in the case of an uncertainty , which makes me think bayes - net should be the way to solve these things . so if you had if for every construction , right ? you could say , `` well , there here 's the where - is construction . `` and for the where - is construction , we know we need to l look at this node , that merges these three things together grad b : mm - hmm . grad c : as for th to decide the response . and since we have a finite number of constructions that we can deal with , we could have a finite number of nodes . grad b : ok . mm - hmm . grad c : say , if we had to y deal with arbitrary language , it would n't make any sense to do that , because there 'd be no way to generate the nodes for every possible sentence . grad b : mm - hmm . grad c : but since we can only deal with a finite amount of stuff grad b : so , basically , the idea is to f to feed the output of that belief - net into another belief - net . grad c : yeah , so basically take these three things and then put them into another belief - net . grad b : but , why why why only those three ? why not the whol grad c : well , i mean , d for the where - is question . so we 'd have a node for the where - is question . grad b : yeah . but we believe that all the decision nodes are can be relevant for the where - is , and the where how - do - i - get - to or the tell - me - something - about . grad c : you can come in if you want . grad b : yes , it is allowed . grad c : as long as y you 're not wearing your h your h headphones . well , i do i see , i do n't know if this is a good idea or not . i 'm just throwing it out . but uh , it seems like we could have i mea or uh we could put all of the all of the r information that could also be relevant into the where - is node answer grad b : mm - hmm . yep . grad c : node thing stuff . and uh grad b : i mean let 's not forget we 're gon na get some very strong input from these sub dis from these discourse things , right ? so . `` tell me the location of x . `` nuh ? or `` where is x located at ? `` grad c : yeah , i know , but the bayes - net would be able to the weights on the on the nodes in the bayes - net would be able to do all that , grad b : mm - hmm . grad c : would n't it ? here 's a k oh ! oh , i 'll wait until you 're plugged in . oh , do n't sit there . sit here . you know how you do n't like that one . it 's ok . that 's the weird one . that 's the one that 's painful . that hurts . it hurts so bad . i 'm h i 'm happy that they 're recording that . that headphone . the headphone that you have to put on backwards , with the little little thing and the little little foam block on it ? it 's a painful , painful microphone . grad b : i think it 's th called `` the crown `` . grad c : the crown ? grad d : what ? grad b : yeah , versus `` the sony `` . grad a : the crown ? is that the actual name ? ok . grad b : mm - hmm . the manufacturer . grad c : i do n't see a manufacturer on it . oh , wait , here it is . h this thingy . yeah , it 's `` the crown `` . the crown of pain ! grad b : you 're on - line ? grad c : are you are your mike o is your mike on ? grad a : indeed . grad c : ok . so you 've been working with these guys ? you know what 's going on ? grad a : yes , i have . and , i do . yeah , alright . s so where are we ? grad c : excellent ! grad b : we 're discussing this . grad a : i do n't think it can handle french , but anyway . grad b : so . assume we have something coming in . a person says , `` where is x ? `` , and we get a certain we have a situation vector and a user vector and everything is fine ? an - an and and our and our grad c : did you just sti did you just stick the m the the the microphone actually in the tea ? grad b : and , um , grad a : i 'm not drinking tea . what are you talking about ? grad c : oh , yeah . sorry . grad b : let 's just assume our bayes - net just has three decision nodes for the time being . these three , he wants to know something about it , he wants to know where it is , he wants to go there . grad c : in terms of , these would be wha how we would answer the question where - is , right ? we u this is i that 's what you s it seemed like , explained it to me earlier grad b : yeah , but , mmm . grad c : w we we 're we wan na know how to answer the question `` where is x ? `` grad b : yeah . no , i can i can do the timing node in here , too , and say `` ok . `` grad c : well , yeah , but in the s uh , let 's just deal with the s the simple case of we 're not worrying about timing or anything . we just want to know how we should answer `` where is x ? `` grad b : ok . and , um , ok , and , go - there has two values , right ? , go - there and not - go - there . let 's assume those are the posterior probabilities of that . grad a : mm - hmm . grad b : info - on has true or false and location . so , he wants to know something about it , and he wants to know something he wants to know where - it - is , grad a : excuse me . grad b : has these values . and , um , grad c : oh , i see why we ca n't do that . grad b : and , um , in this case we would probably all agree that he wants to go there . our belief - net thinks he wants to go there , grad a : yeah . grad b : right ? grad a : mm - hmm . grad b : in the , uh , whatever , if we have something like this here , and this like that and maybe here also some grad a : you should probably { comment } make them out of yeah . grad b : something like that , grad c : well , it grad b : then we would guess , `` aha ! he , our belief - net , { comment } has s stronger beliefs that he wants to know where it is , than actually wants to go there . `` right ? grad c : that it does n't this assume , though , that they 're evenly weighted ? grad d : true . grad c : like i guess they are evenly weighted . grad a : the different decision nodes , you mean ? grad c : yeah , the go - there , the info - on , and the location ? grad a : well , d yeah , this is making the assumption . yes . grad c : like grad b : what do you mean by `` differently weighted `` ? they do n't feed into anything really anymore . grad a : but i mean , why do we grad c : or i jus grad a : if we trusted the go - there node more th much more than we trusted the other ones , then we would conclude , even in this situation , that he wanted to go there . so , in that sense , we weight them equally right now . grad b : ok . makes sense . yeah . but grad c : so the but i guess the k the question that i was as er wondering or maybe robert was proposing to me is how do we d make the decision on as to which one to listen to ? grad a : yeah , so , the final d decision is the combination of these three . so again , it 's it 's some kind of , uh grad c : bayes - net . grad a : yeah , sure . grad c : ok so , then , the question i so then my question is t to you then , would be so is the only r reason we can make all these smaller bayes - nets , because we know we can only deal with a finite set of constructions ? cuz oth if we 're just taking arbitrary language in , we could n't have a node for every possible question , you know ? grad a : a decision node for every possible question , you mean ? grad c : well , i like , in the case of yeah . in the ca any piece of language , we would n't be able to answer it with this system , b if we just h cuz we would n't have the correct node . basically , w what you 're s proposing is a n where - is node , right ? grad a : yeah . grad c : and and if we and if someone says , you know , uh , something in mandarin to the system , we 'd - would n't know which node to look at to answer that question , grad a : so is yeah . yeah . grad c : right ? so , but but if we have a finite what ? grad b : i do n't see your point . what what what i am thinking , or what we 're about to propose here is we 're always gon na get the whole list of values and their posterior probabilities . and now we need an expert system or belief - net or something that interprets that , that looks at all the values and says , `` the winner is timing . now , go there . `` `` uh , go there , timing , now . `` or , `` the winner is info - on , function - off . `` so , he wants to know something about it , and what it does . nuh ? uh , regardless of of of the input . wh - regardle grad c : yeah , but but how does the expert but how does the expert system know how who which one to declare the winner , if it does n't know the question it is , and how that question should be answered ? grad b : based on the k what the question was , so what the discourse , the ontology , the situation and the user model gave us , we came up with these values for these decisions . grad c : yeah i know . but how do we weight what we get out ? as , which one i which ones are important ? so my i so , if we were to it with a bayes - net , we 'd have to have a node for every question that we knew how to deal with , that would take all of the inputs and weight them appropriately for that question . grad b : mm - hmm . grad c : does that make sense ? yay , nay ? grad a : um , i mean , are you saying that , what happens if you try to scale this up to the situation , or are we just dealing with arbitrary language ? grad c : we grad a : is that your point ? grad c : well , no . i i guess my question is , is the reason that we can make a node f or ok . so , lem me see if i 'm confused . are we going to make a node for every question ? does that make sense ? grad a : for every question ? grad c : or not . grad a : like grad c : every construction . grad a : hmm . i do n't not necessarily , i would think . i mean , it 's not based on constructions , it 's based on things like , uh , there 's gon na be a node for go - there or not , and there 's gon na be a node for enter , view , approach . grad c : wel w ok . so , someone asked a question . grad a : yeah . grad c : how do we decide how to answer it ? grad b : well , look at look face yourself with this pr question . you get this you 'll have y this is what you get . and now you have to make a decision . what do we think ? what does this tell us ? and not knowing what was asked , and what happened , and whether the person was a tourist or a local , because all of these factors have presumably already gone into making these posterior probabilities . what what we need is a just a mechanism that says , `` aha ! there is `` grad c : yeah . i just do n't think a `` winner - take - all `` type of thing is the grad a : i mean , in general , like , we wo n't just have those three , right ? we 'll have , uh , like , many , many nodes . so we have to , like so that it 's no longer possible to just look at the nodes themselves and figure out what the person is trying to say . grad b : yep . because there are interdependencies , right ? the uh uh , no . so if if for example , the go - there posterior possibility is so high , um , uh , w if it 's if it has reached reached a certain height , then all of this becomes irrelevant . so . if even if if the function or the history or something is scoring pretty good on the true node , true value grad c : wel i do n't know about that , cuz that would suggest that i mean grad b : he wants to go there and know something about it ? grad c : do they have to be mutual yeah . do they have to be mutually exclusive ? grad b : i think to some extent they are . or maybe they 're not . grad c : cuz i , uh the way you describe what they meant , they were n't mutu uh , they did n't seem mutually exclusive to me . grad b : well , if he does n't want to go there , even if the enter posterior proba so . go - there is no . enter is high , and info - on is high . grad c : well , yeah , just out of the other three , though , that you had in the those three nodes . the - d they did n't seem like they were mutually exclusive . grad b : no , there 's no . but it 's through the grad c : so th s so , yeah , but some so , some things would drop out , and some things would still be important . grad b : mm - hmm . grad c : but i guess what 's confusing me is , if we have a bayes - net to deal w another bayes - net to deal with this stuff , grad a : mm - hmm . grad c : you know , uh , is the only reason ok , so , i guess , if we have a ba - another bayes - net to deal with this stuff , the only r reason we can design it is cuz we know what each question is asking ? grad a : yeah . i think that 's true . grad c : and then , so , the only reason way we would know what question he 's asking is based upon oh , so if let 's say i had a construction parser , and i plug this in , i would know what each construction the communicative intent of the construction was grad a : mm - hmm . grad c : and so then i would know how to weight the nodes appropriately , in response . so no matter what they said , if i could map it onto a where - is construction , i could say , `` ah ! grad a : ge mm - hmm . grad c : well the the intent , here , was where - is `` , grad a : ok , right . grad c : and i could look at those . grad a : yeah . yes , i mean . sure . you do need to know i mean , to have that kind of information . grad b : hmm . yeah , i 'm also agreeing that a simple pru { comment } take the ones where we have a clear winner . forget about the ones where it 's all sort of middle ground . prune those out and just hand over the ones where we have a winner . yeah , because that would be the easiest way . we just compose as an output an xml mes message that says . `` go there now . `` `` enter historical information . `` and not care whether that 's consistent with anything . right ? but in this case if we say , `` definitely he does n't want to go there . he just wants to know where it is . `` or let 's call this this `` look - at - h `` he wants to know something about the history of . so he said , `` tell me something about the history of that . `` now , the e but for some reason the endpoint - approach gets a really high score , too . we ca n't expect this to be sort of at o point { comment } three , three , three , o point , three , three , three , o point , three , three , three . right ? somebody needs to zap that . you know ? or know there needs to be some knowledge that grad c : we yeah , but , the bayes - net that would merge i just realized that i had my hand in between my mouth and my micr er , my and my microphone . so then , the bayes - net that would merge there , that would make the decision between go - there , info - on , and location , would have a node to tell you which one of those three you wanted , and based upon that node , then you would look at the other stuff . grad b : yep . yep . grad c : i mean , it i does that make sense ? grad b : yep . it 's sort of one of those , that 's it 's more like a decision tree , if if you want . you first look o at the lowball ones , grad c : yeah , i grad b : and then grad c : yeah , i did n't intend to say that every possible ok . there was a confusion there , k i did n't intend to say every possible thing should go into the bayes - net , because some of the things are n't relevant in the bayes - net for a specific question . like the endpoint is not necessarily relevant in the bayes - net for where - is until after you 've decided whether you wan na go there or not . grad b : mm - hmm . grad a : right . grad c : show us the way , bhaskara . grad a : i guess the other thing is that um , yeah . i mean , when you 're asked a specific question and you do n't even like , if you 're asked a where - is question , you may not even look like , ask for the posterior probability of the , uh , eva node , right ? cuz , that 's what i mean , in the bayes - net you always ask for the posterior probability of a specific node . so , i mean , you may not even bother to compute things you do n't need . grad b : um . are n't we always computing all ? grad a : no . you can compute , uh , the posterior probability of one subset of the nodes , given some other nodes , but totally ignore some other nodes , also . basically , things you ignore get marginalized over . grad b : yeah , but that 's that 's just shifting the problem . then you would have to make a decision , grad a : yeah . so you have to make grad b : `` ok , if it 's a where - is question , which decision nodes do i query ? `` grad a : yeah . yes . but i would think that 's what you want to do . grad b : that 's un grad a : right ? grad d : well , eventually , you still have to pick out which ones you look at . grad b : yeah . grad d : so it 's pretty much the same problem , grad b : yeah it 's it 's it 's apples and oranges . grad d : is n't it ? grad b : nuh ? i mean , maybe it does make a difference in terms of performance , computational time . grad a : mm - hmm . grad b : so either you always have it compute all the posterior possibilities for all the values for all nodes , and then prune the ones you think that are irrelevant , or you just make a p @ @ { comment } a priori estimate of what you think might be relevant and query those . grad a : yeah . grad c : so basically , you 'd have a decision tree query , go - there . if k if that 's false , query this one . if that 's true , query that one . and just basically do a binary search through the ? grad a : i do n't know if it would necessarily be that , uh , complicated . but , uh i mean , it w grad c : well , in the case of go - there , it would be . in the case cuz if you needed an if y if go - there was true , you 'd wan na know what endpoint was . and if it was false , you 'd wan na d look at either lo - income info - on or history . grad a : yeah . that 's true , i guess . yeah , so , in a way you would have that . grad c : also , i 'm somewhat boggled by that hugin software . grad a : ok , why 's that ? grad c : i ca n't figure out how to get the probabilities into it . like , i 'd look at grad a : mm - hmm . grad c : it 's somewha it 's boggling me . grad a : ok . alright . well , hopefully it 's fixable . it 's there 's a grad c : oh yeah , yeah . i d i just think i have n't figured out what the terms in hugin mean , versus what java bayes terms are . grad b : um , by the way , are do we know whether jerry and nancy are coming ? grad a : so we can figure this out . grad b : or ? grad a : they should come when they 're done their stuff , basically , whenever that is . so . grad c : what d what do they need to do left ? grad a : um , i guess , jerry needs to enter marks , but i do n't know if he 's gon na do that now or later . but , uh , if he 's gon na enter marks , it 's gon na take him awhile , i guess , and he wo n't be here . grad c : and what 's nancy doing ? grad a : nancy ? um , she was sorta finishing up the , uh , calculation of marks and assigning of grades , but i do n't know if she should be here . well or , she should be free after that , so assuming she 's coming to this meeting . i do n't know if she knows about it . grad c : she 's on the email list , right ? grad a : is she ? ok . grad b : mm - hmm . ok . because basically , what where we also have decided , prior to this meeting is that we would have a rerun of the three of us sitting together sometime this week again and finish up the , uh , values of this . so we have , uh believe it or not , we have all the bottom ones here . grad c : well , i grad d : you added a bunch of nodes , for ? grad b : yep . we we we have actually what we have is this line . right ? grad c : uh , what do the , uh , structures do ? so the the the for instance , this location node 's got two inputs , grad a : four inputs . grad c : that one you grad b : four . grad a : those are the bottom things are inputs , also . grad c : oh , i see . grad a : yeah . grad c : ok , that was ok . that makes a lot more sense to me now . cuz i thought it was like , that one in stuart 's book about , you know , the grad a : alarm in the dog ? grad c : u yeah . grad a : yeah . grad c : or the earthquake and the alarm . grad a : sorry . yeah , i 'm confusing two . grad c : yeah , there 's a dog one , too , but that 's in java bayes , grad a : right . grad c : is n't it ? grad a : maybe . grad c : but there 's something about bowel problems or something with the dog . grad a : yeah . grad b : and we have all the top ones , all the ones to which no arrows are pointing . what we 're missing are the these , where arrows are pointing , where we 're combining top ones . so , we have to come up with values for this , and this , this , this , and so forth . and maybe just fiddle around with it a little bit more . and , um . and then it 's just , uh , edges , many of edges . and , um , we wo n't { comment } meet next monday . so . grad c : cuz of memorial day ? grad a : we 'll meet next tuesday , i guess . grad b : yep . yeah . grad c : when 's jerry leaving for italia ? grad b : on on friday . grad a : which friday ? grad b : this this friday . grad d : oh . this friday ? grad b : this friday . grad c : as in , four days ? or , three days ? grad a : is he how long is he gone for ? grad b : two weeks . grad a : italy , huh ? what 's , uh what 's there ? grad b : well , it 's a country . buildings . people . grad a : pasta . grad c : but it 's not a conference or anything . he 's just visiting . grad a : right . just visiting . grad b : vacation . grad a : it 's a pretty nice place , in my brief , uh , encounter with it . grad b : do you guys oh , yeah . so . part of what we actually want to do is sort of schedule out what we want to surprise him with when when he comes back . um , so grad c : oh , i think we should disappoint him . grad b : yeah ? you or have a finished construction parser and a working belief - net , and uh grad c : that would n't be disappointing . i think w we should do absolutely no work for the two weeks that he 's gone . grad b : well , that 's actually what i had planned , personally . i had i i had sort of scheduled out in my mind that you guys do a lot of work , and i do nothing . and then , i sort of grad c : oh , yeah , that sounds good , too . grad b : sort of bask in in your glory . but , uh , i do you guys have any vacation plans , because i myself am going to be , um , gone , but this is actually not really important . just this weekend we 're going camping . grad c : yeah , i 'm wan na be this gone this weekend , too . grad b : ah . but we 're all going to be here on tuesday again ? looks like it ? grad d : yeah . grad b : ok , then . let 's meet meet again next tuesday . and , um , finish up this bayes - net . and once we have finished it , i guess we can , um and that 's going to be more just you and me , because bhaskara is doing probabilistic , recursive , structured , object - oriented , uh , grad c : killing machines ! grad b : reasoning machines . and , um grad c : killing , reasoning . what 's the difference ? grad d : wait . so you 're saying , next tuesday , is it the whole group meeting , or just us three working on it , or or ? grad b : uh . the whole group . and we present our results , our final , definite grad d : so , when you were saying we need to do a re - run of , like grad a : h what ? grad d : what like , just working out the rest of the grad b : yeah . we should do this th the upcoming days . grad d : this week ? grad b : so , this week , yeah . grad c : when you say , `` the whole group `` , you mean the four of us , and keith ? grad b : and , ami might . grad c : ami might be here , and it 's possible that nancy 'll be here ? so , yeah . grad b : because , th you know , once we have the belief - net done grad c : you 're just gon na have to explain it to me , then , on tuesday , how it 's all gon na work out . you know . grad b : we will . ok . because then , once we have it sort of up and running , then we can start you know , defining the interfaces and then feed stuff into it and get stuff out of it , and then hook it up to some fake construction parser and grad c : that you will have in about nine months or so . grad b : yeah . grad c : yeah . grad b : and , um , grad c : the first bad version 'll be done in nine months . grad b : yeah , i can worry about the ontology interface and you can keith can worry about the discourse . i mean , this is pretty um , i mean , i i i hope everybody uh knows that these are just going to be uh dummy values , right ? grad a : which grad b : where the grad a : which ones ? grad b : s so so if the endpoint if the go - there is yes and no , then go - there - discourse will just be fifty - fifty . right ? grad a : um , what do you mean ? if the go - there says no , then the go - there is grad d : i do n't get it . grad a : i do n't u understand . like , the go - there depends on all those four things . yeah . grad b : but , what are the values of the go - there - discourse ? grad a : well , it depends on the situation . if the discourse is strongly indicating that grad b : yeah , but , uh , we have no discourse input . grad a : oh , i see . the d see , uh , specifically in our situation , d and o are gon na be , uh yeah . sure . so , whatever . grad d : so , so far we have is that what the keith node is ? ok . and you 're taking it out ? for now ? grad b : well , this is d grad d : or ? grad b : ok , this , i can i can get it in here . grad d : all the d 's are grad b : i can get it in here , so th we have the , uh , um , sk let 's let 's call it `` keith - johno grad a : johno ? grad b : node `` . there is an h { comment } somewhere printed . grad c : there you go . grad a : yeah . people have the same problem with my name . grad b : yeah . grad a : oops . grad b : and , um , grad c : does th th does the h go b before the a or after the a ? grad a : oh , in my name ? before the a . grad c : yeah . ok , good . cuz you kn when you said people have the same problem , i thought cuz my h goes after the uh e e e the v grad a : people have the inverse problem with my name . grad c : ok . i always have to check , every time y i send you an email , { comment } a past email of yours , { comment } to make sure i 'm spelling your name correctly . grad a : yeah . that 's good . grad c : i worry about you . grad a : i appreciate that . grad b : but , when you abbreviate yourself as the `` basman `` , you do n't use any h 's . grad a : `` basman `` ? yeah , it 's because of the chessplayer named michael basman , who is my hero . grad c : you 're a geek . it 's o k . i how do you pronou how do you pronounce your name ? grad a : not eva ? grad d : yeah . grad c : what if i were what if i were to call you eva ? grad d : i 'd probably still respond to it . i 've had people call me eva , but i do n't know . grad c : no , not just eva , eva . like if i u take the v and s pronounce it like it was a german v ? grad b : which is f . grad c : yeah . grad d : um , no idea then . grad b : voiced . grad d : what ? grad c : it sounds like an f . grad d : i grad c : there 's also an f in german , grad b : well , it 's just the difference between voiced and unvoiced . grad c : which is why i yeah . as long as that 's o k . i mean , i might slip out and say it accidentally . that 's all i 'm saying . grad d : that 's fine . grad a : yeah . it does n't matter what those nodes are , anyway , because we 'll just make the weights `` zero `` for now . grad b : yep . we 'll make them zero for now , because it who who knows what they come up with , what 's gon na come in there . ok . and , um , then should we start on thursday ? and not meet tomorrow ? grad a : sure . grad b : ok . i 'll send an email , make a time suggestion . grad c : wait , maybe it 's ok , so that that that we can that we have one node per construction . cuz even in people , like , they do n't know what you 're talking about if you 're using some sort of strange construction . grad b : yeah , they would still c sort of get the closest , best fit . grad c : well , yeah , but i mean , the uh , i mean , that 's what the construction parser would do . grad b : mm - hmm . grad c : uh , i mean , if you said something completely arbitrary , it would f find the closest construction , right ? but if you said something that was completel er h theoretically the construction parser would do that but if you said something for which there was no construction whatsoever , n people would n't have any idea what you were talking about . grad b : mm - hmm . grad c : like `` bus dog fried egg . `` i mean . you know . grad b : or , if even something chinese , for example . grad c : or , something in mandarin , yeah . or cantonese , as the case may be . what do you think about that , bhaskara ? grad a : i mean well but how many constructions do could we possibly have nodes for ? grad c : in this system , or in r grad a : no , we . like , when people do this kind of thing . grad c : oh , when p how many constructions do people have ? grad a : yeah . grad c : i have not { comment } the slightest idea . grad a : is it considered to be like in are they considered to be like very , uh , sort of s abstract things ? grad c : every noun is a construction . grad a : ok , so it 's like in the thousands . grad c : the yeah . any any form - meaning pair , to my understanding , is a construction . and form u starts at the level of noun or actually , maybe even sounds . grad b : phoneme . yep . grad c : yeah . and goes upwards until you get the ditransitive construction . and then , of course , the c i guess , maybe there can be the can there be combinations of the dit grad a : discourse - level constructions . grad c : yeah . the `` giving a speech `` construction , grad b : rhetorical constructions . yeah . but , i mean , you know , you can probably count count the ways . i mean . grad c : it 's probab yeah , i would s definitely say it 's finite . grad b : yeah . grad c : and at least in compilers , that 's all that really matters , as long as your analysis is finite . grad a : how 's that ? how it can be finite , again ? grad c : nah , i ca n't think of a way it would be infinite . grad b : well , you can come up with new constructions . grad c : yeah . { comment } if the if your if your brain was totally non - deterministic , then perhaps there 's a way to get , uh , infin an infinite number of constructions that you 'd have to worry about . grad a : but , i mean , in the practical sense , it 's impossible . grad c : right . cuz if we have a fixed number of neurons ? grad a : yeah . grad c : so the best - case scenario would be the number of constructions or , the worst - case scenario is the number of constructions equals the number of neurons . grad a : well , two to the power of the number of neurons . grad c : right . but still finite . no , wait . not necessarily , is it ? we can end the meeting . i just ca n't you use different var different levels of activation ? across , uh lots of different neurons , to specify different values ? grad b : mm - hmm . grad a : um , yeah , but there 's , like , a certain level of grad c : there 's a bandwidth issue , grad a : bandw - yeah , so you ca n't do better than something . grad c : right ? yeah ."
}