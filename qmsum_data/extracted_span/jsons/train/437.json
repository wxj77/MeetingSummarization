{
    "query": "<s> summarize the discussion on the mean log magnitude spectral subtraction",
    "answer": "grad e : uh - huh . so that was encouraging . and , um , that that um , that 's encouraging for for the idea of using it in an interactive system like and , um , another issue i 'm i 'm thinking about is in the smartkom system . so say twe twelve seconds in the earlier test seemed like a good length of time , but what happens if you have less than twelve seconds ? and , um so i w bef before , um back in may , i did some experiments using , say , two seconds , or four seconds , or six seconds . in those i trained the models using mean subtraction with the means calculated over two seconds , or four seconds , or six seconds . and , um , here , i was curious , what if i trained the models using twelve seconds but i f i gave it a situation where the test set i was subtracted using two seconds , or four seconds , or six seconds . and , um so i did that for about three different conditions . and , um i mean , i th i think it was , um , four se i think i think it was , um , something like four seconds and , um , six seconds , and eight seconds . something like that . and it seems like it it it hurts compared to if you actually train the models { comment } using th that same length of time but it it does n't hurt that much . um , u usually less than point five percent , although i think i did see one where it was a point eight percent or so rise in word error rate . but this is , um , w where , um , even if i train on the , uh , model , and mean subtracted it with the same length of time as in the test , it the word error rate is around , um , ten percent or nine percent . so it does n't seem like that big a d a difference . professor c : but it but looking at it the other way , is n't it what you 're saying that it did n't help you to have the longer time for training , if you were going to have a short time for grad e : that that 's true . um , professor c : i mean , why would you do it , if you knew that you were going to have short windows in testing . phd a : yeah , it seems like for your i mean , in normal situations you would never get twelve seconds of speech , right ? i 'm not e u phd b : you need twelve seconds in the past to estimate , right ? or l or you 're looking at six sec seconds in future and six in professor c : yeah . grad e : um , t twelve s professor c : no , total . grad e : n n uh for the test it 's just twelve seconds in the past . phd b : no , it 's all oh , ok . phd a : is this twelve seconds of uh , regardless of speech or silence ? or twelve seconds of speech ? grad e : of of speech . phd b : mm - hmm . professor c : the other thing , um , which maybe relates a little bit to something else we 've talked about in terms of windowing and so on is , that , um , i wonder if you trained with twelve seconds , and then when you were two seconds in you used two seconds , and when you were four seconds in , you used four seconds , and when you were six and you basically build up to the twelve seconds . so that if you have very long utterances you have the best , grad e : yeah . professor c : but if you have shorter utterances you use what you can . grad e : right . and that 's actually what we 're planning to do in professor c : ok . yeah . grad e : but s so i g so i guess the que the question i was trying to get at with those experiments is , `` does it matter what models you use ? does it matter how much time y you use to calculate the mean when you were , um , tra doing the training data ? `` professor c : right . but i mean the other thing is that that 's i mean , the other way of looking at this , going back to , uh , mean cepstral subtraction versus rasta kind of things , is that you could look at mean cepstral subtraction , especially the way you 're doing it , uh , as being a kind of filter . and so , the other thing is just to design a filter . you know , basically you 're you 're you 're doing a high - pass filter or a band - pass filter of some sort and and just design a filter . and then , you know , a filter will have a certain behavior and you loo can look at the start up behavior when you start up with nothing . grad e : mm - hmm . professor c : and and , you know , it will , uh , if you have an iir filter for instance , it will , um , uh , not behave in the steady - state way that you would like it to behave until you get a long enough period , but , um , uh , by just constraining yourself to have your filter be only a subtraction of the mean , you 're kind of , you know , tying your hands behind your back because there 's filters have all sorts of be temporal and spectral behaviors . grad e : mm - hmm . professor c : and the only thing , you know , consistent that we know about is that you want to get rid of the very low frequency component . phd b : but do you really want to calculate the mean ? and you neglect all the silence regions { comment } or you just use everything that 's twelve seconds , and grad e : um , you do you mean in my tests so far ? phd b : ye - yeah . grad e : most of the silence has been cut out . just there 's just inter - word silences . phd b : mm - hmm . and they are , like , pretty short . shor grad e : pretty short . phd b : yeah , ok . grad e : yeah . phd b : yeah . mm - hmm . so you really need a lot of speech to estimate the mean of it . grad e : well , if i only use six seconds , it still works pretty well . phd b : yeah . yeah . uh - huh . grad e : i saw in my test before . i was trying twelve seconds cuz that was the best in my test before and that increasing past twelve seconds did n't seem to help . phd b : hmm . huh . grad e : th um , yeah , i guess it 's something i need to play with more to decide how to set that up for the smartkom system . like , may maybe if i trained on six seconds it would work better when i only had two seconds or four seconds , and professor c : yeah . yeah . and , um yeah , and again , if you take this filtering perspective and if you essentially have it build up over time . i mean , if you computed means over two and then over four , and over six , essentially what you 're getting at is a kind of , uh , ramp up of a filter anyway . and so you may may just want to think of it as a filter . but , uh , if you do that , then , um , in practice somebody using the smartkom system , one would think { comment } if they 're using it for a while , it means that their first utterance , instead of , you know , getting , uh , a forty percent error rate reduction , they 'll get a uh , over what , uh , you 'd get without this , uh , um , policy , uh , you get thirty percent . and then the second utterance that you give , they get the full you know , uh , full benefit of it if it 's this ongoing thing . phd a : oh , so you you cache the utterances ? that 's how you get your , uh"
}