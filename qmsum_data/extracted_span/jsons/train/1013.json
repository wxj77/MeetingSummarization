{
    "query": "<s> what did the professor think about the latency ?",
    "answer": "professor c : i think the best systems so , uh , everything that we did in in a way it was it was just adamantly insisting on going in with a brain damaged system , which is something actually , we 've done a lot over the last thirteen years . uh , which is we say , well this is the way we should do it . and then we do it . and then someone else does something that 's straight forward . so , w th w this was a test that largely had additive noise and we did we adde did absolutely nothing explicitly to handle ad additive noise . phd a : right . professor c : we just , uh , you know , trained up systems to be more discriminant . and , uh , we did this , uh , rasta - like filtering which was done in the log domain and was tending to handle convolutional noise . we did we actually did nothing about additive noise . so , um , the , uh , spectral sub subtraction schemes a couple places did seem to seem to do a nice job . and so , uh , we 're talking about putting putting some of that in while still keeping some of our stuff . i think you should be able to end up with a system that 's better than both but clearly the way that we 're operating for this other stuff does involved some latency to to get rid of most of that latency . to get down to forty or fifty milliseconds we 'd have to throw out most of what we 're doing . and and , uh , i do n't think there 's any good reason for it in the application actually . i mean , you 're you 're you 're speaking to a recognizer on a remote server and , uh , having a a a quarter second for some processing to clean it up . it does n't seem like it 's that big a deal . phd a : mm - hmm . professor c : these are n't large vocabulary things so the decoder should n't take a really long time , and . phd a : and i do n't think anybody 's gon na notice the difference between a quarter of a second of latency and thirty milliseconds of latency ."
}