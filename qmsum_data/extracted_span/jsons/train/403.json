{
    "query": "<s> what did the team discuss about the cheating experiment ?",
    "answer": "phd a : so , um , i mean , if your net is gon na be outputting , uh , a vector of basically of well , it 's gon na have probabilities , but let 's say that they were ones and zeros , then y and you know for each , um , i do n't know if you know this for your testing data , but if you know for your test data , you know , what the string of phones is and and you have them aligned , then you can just instead of going through the net , just create the vector for each phone and feed that in to see if that data helps . eh , eh , what made me think about this is , i was talking with hynek and he said that there was a guy at a t - andt who spent eighteen months working on a single feature . and because they had done some cheating experiments professor f : this was the guy that we were just talking a that we saw on campus . so , this was larry saul who did this did this . phd a : oh , ok . professor f : he used sonorants . phd a : right , ok , professor f : was what he was doing . phd a : right . and they they had done a cheating experiment or something , right ? professor f : yeah . phd a : and determined that professor f : he he di he did n't mention that part . phd a : well , hynek said that that , i guess before they had him work on this , they had done some experiment where if they could get that one feature right , it dramatically improved the result . professor f : but . i see . ok . phd a : so i was thinking , you know it made me think about this , that if it 'd be an interesting experiment just to see , you know , if you did get all of those right . professor f : should be . because if you get all of them in there , that defines all of the phones . so that 's that 's equivalent to saying that you 've got got all the phones right . phd a : right . professor f : so , if that does n't help , there 's phd a : yeah . professor f : although , yeah , it would be make an interesting cheating experiment because we are using it in this funny way , phd a : yeah . professor f : where we 're converting it into features . phd a : and then you also do n't know what error they 've got on the htk side . you know ? it sort of gives you your the best you could hope for , kind of . professor f : yeah . grad c : mmm . mmm , i see . phd b : the soft training of the nets still requires the vector to sum to one , though , right ? grad c : to sum up to one . phd b : so you ca n't really feed it , like , two articulatory features that are on at the same time with ones cuz it 'll kind of normalize them down to one half or something like that , for instance . phd g : but perhaps you have the choice of the final nonl grad c : right . nonlinearity ? phd g : uh , nonlinearity , yeah . is it always softmax grad c : it 's sig no , it 's actually sigmoid - x phd g : or ? yeah . grad c : for the phd g : so if you choose sigmoid it 's o it 's ok ? grad c : you , um professor f : did we just run out of disk , grad c : i think i think apparently , the , uh professor f : or ? phd b : why do n't you just choose linear ? right ? grad c : what 's that ? phd b : linear outputs ? grad c : linear outputs ? phd b : is n't that what you 'll want ? if you 're gon na do a kl transform on it . grad c : right , right . right , but during the training , we would train on sigmoid - x"
}