{
    "query": "<s> summarize the meeting",
    "answer": "professor e : let 's see . test ? test ? yeah . ok . grad a : hello ? phd b : channel one . grad a : hello ? phd c : test . professor e : i was saying hynek 'll be here next week , uh , wednesday through friday uh , through saturday , and , um , i wo n't be here thursday and friday . but my suggestion is that , uh , at least for this meeting , people should go ahead , uh , cuz hynek will be here , and , you know , we do n't have any czech accent yet , uh , as far as i know , so there we go . um . so other than reading digits , what 's our agenda ? phd f : i do n't really have , uh , anything new . been working on meeting recorder stuff . so . professor e : ok . um . do you think that would be the case for next week also ? or is is , uh ? what 's your projection on ? cuz the one thing the one thing that seems to me we really should try , if you had n't tried it before , because it had n't occurred to me it was sort of an obvious thing is , um , adjusting the , uh , sca the scaling and , uh , insertion penalty sorta stuff . phd f : i did play with that , actually , a little bit . um . what happens is , uh , when you get to the noisy stuff , you start getting lots of insertions . professor e : right . phd f : and , um , so i 've tried playing around a little bit with , um , the insertion penalties and things like that . professor e : yeah . phd f : um . i mean , it it did n't make a whole lot of difference . like for the well - matched case , it seemed like it was pretty good . um . i could do more playing with that , though . and , uh professor e : but you were looking at mel cepstrum . phd f : and see . yes . professor e : right . phd f : oh , you 're talking about for th for our features . professor e : right . so , i mean , i it it 's not the direction that you were working with that we were saying what 's the uh , what 's the best you can do with with mel cepstrum . but , they raised a very valid point , which , i guess so , to first order i mean , you have other things you were gon na do , but to first order , i would say that the conclusion is that if you , um , do , uh , some monkeying around with , uh , the exact htk training and @ @ { comment } with , uh , you know , how many states and so forth , that it it does n't particularly improve the performance . in other words , that even though it sounds pretty dumb , just applying the same number of states to everything , more or less , no matter what language , is n't so bad . right ? and i guess you had n't gotten to all the experiments you wanted to do with number of gaussians , phd f : right . professor e : but , um , let 's just if we had to if we had to draw a conclusion on the information we have so far , we 'd say something like that . right ? phd f : mm - hmm . professor e : uh , so the next question to ask , which is i think the one that that that andreas was dre addressing himself to in the lunch meeting , is , um , we 're not supposed to adjust the back - end , but anybody using the system would . phd f : yeah . professor e : so , if you were just adjusting the back - end , how much better would you do , uh , in noise ? uh , because the language scaling and insertion penalties and so forth are probably set to be about right for mel cepstrum . phd f : mm - hmm . professor e : but , um , they 're probably not at all set right for these things , particularly these things that look over , uh , larger time windows , in one way or another with with lda and klt and neural nets and all these things . in the fa past we 've always found that we had to increase the insertion penalty to to correspond to such things . so , i think that 's , uh , @ @ { comment } that 's kind of a first - order thing that that we should try . phd f : so for th so the experiment is to , um , run our front - end like normal , with the default , uh , insertion penalties and so forth , and then tweak that a little bit and see how much of a difference it makes professor e : so by `` our front - end `` i mean take , you know , the aurora - two s take some version that stephane has that is , you know , our current best version of something . phd f : if we were mm - hmm . professor e : um . i mean , y do n't wan na do this over a hundred different things that they 've tried but , you know , for some version that you say is a good one . you know ? um . how how much , uh , does it improve if you actually adjust that ? but it is interesting . you say you you have for the noisy how about for the for the mismatched or or or or the or the medium mismatched conditions ? have you ? when you adjusted those numbers for mel cepstrum , did it ? phd f : uh , i i do n't remember off the top of my head . um . yeah . i did n't even write them down . i i i do n't remember . i would need to well , i did write down , um so , when i was doing i just wrote down some numbers for the well - matched case . professor e : yeah . phd f : um . looking at the i wrote down what the deletions , substitutions , and insertions were , uh , for different numbers of states per phone . professor e : yeah . phd f : um , but , uh , that that 's all i wrote down . so . i i would yeah . i would need to do that . professor e : ok . so phd f : i can do that for next week . professor e : yeah . and , um yeah . also , eh , eh , sometimes if you run behind on some of these things , maybe we can get someone else to do it and you can supervise or something . but but i think it would be it 'd be good to know that . phd f : ok . i just need to get , um , front - end , uh , stuff from you or you point me to some files that you 've already calculated . phd b : yeah . alright . professor e : ok . uh . phd f : i probably will have time to do that and time to play a little bit with the silence model . professor e : mm - hmm . phd f : so maybe i can have that for next week when hynek 's here . professor e : yeah . phd b : mm - hmm . professor e : yeah . cuz , i mean , the the other that , in fact , might have been part of what , uh , the difference was at least part of it that that we were seeing . remember we were seeing the sri system was so much better than the tandem system . part of it might just be that the sri system , they they they always adjust these things to be sort of optimized , phd f : is there ? professor e : and phd f : i wonder if there 's anything that we could do to the front - end that would affect the insertion professor e : yes . i think you can . phd f : what could you do ? professor e : well , um uh , part of what 's going on , um , is the , uh , the range of values . so , if you have something that has a much smaller range or a much larger range , and taking the appropriate root . phd f : oh . mm - hmm . professor e : you know ? if something is kind of like the equivalent of a bunch of probabilities multiplied together , you can take a root of some sort . if it 's like seven probabilities together , you can take the seventh root of it or something , or if it 's in the log domain , divide it by seven . phd f : mm - hmm . professor e : but but , um , that has a similar effect because it changes the scale of the numbers of the differences between different candidates from the acoustic model phd f : oh , right . professor e : as opposed to what 's coming from the language model . phd f : so that w right . so , in effect , that 's changing the value of your insertion penalty . professor e : yeah . i mean , it 's more directly like the the language scaling or the , uh the model scaling or acoustic scaling , phd f : that 's interesting . professor e : but you know that those things have kind of a similar effect to the insertion penalty phd f : mm - hmm . professor e : anyway . they 're a slightly different way of of handling it . phd f : right . professor e : so , um phd f : so if we know what the insertion penalty is , then we can get an idea about what range our number should be in , professor e : i think so . phd f : so that they match with that . professor e : yeah . yeah . so that 's why i think that 's another reason other than curiosity as to why i it would in fact be kinda neat to find out if we 're way off . i mean , the other thing is , are are n't we seeing ? y y phd f : mm - hmm . professor e : i 'm sure you 've already looked at this bu in these noisy cases , are ? we are seeing lots of insertions . right ? the insertion number is quite high ? phd b : yeah . professor e : i know the vad takes pre care of part of that , phd f : yeah . phd b : yeah . professor e : but phd f : i 've seen that with the mel cepstrum . i do n't i do n't know about the aurora front - end , but phd b : i think it 's much more balanced with , uh when the front - end is more robust . yeah . i could look at it at this . yeah . mm - hmm . professor e : yeah . wha - what 's a typical number ? phd b : i do n't i do n't know . professor e : do we ? oh , you oh , you do n't know . phd b : i do n't have this in professor e : ok . i 'm sure it 's more balanced , phd b : mm - hmm . professor e : but it it it would n't surprise me if there 's still phd b : mm - hmm . professor e : i mean , in in the the the old systems we used to do , i i uh , i remember numbers kind of like insertions being half the number of deletions , as being and both numbers being tend to be on the small side comparing to to , uh , substitutions . phd b : mm - hmm . phd f : well , this the whole problem with insertions was what i think , um , we talked about when the guy from ogi came down that one time and and that was when people were saying , well we should have a , uh , uh , voice activity detector professor e : right . phd f : that , because all that stuff { comment } that we 're getting thr the silence that 's getting through is causing insertions . so . professor e : right . phd f : i 'll bet you there 's still a lot of insertions . phd b : mm - hmm . professor e : yeah . and it may be less of a critical thing . i mean , the fact that some get by may be less of a critical thing if you , uh , get things in the right range . phd f : mm - hmm . professor e : so , i mean , the insertions is is a symptom . it 's a symptom that there 's something , uh , wrong with the range . phd f : right . professor e : but there 's uh , your your your substitutions tend to go up as well . so , uh , i i i think that , phd f : mm - hmm . professor e : uh , the most obvious thing is just the insertions , @ @ . but uh um . if you 're operating in the wrong range i mean , that 's why just in general , if you change what these these penalties and scaling factors are , you reach some point that 's a that 's a minimum . so . um . um . we do have to do well over a range of different conditions , some of which are noisier than others . um . but , um , i think we may get a better handle on that if we if we see um , i mean we ca it 's if we actually could pick a a a more stable value for the range of these features , it , um , uh , could uh even though it 's it 's it 's true that in a real situation you can in fact adjust the these these scaling factors in the back - end , and it 's ar artificial here that we 're not adjusting those , you certainly do n't wan na be adjusting those all the time . and if you have a nice front - end that 's in roughly the right range i remember after we got our stuff more or less together in the previous systems we built , that we tended to set those scaling factors at kind of a standard level , and we would rarely adjust them again , even though you could get a phd f : mm - hmm . professor e : for an evaluation you can get an extra point or something if you tweaked it a little bit . but , once we knew what rou roughly the right operating range was , it was pretty stable , and uh , we might just not even be in the right operating range . phd f : so , would the ? uh , would a good idea be to try to map it into the same range that you get in the well - matched case ? so , if we computed what the range was in well - matched , and then when we get our noisy conditions out we try to make it have the same range as ? professor e : no . you do n't wan na change it for different conditions . no . no . i i i what what i 'm saying phd f : oh , i was n't suggesting change it for different conditions . i was just saying that when we pick a range , we we wan na pick a range that we map our numbers into professor e : yeah . phd f : we should probably pick it based on the range that we get in the well - matched case . otherwise , i mean , what range are we gon na choose to to map everything into ? professor e : well . it depends how much we wan na do gamesmanship and how much we wan na do i mean , i if he it to me , actually , even if you wan na be play on the gamesmanship side , it can be kinda tricky . so , i mean , what you would do is set the set the scaling factors , uh , so that you got the best number for this point four five times the you know , and so on . phd f : mm - hmm . professor e : but they might change that those weightings . phd f : yeah . professor e : um . so uh i just sorta think we need to explore the space . just take a look at it a little bit . phd f : mm - hmm . professor e : and we we we may just find that that we 're way off . phd f : ok . mm - hmm . professor e : maybe we 're not . you know ? as for these other things , it may turn out that , uh , it 's kind of reasonable . but then i mean , andreas gave a very reasonable response , and he 's probably not gon na be the only one who 's gon na say this in the future of , you know , people people within this tight - knit community who are doing this evaluation are accepting , uh , more or less , that these are the rules . but , people outside of it who look in at the broader picture are certainly gon na say `` well , wait a minute . you 're doing all this standing on your head , uh , on the front - end , phd f : yeah . professor e : when all you could do is just adjust this in the back - end with one s one knob . `` phd f : mm - hmm . professor e : and so we have to at least , i think , determine that that 's not true , which would be ok , or determine that it is true , in which case we want to adjust that and then continue with with what we 're doing . and as you say as you point out finding ways to then compensate for that in the front - end also then becomes a priority for this particular test , phd f : right . professor e : and saying you do n't have to do that . phd f : mm - hmm . professor e : so . ok . so , uh what 's new with you ? phd b : uh . so there 's nothing new . um . professor e : uh , what 's old with you that 's developed ? phd b : i 'm sorry ? professor e : you ok . what 's old with you that has developed over the last week or two ? phd b : mmm . well , so we 've been mainly working on the report and and yeah . phd f : mainly working on what ? phd b : on the report of the work that was already done . um . mm - hmm . that 's all . phd f : how about that ? any - anything new on the thing that , uh , you were working on with the , uh ? phd c : i do n't have results yet . phd f : no results ? yeah . professor e : what was that ? phd f : the the , uh , grad a : voicing thing . phd f : voicing detector . professor e : i mean , what what 's what 's going on now ? what are you doing ? phd c : uh , to try to found , nnn , robust feature for detect between voice and unvoice . and we w we try to use the variance of the es difference between the fft spectrum and mel filter bank spectrum . professor e : yeah . phd c : uh , also the another parameter is relates with the auto - correlation function . professor e : uh - huh . phd c : r - ze energy and the variance a also of the auto - correlation function . professor e : uh - huh . so , that 's yeah . that 's what you were describing , i guess , a week or two ago . phd c : yeah . but we do n't have res we do n't have result of the auro for aurora yet . we need to train the neural network professor e : mm - hmm . phd c : and professor e : so you 're training neural networks now ? phd c : no , not yet . professor e : so , what wha wh wha what what 's going on ? phd c : well , we work in the report , too , because we have a lot of result , professor e : uh - huh . phd c : they are very dispersed , and was necessary to to look in all the directory to to to give some more structure . professor e : so . b so yeah . i if i can summarize , basically what 's going on is that you 're going over a lot of material that you have generated in furious fashion , f generating many results and doing many experiments and trying to pull it together into some coherent form to be able to see wha see what happens . phd c : hm - hmm . phd b : uh , y yeah . basically we we 've stopped , uh , experimenting , i mean . we 're just writing some kind of technical report . and phd f : is this a report that 's for aurora ? or is it just like a tech report for icsi , phd b : yeah . phd c : for icsi . phd f : or ? ah . i see . phd b : yeah . phd c : just summary of the experiment and the conclusion and something like that . professor e : yeah . phd b : mm - hmm . professor e : ok . so , my suggestion , though , is that you you not necessarily finish that . but that you put it all together so that it 's you 've got you 've got a clearer structure to it . you know what things are , you have things documented , you 've looked things up that you needed to look up . phd b : mm - hmm . professor e : so that , you know so that such a thing can be written . and , um when when when do you leave again ? phd c : uh , in july . first of july . professor e : first of july ? ok . and that you figure on actually finishing it in in june . because , you know , you 're gon na have another bunch of results to fit in there anyway . phd b : mm - hmm . phd c : mm - hmm . professor e : and right now it 's kind of important that we actually go forward with experiments . phd c : it 's not . professor e : so so , i i think it 's good to pause , and to gather everything together and make sure it 's in good shape , so that other people can get access to it and so that it can go into a report in june . but i think to to really work on on fine - tuning the report n at this point is is probably bad timing , i i think . phd b : mm - hmm . yeah . well , we did n't we just planned to work on it one week on this report , not no more , anyway . um . professor e : but you ma you may really wan na add other things later anyway phd b : yeah . mm - hmm . professor e : because you there 's more to go ? phd b : yeah . well , so i do n't know . there are small things that we started to to do . but phd f : are you discovering anything , uh , that makes you scratch your head as you write this report , like why did we do that , or why did n't we do this , or ? phd b : yeah . yeah . and actually , there were some tables that were also with partial results . we just noticed that , wh while gathering the result that for some conditions we did n't have everything . but anyway . um . yeah , yeah . we have , yeah , extracted actually the noises from the speechdat - car . and so , we can train neural network with speech and these noises . um . it 's difficult to say what it will give , because when we look at the aurora the ti - digits experiments , um , they have these three conditions that have different noises , and apparently this system perform as well on the seen noises on the unseen noises and on the seen noises . but , i think this is something we have to try anyway . so adding the noises from from the speechdat - car . um . professor e : that 's that 's , uh that 's permitted ? phd b : uh . well , ogi does did that . um . at some point they did that for for the voice activity detector . phd c : uh , for a v vad . phd b : right ? um . phd f : could you say it again ? what what exactly did they do ? phd b : they used some parts of the , um , italian database to train the voice activity detector , i think . it professor e : yeah . i guess the thing is yeah . i guess that 's a matter of interpretation . the rules as i understand it , is that in principle the italian and the spanish and the english no , italian and the finnish and the english ? were development data phd b : yeah . and spanish , yeah . professor e : on which you could adjust things . and the and the german and danish were the evaluation data . phd b : mm - hmm . professor e : and then when they finally actually evaluated things they used everything . phd b : yeah . that 's right . uh professor e : so uh , and it is true that the performance , uh , on the german was i mean , even though the improvement was n't so good , the pre the raw performance was really pretty good . phd b : mm - hmm . professor e : so and , uh , it it does n't appear that there 's strong evidence that even though things were somewhat tuned on those three or four languages , that that going to a different language really hurt you . and the noises were not exactly the same . right ? because it was taken from a different , uh i mean they were different drives . phd b : different cars . yeah . professor e : i mean , it was it was actual different cars and so on . phd b : yeah . professor e : so . um , it 's somewhat tuned . it 's tuned more than , you know , a a a a phd b : mm - hmm . professor e : you 'd really like to have something that needed no particular noise at all , maybe just some white noise or something like that a at most . phd b : mm - hmm . professor e : but that 's not really what this contest is . so . um , i guess it 's ok . phd b : mm - hmm . professor e : that 's something i 'd like to understand before we actually use something from it , phd f : i think it 's professor e : because it would phd f : it 's probably something that , mmm , the you know , the , uh , experiment designers did n't really think about , because i think most people are n't doing trained systems , or , you know , uh , systems that are like ours , where you actually use the data to build models . i mean , they just doing signal - processing . phd b : yeah . professor e : well , it 's true , except that , uh , that 's what we used in aurora one , and then they designed the things for aurora - two knowing that we were doing that . phd f : yeah . that 's true . and they did n't forbid us right ? to build models on the data ? professor e : no . but , i think i think that it it it probably would be the case that if , say , we trained on italian , uh , data and then , uh , we tested on danish data and it did terribly , uh , that that it would look bad . and i think someone would notice and would say `` well , look . this is not generalizing . `` i would hope tha i would hope they would . phd f : mm - hmm . professor e : um . but , uh , it 's true . you know , maybe there 's parameters that other people have used you know , th that they have tuned in some way for other things . so it 's it 's , uh we should we should maybe that 's maybe a topic especially if you talk with him when i 'm not here , that 's a topic you should discuss with hynek phd b : mm - hmm . professor e : to , you know , double check it 's ok . phd f : do we know anything about the speakers for each of the , uh , training utterances ? phd b : what do you mean ? we we phd f : do you have speaker information ? professor e : social security number phd f : that would be good . phd b : like , we have male , female , phd f : bank pin . phd b : at least . phd f : just male f female ? professor e : what kind of information do you mean ? phd f : well , i was thinking about things like , you know , gender , uh you know , gender - specific nets and , uh , vocal tract length normalization . phd b : mm - hmm . phd f : things like that . i d i do n't i did n't know what information we have about the speakers that we could try to take advantage of . phd b : mm - hmm . professor e : hmm . uh . right . i mean , again , i if you had the whole system you were optimizing , that would be easy to see . but if you 're supposedly just using a fixed back - end and you 're just coming up with a feature vector , w w i 'm not sure i mean , having the two nets suppose you detected that it was male , it was female you come up with different phd f : well , you could put them both in as separate streams or something . uh . phd b : mm - hmm . professor e : maybe . phd f : i do n't know . i was just wondering if there was other information we could exploit . phd b : mm - hmm . professor e : hmm . yeah , it 's an interesting thought . maybe having something along the i mean , you ca n't really do vocal tract normalization . but something that had some of that effect phd f : yeah . professor e : being applied to the data in some way . phd f : mm - hmm . phd b : do you have something simple in mind for i mean , vocal tract length normalization ? phd f : uh no . i had n't i had n't thought it was thought too much about it , really . it just something that popped into my head just now . and so i i i mean , you could maybe use the ideas a similar idea to what they do in vocal tract length normalization . you know , you have some sort of a , uh , general speech model , you know , maybe just a mixture of gaussians that you evaluate every utterance against , and then you see where each , you know , utterance like , the likelihood of each utterance . you divide the the range of the likelihoods up into discrete bins and then each bin 's got some knob uh , setting . professor e : yeah . but just listen to yourself . i mean , that uh really does n't sound like a real - time thing with less than two hundred milliseconds , uh , latency that and where you 're not adjusting the statistical engine at all . phd f : yeah . yeah . phd b : mm - hmm . phd f : yeah . that 's true . professor e : you know , that just phd f : right . professor e : i mean yeah . phd f : could be expensive . professor e : no . well not just expensive . i i i do n't see how you could possibly do it . you ca n't look at the whole utterance and do anything . you know , you can only right ? each frame comes in and it 's got ta go out the other end . phd f : right . professor e : so , uh phd f : right . so whatever it was , it would have to be uh sort of on a per frame basis . professor e : yeah . phd b : mm - hmm . professor e : yeah . i mean , you can do , um fairly quickly you can do male female f male female stuff . phd f : yeah . yeah . professor e : but as far as , i mean like i thought bbn did a thing with , uh , uh , vocal tract normalization a ways back . maybe other people did too . with with , uh , uh , l trying to identify third formant average third formant using that as an indicator of phd f : i do n't know . professor e : so . you know , third formant i if you imagine that to first order what happens with , uh , changing vocal tract is that , uh , the formants get moved out by some proportion phd f : mm - hmm . professor e : so , if you had a first formant that was one hundred hertz before , if the fifty if the vocal tract is fifty percent shorter , then it would be out at seven fifty hertz , and so on . so , that 's a move of two hundred fifty hertz . whereas the third formant which might have started off at twenty - five hundred hertz , you know , might be out to thirty - seven fifty , you know so it 's at so , although , you frequently get less distinct higher formants , it 's still third formant 's kind of a reasonable compromise , and phd f : mm - hmm . professor e : so , i think , eh , if i recall correctly , they did something like that . and and but um , that does n't work for just having one frame or something . phd f : yeah . phd b : mm - hmm . professor e : you know ? that 's more like looking at third formant over over a turn or something like that , phd b : mm - hmm . professor e : and phd f : right . professor e : um . so . but on the other hand , male female is a is a is a much simpler categorization than figuring out a a factor to , uh , squish or expand the the spectrum . phd f : mm - hmm . professor e : so , um . y you could imagine that i mean , just like we 're saying voiced - unvoiced is good to know uh , male female is good to know also . um . phd f : mm - hmm . professor e : but , you 'd have to figure out a way to to to , uh , incorporate it on the fly . uh , i mean , i guess , as you say , one thing you could do is simply , uh , have the the male and female output vectors you know , tr nets trained only on males and n trained only on females or or , uh , you know . but um . i do n't know if that would really help , because you already have males and females and it 's mm - hmm putting into one net . so is it ? phd f : is it balanced , um , in terms of gender the data ? professor e : do you know ? phd b : almost , yeah . mm - hmm . professor e : hmm . ok . y you 're you were saying before ? phd b : uh . yeah . so , this noise , um yeah . the msg um . mmm . there is something perhaps , i could spend some days to look at this thing , cuz it seems that when we train networks on let 's say , on timit with msg features , they they look as good as networks trained on plp . but , um , when they are used on on the speechdat - car data , it 's not the case oh , well . the msg features are much worse , and so maybe they 're , um , less more sensitive to different recording conditions , or shou professor e : should n't be . they should be less so . phd b : yeah . but professor e : r right ? wh - ? but let me ask you this . what what 's the , um ? do you kno recall if the insertions were were higher with msg ? phd b : i do n't know . i can not tell . but it 's it the the error rate is higher . so , i don professor e : yeah . but you should always look at insertions , deletions , and substitutions . phd b : yeah . mm - hmm . professor e : so phd b : mm - hmm . professor e : so , uh msg is very , very dif eh , plp is very much like mel cepstrum . msg is very different from both of them . phd b : mm - hmm . professor e : so , if it 's very different , then this is the sort of thing i mean i 'm really glad andreas brought this point up . i sort of had forgotten to discuss it . um . you always have to look at how this uh , these adjustments , uh , affect things . and even though we 're not allowed to do that , again we maybe could reflect that back to our use of the features . phd b : mm - hmm . professor e : so if it if in fact , uh the problem might be that the range of the msg features is quite different than the range of the plp or mel cepstrum . phd b : mm - hmm . mm - hmm . professor e : and you might wan na change that . phd b : but yeah . but , it 's d it 's after well , it 's tandem features , so mmm . professor e : yeah . phd b : yeah . we we have estimation of post posteriors with plp and with msg as input , professor e : yeah . phd b : so i don well . i do n't know . professor e : that means they 're between zero and one . phd b : mm - hmm . professor e : but i it it it it does n't necessarily you know , they could be , um do - does n't tell you what the variance of the things is . phd b : mmm . mm - hmm . professor e : right ? cuz if you 're taking the log of these things , it could be , uh knowing what the sum of the probabilities are , does n't tell you what the sum of the logs are . phd b : mm - hmm . yeah . yeah . so we should look at the likelihood , or or what ? or well , at the log , perhaps , and professor e : yeah . yeah . phd b : mm - hmm . professor e : or what you know , what you 're uh the thing you 're actually looking at . phd b : mm - hmm . professor e : so your your the values that are are actually being fed into htk . phd b : mm - hmm . but professor e : what do they look like ? phd f : no and so th the , uh for the tandem system , the values that come out of the net do n't go through the sigmoid . right ? they 're sort of the pre - nonlinearity values ? professor e : right . so they 're kinda like log probabilities is what i was saying . phd f : and those ok . and tho that 's what goes into htk ? professor e : uh , almost . but then you actually do a klt on them . um . they are n't normalized after that , are they ? phd b : mmm . no , they are not no . professor e : no . ok . so , um . right . so the question is yeah . whatever they are at that point , um , are they something for which taking a square root or cube root or fourth root or something like that is is gon na be a good or a bad thing ? so . phd b : mm - hmm . professor e : uh , and that 's something that nothing nothing else after that is gon na uh , things are gon na scale it uh , you know , subtract things from it , scale it from it , but nothing will have that same effect . um . so . um . anyway , eh phd f : yeah . cuz if if the log probs that are coming out of the msg are really big , the standard insertion penalty is gon na have very little effect professor e : well , the right . phd f : compared to , you know , a smaller set of log probs . professor e : yeah . no . again you do n't really look at that . it 's something that , and then it 's going through this transformation that 's probably pretty close to it 's , eh , whatever the klt is doing . but it 's probably pretty close to what a a a discrete cosine transformation is doing . phd f : yeah . professor e : but still it 's it 's not gon na probably radically change the scale of things . i would think . and , uh yeah . it may be entirely off and and it may be at the very least it may be quite different for msg than it is for mel cepstrum or plp . so that would be so the first thing i 'd look at without adjusting anything would just be to go back to the experiment and look at the , uh , substitutions , insertions , and deletions . and if the if the , uh i if there 's a fairly large effect of the difference , say , uh , uh , the r ratio between insertions and deletions for the two cases then that would be , uh , an indicator that it might might be in that direction . phd b : mm - hmm . mm - hmm . yeah . but , professor e : anything else ? phd b : my my point was more that it it works sometimes and but sometimes it does n't work . professor e : yeah . well . phd b : and it works on ti - digits and on speechdat - car it does n't work , and professor e : yeah . phd b : mm - hmm . yeah . well . professor e : but , you know , some problems are harder than others , phd b : mm - hmm . yeah . professor e : and and , uh , sometimes , you know , there 's enough evidence for something to work and then it 's harder , it breaks . you know , phd b : mm - hmm . professor e : so it 's but it but , um , i it it could be that when you say it works maybe we could be doing much better , even in ti - digits . right ? phd b : yeah . yeah , sure . professor e : hmm ? yeah . phd b : yeah . well , there is also the spectral subtraction , which , um i think maybe we should , uh , try to integrate it in in our system . professor e : yeah . phd b : mmm . mm - hmm . professor e : right . phd b : i think that would involve to to mmm use a big a al already a big bunch of the system of ericsson . because he has spectral subtraction , then it 's followed by , um , other kind of processing that 's are dependent on the uh , if it 's speech or noi or silence . professor e : mm - hmm . phd b : and there is this kind of spectral flattening after if it 's silence , and and s i i think it 's important , um , to reduce this musical noise and this this increase of variance during silence portions . so . well . this was in this would involve to take almost everything from from the this proposal and and then just add some kind of on - line normalization in in the neural network . mmm . professor e : ok . well , this 'll be , i think , something for discussion with hynek next week . phd b : yeah . mm - hmm . professor e : yeah . ok . right . so . how are , uh , uh how are things going with what you 're doing ? grad d : oh . well , um , i took a lot of time just getting my taxes out of the way multi - national taxes . so , i 'm i 'm starting to write code now for my work but i do n't have any results yet . um , i it would be good for me to talk to hynek , i think , when he 's here . professor e : yeah . grad d : do you know what his schedule will be like ? professor e : uh , he 'll be around for three days . grad d : ok . so , y professor e : uh , we 'll have a lot of time . so , uh um . i 'll , uh you know , he 's he 'll he 'll be talking with everybody in this room so . phd f : but you said you wo n't you wo n't be here next thursday ? professor e : not thursday and friday . yeah . cuz i will be at faculty retreat . so . i 'll try to connect with him and people as as i can on on wednesday . but um . oh , how 'd taxes go ? taxes go ok ? grad d : mmm . yeah . professor e : yeah . oh , good . yeah . yeah . that 's just that 's that 's one of the big advantages of not making much money is the taxes are easier . yeah . phd f : unless you 're getting money in two countries . professor e : i think you are . are n't you ? phd f : they both want their cut . grad d : hmm . yeah . phd f : right ? professor e : yeah . yeah . huh . canada w canada wants a cut ? grad d : mm - hmm . professor e : have to do so you you have to do two returns ? grad d : mmm . w uh , for two thousand i did . yeah . professor e : oh , oh . yeah . for tw that 's right , ju phd f : but not for this next year ? professor e : two thousand . yeah . probably not this next year , i guess . yeah . yeah . grad d : uh , i 'll i 'll still have a bit of canadian income but it 'll be less complicated because i will not be a considered a resident of canada anymore , so i wo n't have to declare my american income on my canadian return . professor e : ok . alright . uh . barry , do you wan na say something about your stuff here ? grad a : oh , um . right . i just , um , continuing looking at , uh , ph uh , phonetic events , and , uh , this tuesday gon na be , uh , meeting with john ohala with chuck to talk some more about these , uh , ph um , phonetic events . um , came up with , uh , a plan of attack , uh , gon na execute , and um yeah . it 's that 's pretty much it . professor e : oh , well . no um , why do n't you say something about what it is ? grad a : oh , you oh , you want you want details . hmm . ok . professor e : well , we 're all gathered here together . i thought we 'd , you know grad a : i was hoping i could wave my hands . um . so , um . so , once wa i i was thinking getting getting us a set of acoustic events to um , to be able to distinguish between , uh , phones and words and stuff . and um , once we we would figure out a set of these events that can be , you know , um , hand - labeled or or derived , uh , from h the hand - labeled phone targets . um , we could take these events and , um , do some cheating experiments , um , where we feed , um , these events into an sri system , um , eh , and evaluate its performance on a switchboard task . uh , yeah . grad d : hey , barry ? can you give an example of an event ? grad a : yeah . sure . um , i i can give you an example of twenty - odd events . um so , he in this paper , um , it 's talking about phoneme recognition using acoustic events . so , things like frication or , uh , nasality . professor e : whose paper is it ? grad a : um , this is a paper by hubener and cardson benson bernds - berndsen . professor e : yeah . huh . from , uh , university of hamburg and bielefeld . grad a : mm - hmm . phd f : yeah . i think the just to expand a little bit on the idea of acoustic event . grad a : mm - hmm . phd f : there 's , um in my mind , anyways , there 's a difference between , um , acoustic features and acoustic events . and i think of acoustic features as being , um , things that linguists talk about , like , um professor e : so , stuff that 's not based on data . phd f : stuff that 's not based on data , necessarily . professor e : yeah . oh , ok . yeah . yeah , ok . phd f : right . that 's not based on , you know , acoustic data . so they talk about features for phones , like , uh , its height , grad a : yeah . phd f : its tenseness , laxness , things like that , grad a : mm - hmm . phd f : which may or may not be all that easy to measure in the acoustic signal . versus an acoustic event , which is just some something in the acoustic signal that is fairly easy to measure . um . so it 's , um it 's a little different , in at least in my mind . professor e : i mean , when we did the spam work i mean , there we had we had this notion of an , uh , auditory @ @ { comment } auditory event . grad a : good . that 's great . professor e : and , uh , um , called them `` avents `` , uh , uh , uh , with an a at the front . phd f : mm - hmm . professor e : uh . and the the the idea was something that occurred that is important to a bunch of neurons somewhere . so . grad a : mm - hmm . professor e : um . a sudden change or a relatively rapid change in some spectral characteristic will will do sort of this . i mean , there 's certainly a bunch of a bunch of places where you know that neurons are gon na fire because something novel has happened . that was that was the main thing that we were focusing on there . but there 's certainly other things beyond what we talked about there that are n't just sort of rapid changes , but phd f : it 's kinda like the difference between top - down and bottom - up . professor e : yeah . phd f : i think of the acoustic you know , phonetic features as being top - down . you know , you look at the phone and you say this phone is supposed to be you know , have this feature , this feature , and this feature . whether tha those features show up in the acoustic signal is sort of irrelevant . whereas , an acoustic event goes the other way . here 's the signal . here 's some event . grad a : mm - hmm . phd f : what ? and then that you know , that may map to this phone sometimes , and sometimes it may not . it just depen maybe depends on the context , things like that . professor e : mm - hmm . phd f : and so it 's sort of a different way of looking . professor e : mm - hmm . grad a : yeah . so . yeah . mm - hmm . um using these these events , um , you know , we can we can perform these these , uh , cheating experiments . see how how how good they are , um , in , um in terms of phoneme recognition or word recognition . and , um and then from that point on , i would , uh , s design robust event detectors , um , in a similar , um , wa spirit that saul has done w uh , with his graphical models , and this this probabilistic and - or model that he uses . um , eh , try to extend it to , um to account for other other phenomena like , um , cmr co - modulation release . and , um and maybe also investigate ways to to modify the structure of these models , um , in a data - driven way , uh , similar to the way that , uh , jeff jeff , uh , bilmes did his work . um , and while i 'm i 'm doing these , um , event detectors , you know , i can ma mea measure my progress by comparing , um , the error rates in clean and noisy conditions to something like , uh , neural nets . um , and so so , once we have these these , uh , event detectors , um , we could put them together and and feed the outputs of the event detectors into into the sri , um , hmm hmm system , and , um and test it on on switchboard or , um , maybe even aurora stuff . and , that 's pretty much the the big picture of of um , the plan . professor e : by the way , um , there 's , uh , a couple people who are gon na be here i forget if i already told you this , but , a couple people who are gon na be here for six months . grad a : mm - hmm . professor e : uh uh , there 's a professor kollmeier , uh , from germany who 's , uh , uh , quite big in the , uh , hearing - aid signal - processing area and , um , michael kleinschmidt , who 's worked with him , who also looks at auditory properties inspired by various , uh , brain function things . so , um , um , i think they 'll be interesting to talk to , in this sort of issue as these detectors are are , uh , developing . grad a : hmm . ok . professor e : so , he looks at interesting interesting things in in the different ways of looking at spectra in order to to get various speech properties out . so . ok . well , short meeting , but that 's ok . and , uh , we might as well do our digits . and like i say , i i encourage you to go ahead and meet , uh , next week with , uh , uh , hynek . alright , i 'll i 'll start . it 's , uh , one thirty - five . seventeen ok"
}