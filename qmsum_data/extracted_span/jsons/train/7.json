{
    "query": "<s> summarize the meeting",
    "answer": "phd b : ok . we 're on . grad e : hello ? professor a : ok , so uh had some interesting mail from uh dan ellis . actually , i think he he redirected it to everybody also so uh the pda mikes uh have a big bunch of energy at at uh five hertz uh where this came up was that uh i was showing off these wave forms that we have on the web and and uh i just sort of had n't noticed this , but that the major , major component in the wave in the second wave form in that pair of wave forms is actually the air conditioner . so . so . i i have to be more careful about using that as a as a as a good illustration , uh , in fact it 's not , of uh of the effects of room reverberation . it is is n't a bad illustration of the effects of uh room noise . on on uh some mikes uh but so . and then we had this other discussion about um whether this affects the dynamic range , cuz i know , although we start off with thirty two bits , you end up with uh sixteen bits and you know , are we getting hurt there ? but uh dan is pretty confident that we 're not , that that quantization error is not is still not a significant factor there . so . so there was a question of whether we should change things here , whether we should change a capacitor on the input box for that or whether we should phd b : yeah , he suggested a smaller capacitor , right ? professor a : right . but then i had some other uh thing discussions with him phd b : for the p d professor a : and the feeling was once we start monk monkeying with that , uh , many other problems could ha happen . and additionally we we already have a lot of data that 's been collected with that , so . phd b : yeah . professor a : a simple thing to do is he he he has a i forget if it this was in that mail or in the following mail , but he has a a simple filter , a digital filter that he suggested . we just run over the data before we deal with it . phd b : mm - hmm . professor a : um the other thing that i do n't know the answer to , but when people are using feacalc here , uh whether they 're using it with the high - pass filter option or not . and i do n't know if anybody knows . grad e : um . i could go check . professor a : but . yeah . so when we 're doing all these things using our software there is um if it 's if it 's based on the rasta - plp program , which does both plp and rasta - plp um then uh there is an option there which then comes up through to feacalc which um allows you to do high - pass filtering and in general we like to do that , because of things like this and it 's it 's pretty it 's not a very severe filter . does n't affect speech frequencies , even pretty low speech frequencies , at all , but it 's phd b : what 's the cut - off frequency it used ? professor a : oh . i do n't know i wrote this a while ago phd b : is it like twenty ? professor a : something like that . phd b : yeah . professor a : yeah . i mean i think there 's some effect above twenty but it 's it 's it 's it 's mild . so , i mean it probably there 's probably some effect up to a hundred hertz or something but it 's it 's pretty mild . i do n't know in the in the strut implementation of the stuff is there a high - pass filter or a pre pre - emphasis or something in the phd f : uh . i think we use a pre - emphasis . yeah . yeah . professor a : so . we we we want to go and check that in i for anything that we 're going to use the p d a mike for . uh he says that there 's a pretty good roll off in the pzm mikes so we do n't need need to worry about them one way or the other but if we do make use of the cheap mikes , uh we want to be sure to do that that filtering before we process it . and then again if it 's uh depending on the option that the our our software is being run with , it 's it 's quite possible that 's already being taken care of . uh but i also have to pick a different picture to show the effects of reverberation . uh phd b : did somebody notice it during your talk ? professor a : uh no . well . uh well . if they made output they were they were , you know they were nice . phd b : did n't say anything ? professor a : but . i mean the thing is it was since i was talking about reverberation and showing this thing that was noise , it was n't a good match , but it certainly was still uh an indication of the fact that you get noise with distant mikes . uh it 's just not a great example because not only is n't it reverberation but it 's a noise that we definitely know what to do . phd b : mm - hmm . professor a : so , i mean , it does n't take deep a new bold new methods to get rid of uh five hertz noise , so . phd b : yeah . professor a : um uh but . so it was it was a bad example in that way , but it 's it still is it 's the real thing that we did get out of the microphone at distance , so it was n't it w it w was n't wrong it was inappropriate . so . so uh , but uh , yeah , someone noticed it later pointed it out to me , and i went `` oh , man . why did n't i notice that ? `` um . so . um so i think we 'll change our our picture on the web , when we 're @ @ . one of the things i was i mean , i was trying to think about what what 's the best way to show the difference an and i had a couple of thoughts one was , that spectrogram that we show is o k , but the thing is the eyes uh and the the brain behind them are so good at picking out patterns from from noise that in first glance you look at them it does n't seem like it 's that bad uh because there 's many features that are still preserved . so one thing to do might be to just take a piece of the spec uh of the spectrogram where you can see that something looks different , an and blow it up , and have that be the part that 's just to show as well . you know . phd b : mm - hmm . mm - hmm . professor a : i i some things are going to be hurt . um another , i was thinking of was um taking some spectral slices , like uh like we look at with the recognizer , and look at the spectrum or cepstrum that you get out of there , and the the uh , um , the reverberation uh does make it does change that . and so maybe maybe that would be more obvious . grad c : spectral slices ? professor a : yeah . grad c : w w what d what do you mean ? professor a : well , i mean um all the recognizers look at frames . so they they look at phd b : so like one instant in time . professor a : yeah , look at a so it 's , yeah , at one point in time or uh twenty over twenty milliseconds or something , you have a spectrum or a cepstrum . that 's what i meant by a slice . grad c : i see . professor a : yeah . and if you look at phd b : you could just you could just throw up , you know , uh the uh some mfcc feature vectors . you know , one from one , one from the other , and then , you know , you can look and see how different the numbers are . professor a : right . well , that 's why i saying either well , either spectrum or cepstrum phd b : i 'm just kidding . professor a : but but i think the thing is you wan na phd b : i do n't mean a graph . i mean the actual numbers . professor a : oh . i see . oh . that would be lovely , yeah . phd b : yeah . `` see how different these sequences of numbers are ? `` professor a : yeah . or i could just add them up and get a different total . phd b : yeah . it 's not the square . professor a : ok . uh . what else wh what 's what else is going on ? phd f : uh , yeah . yeah , at first i had a remark why i am wondering why the pda is always so far . i mean we are always meeting at the beginning of the table and the pda 's there . professor a : uh . i guess cuz we have n't wanted to move it . we we could we could move us , phd f : yeah ? grad e : that 's right . phd f : well , anyway . um . yeah , so . uh . since the last meeting we 've we 've tried to put together um the clean low - pass um downsampling , upsampling , i mean , uh the new filter that 's replacing the lda filters , and also the um delay issue so that we considered th the the delay issue on the for the on - line normalization . mmm . so we 've put together all this and then we have results that are not um very impressive . well , there is no real improvement . professor a : but it 's not wer worse and it 's better better latency , phd f : it 's not professor a : right ? phd f : yeah . yeah . well . actually it 's better . it seems better when we look at the mismatched case but i think we are like like cheated here by the th this problem that uh in some cases when you modify slight slightly modify the initial condition you end up completely somewhere air somewhere else in the in the space , the parameters . professor a : yeah . phd f : so . well . the other system are for instance . for italian is at seventy - eight percent recognition rate on the mismatch , and this new system has eighty - nine . but i do n't think it indicates something , really . i do n't i do n't think it means that the new system is more robust professor a : uh - huh . phd f : or it 's simply the fact that well . professor a : well , the test would be if you then tried it on one of the other test sets , if if it was right . so this was italian , right ? phd f : yeah . yeah . professor a : so then if you take your changes phd f : it 's similar for other test sets professor a : and then phd f : but i mean from this se seventy - eight um percent recognition rate system , i could change the transition probabilities for the the first hmm and it will end up to eighty - nine also . professor a : uh - huh . phd f : by using point five instead of point six , point four as in the the htk script . professor a : uh - huh . yeah . phd f : so . well . that 's phd b : yeah . yeah i looked at um looked at the results when stephane did that phd f : well . eh uh phd b : and it 's it 's really wo really happens . phd f : this really happens . phd b : i mean th the only difference is you change the self - loop transition probability by a tenth of a percent phd f : yeah . professor a : yeah . phd b : and it causes ten percent difference in the word error rate . professor a : a tenth of a per cent . phd b : yeah . from point phd f : even tenth of a percent ? phd b : i i 'm sorry phd f : well , we tried we tried point one , phd b : f for point from you change at point one phd f : yeah . phd b : and n not tenth of a percent , one tenth , professor a : yeah . phd b : alright ? um so from point five so from point six to point five and you get ten percent better . professor a : mm - hmm . phd b : and it 's i think it 's what you basically hypothesized in the last meeting about uh it just being very phd f : mm - hmm . phd b : and i think you mentioned this in your email too it 's just very um phd f : mmm , yeah . phd b : you know get stuck in some local minimum and this thing throws you out of it i guess . phd f : mm - hmm . professor a : well , what 's what are according to the rules what what are we supposed to do about the transition probabilities ? are they supposed to be point five or point six ? phd b : i think you 're not allowed to yeah . that 's supposed to be point six , for the self - loop . phd f : yeah . professor a : point it 's supposed to be point six . phd b : yeah . but changing it to point five i think is which gives you much better results , but that 's not allowed . professor a : but not allowed ? yeah . ok . phd b : yeah . phd f : yeah , but even if you use point five , i 'm not sure it will always give you the better results phd b : yeah . phd f : on other test set or it phd b : right . we only tested it on the the medium mismatch , phd f : on the other training set , i mean . phd b : right ? you said on the other cases you did n't notice phd f : yeah . but . i think , yeah . i think the reason is , yeah , i not i it was in my mail i think also , is the fact that the mismatch is trained only on the far microphone . well , in for the mismatched case everything is um using the far microphone training and testing , whereas for the highly mismatched , training is done on the close microphone so it 's it 's clean speech basically so you do n't have this problem of local minima probably and for the well - match , it 's a mix of close microphone and distant microphone and well . phd b : i did notice uh something phd f : so th i think the mismatch is the more difficult for the training part . phd b : somebody , i think it was morgan , suggested at the last meeting that i actually count to see how many parameters and how many frames . professor a : mm - hmm . phd f : mm - hmm . phd b : and there are uh almost one point eight million frames of training data and less than forty thousand parameters in the baseline system . phd f : yeah . phd b : so it 's very , very few parameters compared to how much training data . professor a : well . yes . phd d : mm - hmm . professor a : so . and that that says that we could have lots more parameters actually . phd b : yeah . yeah . phd f : mm - hmm . phd b : i did one quick experiment just to make sure i had everything worked out and i just uh f for most of the um for for all of the digit models , they end up at three mixtures per state . and so i just did a quick experiment , where i changed it so it went to four and um it it it did n't have a r any significant effect at the uh medium mismatch and high mismatch cases and it had it was just barely significant for the well - matched better . uh so i 'm r gon na run that again but um with many more uh mixtures per state . professor a : yeah . cuz at forty thou i mean you could you could have uh yeah , easily four times as many parameters . phd b : mm - hmm . and i think also just seeing what we saw uh in terms of the expected duration of the silence model ? when we did this tweaking of the self - loop ? the silence model expected duration was really different . phd f : yeah . phd b : and so in the case where um it had a better score , the silence model expected duration was much longer . phd f : yeah . phd b : so it was like it was a better match . i think you know if we make a better silence model i think that will help a lot too um for a lot of these cases so but one one thing i i wanted to check out before i increased the um number of mixtures per state was uh in their default training script they do an initial set of three re - estimations and then they built the silence model and then they do seven iterations then the add mixtures and they do another seven then they add mixtures then they do a final set of seven and they quit . seven seems like a lot to me and it also makes the experiments go take a really long time i mean to do one turn - around of the well matched case takes like a day . professor a : mm - hmm . mm - hmm . phd b : and so you know in trying to run these experiments i notice , you know , it 's difficult to find machines , you know , compute the run on . and so one of the things i did was i compiled htk for the linux machines professor a : mm - hmm . phd b : cuz we have this one from ibm that 's got like five processors in it ? professor a : right . phd b : and so now i 'm you can run stuff on that and that really helps a lot because now we 've got you know , extra machines that we can use for compute . and if i 'm do running an experiment right now where i 'm changing the number of iterations ? from seven to three ? phd d : mm - hmm . professor a : yeah . phd b : just to see how it affects the baseline system . and so if we can get away with just doing three , we can do many more experiments more quickly . and if it 's not a a huge difference from running with seven iterations , um , you know , we should be able to get a lot more experiments done . and so . i 'll let you know what what happens with that . but if we can you know , run all of these back - ends f with many fewer iterations and on linux boxes we should be able to get a lot more experimenting done . professor a : mm - hmm . phd b : so . so i wanted to experiment with cutting down the number of iterations before i increased the number of gaussians . professor a : right . sorry . so um , how 's it going on the so . you you did some things . they did n't improve things in a way that convinced you you 'd substantially improved anything . phd f : yeah . professor a : but they 're not making things worse and we have reduced latency , right ? phd f : yeah . but actually um actually it seems to do a little bit worse for the well - matched case and we just noticed that yeah , actually the way the final score is computed is quite funny . it 's not a mean of word error rate . it 's not a weighted mean of word error rate , it 's a weighted mean of improvements . professor a : uh - huh . phd f : so . which means that actually the weight on the well - matched is well i well what what what happened is that if you have a small improvement or a small if on the well - matched case it will have uh huge influence on the improvement compared to the reference because the reference system is is is quite good for for the well - ma well - matched case also . phd b : so it it weights the improvement on the well - matched case really heavily compared to the improvement on the other cases ? phd f : no , but it 's the weighting of the of the improvement not of the error rate . phd b : yeah . yeah , and it 's hard to improve on the on the best case , cuz it 's already so good , right ? phd f : yeah but what i mean is that you can have a huge improvement on the h hmk 's , uh like five percent uh absolute , and this will not affect the final score almost uh this will almost not affect the final score because this improvement because the improvement uh relative to the the baseline is small professor a : so they do improvement in terms of uh accuracy ? rather than word error rate ? phd f : uh . uh improvement ? professor a : so phd f : no , it 's compared to the word er it 's improvement on the word error rate , yeah . sorry . professor a : so if you have uh ten percent error and you get five percent absolute uh improvement then that 's fifty percent . phd f : mm - hmm . professor a : ok . so what you 're saying then is that if it 's something that has a small word error rate , then uh a even a relatively small improvement on it , in absolute terms , will show up as quite quite large in this . phd f : mm - hmm . professor a : is that what you 're saying ? phd f : yeah . yeah . professor a : ok . but yeah that 's that 's it 's the notion of relative improvement . word error rate . phd f : yeah . sure , but when we think about the weighting , which is point five , point three , point two , it 's on absolute on on relative figures , professor a : yeah . phd f : not professor a : yeah . phd f : so when we look at this error rate professor a : no . that 's why i 've been saying we should be looking at word error rate uh and and not not at at accuracies . phd f : uh mmm , yeah . mmm , yeah . professor a : it 's phd f : mm - hmm . professor a : i mean uh we probably should have standardized on that all the way through . it 's just phd b : well . phd f : mm - hmm . phd b : i mean , it 's not it 's not that different , right ? i mean , just subtract the accuracy . professor a : yeah but you 're but when you look at the numbers , your sense of the relative size of things is quite different . phd b : i mean oh . oh , i see . yeah . professor a : if you had ninety percent uh correct and five percent , five over ninety does n't look like it 's a big difference , but five over ten is is big . phd b : mm - hmm . phd f : mm - hmm . professor a : so just when we were looking at a lot of numbers and getting sense of what was important . phd b : i see . i see . yeah . that makes sense . phd f : well anyway uh . so . yeah . so it hurts a little bit on the well - match and yeah . professor a : what 's a little bit ? like phd f : like , it 's difficult to say because again um i 'm not sure i have the um phd b : hey morgan ? do you remember that signif program that we used to use for testing signi ? is that still valid ? i i 've been using that . professor a : yeah . yeah , it was actually updated . uh . jeff updated it some years ago phd b : oh , it was . oh , i shoul professor a : and and uh cleaned it up made some things better in it . so . phd b : ok . i should find that new one . i just use my old one from ninety - two or whatever professor a : yeah , i 'm sure it 's not that different but but he he uh he was a little more rigorous , as i recall . phd f : right . so it 's around , like , point five . no , point six { comment } uh percent absolute on italian professor a : worse . phd f : worse , yep . professor a : out of what ? i mean . s phd f : uh well we start from ninety - four point sixty - four , and we go to ninety - four point o four . professor a : uh - huh . so that 's six six point th phd b : ninety - three point six four , right ? is the baseline . phd f : oh , no , i 've ninety - four . oh , the baseline , you mean . phd b : yeah . phd f : well i do n't i 'm not talking about the baseline here . phd b : oh . oh . i 'm sorry . phd f : i uh my baseline is the submitted system . phd b : ah ! ok . ah , ah . professor a : yeah . phd b : sorry . phd f : oh yeah . for finnish , we start to ninety - three point eight - four and we go to ninety - three point seventy - four . and for spanish we are we were at ninety - five point o five and we go to ninety - three - s point sixty one . professor a : ok , so we are getting hurt somewhat . and is that wh what do you know what piece you 've done several changes here . uh , do you know what pie phd f : yeah . i guess i guess it 's it 's the filter . because nnn , well uh we do n't have complete result , but the filter so the filter with the shorter delay hurts on italian well - matched , which and , yeah . and the other things , like um downsampling , upsampling , do n't seem to hurt and the new on - line normalization , neither . phd b : i 'm i 'm really confused about something . if we saw that making a small change like , you know , a tenth , to the self - loop had a huge effect , can we really make any conclusions about differences in this stuff ? phd f : mm - hmm . yeah that 's th yeah . phd b : i mean , especially when they 're this small . i mean . phd f : i think we can be completely fooled by this thing , but i do n't know . professor a : well , yeah . phd f : so . there is first this thing , and then the yeah , i computed the um like , the confidence level on the different test sets . and for the well - matched they are around um point six uh percent . for the mismatched they are around like let 's say one point five percent . and for the well - m uh hm they are also around one point five . professor a : but ok , so you these these degradations you were talking about were on the well - matched case uh . do the does the new filter make things uh better or worse for the other cases ? phd f : yeah . but . uh . about the same . it does n't hurt . yeah . professor a : does n't hurt , but does n't get a little better , or something . no . ok , so um i guess the argument one might make is that , `` yeah , if you looked at one of these cases and you jiggle something and it changes then uh you 're not quite sure what to make of it . but when you look across a bunch of these and there 's some some pattern , um i mean , so eh h here 's all the if if in all these different cases it never gets better , and there 's significant number of cases where it gets worse , then you 're probably hurting things , i would say . so um i mean at the very least that would be a reasonably prediction of what would happen with with a different test set , that you 're not jiggling things with . so i guess the question is if you can do better than this . if you can if we can approximate the old numbers while still keeping the latency down . phd f : mmm . yeah . professor a : uh , so . um . what i was asking , though , is uh are what 's what 's the level of communication with uh the o g i gang now , about this and phd f : well , we are exchanging mail as soon as we we have significant results . professor a : yeah . phd f : um . yeah . for the moment , they are working on integrating the um spectral subtraction apparently from ericsson . professor a : mm - hmm . phd f : um . yeah . and so . yeah . we are working on our side on other things like uh also trying a sup spectral subtraction but of of our own , i mean , another spectral substraction . professor a : mm - hmm . phd f : um . yeah . so i think it 's it 's ok . it 's going professor a : is there any further discussion about this this idea of of having some sort of source code control ? phd f : yeah . well . for the moment they 're uh everybody 's quite um there is this eurospeech deadline , so . professor a : i see . phd f : um . and . yeah . but yeah . as soon as we have something that 's significant and that 's better than than what was submitted , we will fix fix the system and but we 've not discussed it it it this yet , yeah . professor a : yeah . sounds like a great idea but but i think that that um he 's saying people are sort of scrambling for a eurospeech deadline . but that 'll be uh , uh done in a week . so , maybe after this next one . phd f : yeah . phd b : wow ! already a week ! man ! professor a : yeah . phd b : you 're right . that 's amazing . professor a : yeah . anybo - anybody in the in this group do doing anything for eurospeech ? or , is that what is that phd f : yeah we are we are trying to to do something with the meeting recorder digits , professor a : right . phd f : and but yeah . yeah . and the good thing is that there is this first deadline , professor a : yeah . phd f : and , well , some people from ogi are working on a paper for this , but there is also the um special session about th aurora which is uh which has an extended deadline . so . the deadline is in may . professor a : for uh oh , for eurospeech ? phd f : for th yeah . so f only for the experiments on aurora . so it it 's good , professor a : oh , a special dispensation . phd f : yeah . professor a : that 's great . phd b : mm - hmm . where is eurospeech this year ? phd f : it 's in denmark . professor a : aalborg aalborg uh so the deadline when 's the deadline ? when 's the deadline ? phd f : hmm ? i think it 's the thirteenth of may . professor a : that 's great ! it 's great . so we should definitely get something in for that . phd f : yeah . professor a : but on meeting digits , maybe there 's maybe . phd f : yeah . professor a : maybe . phd f : so it would be for the first deadline . professor a : yeah . yeah . so , i mean , i i think that you could certainly start looking at at the issue uh but but uh i think it 's probably , on s from what stephane is saying , it 's it 's unlikely to get sort of active participation from the two sides until after they 've phd b : well i could at least well , i 'm going to be out next week but i could try to look into like this uh cvs over the web . that seems to be a very popular way of people distributing changes and over , you know , multiple sites and things professor a : mm - hmm . phd b : so maybe if i can figure out how do that easily and then pass the information on to everybody so that it 's you know , as easy to do as possible and and people do n't it wo n't interfere with { comment } their regular work , then maybe that would be good . and i think we could use it for other things around here too . so . professor a : good . grad c : that 's cool . and if you 're interested in using cvs , i 've set it up here , phd b : oh great . i used it a long time ago but it 's been a while so maybe i can ask you some questions . grad c : oh . so . i 'll be away tomorrow and monday but i 'll be back on tuesday or wednesday . professor a : yeah . dave , the other thing , actually , is is this business about this wave form . maybe you and i can talk a little bit at some point about coming up with a better uh demonstration of the effects of reverberation for our web page , cuz uh the uh um i mean , actually the the uh it made a good good audio demonstration because when we could play that clip the the the really obvious difference is that you can hear two voices and in the second one and only hear phd b : maybe we could just like , talk into a cup . professor a : yeah . phd b : some good reverb . professor a : no , i mean , it sound it sounds pretty reverberant , but i mean you ca n't when you play it back in a room with a you know a big room , nobody can hear that difference really . grad c : yeah . professor a : they hear that it 's lower amplitude and they hear there 's a second voice , grad c : uh - huh . professor a : um but uh that actually that makes for a perfectly good demo because that 's a real obvious thing , that you hear two voices . phd b : but not of reverberation . professor a : yeah . grad c : a boom . professor a : well that that that 's ok . but for the the visual , just , you know , i 'd like to have uh uh , you know , the spectrogram again , grad c : yeah . professor a : because you 're you 're you 're visual uh abilities as a human being are so good you can pick out you know , you you look at the good one , you look at the cru the screwed up one , and and you can see the features in it without trying to @ @ phd b : i noticed that in the pictures . professor a : yeah . phd b : i thought `` hey , you know th `` i my initial thought was `` this is not too bad ! `` professor a : right . but you have to you know , if you look at it closely , you see `` well , here 's a place where this one has a big formant uh uh formant maj major formants here are are moving quite a bit . `` and then you look in the other one and they look practically flat . phd b : mm - hmm . professor a : so i mean you could that 's why i was thinking , in a section like that , you could take a look look at just that part of the spectrogram and you could say `` oh yeah . this this really distorted it quite a bit . `` phd b : yeah . the main thing that struck me in looking at those two spectrograms was the difference in the high frequencies . it looked like for the one that was farther away , you know , it really everything was attenuated professor a : right . phd b : and i mean that was the main visual thing that i noticed . professor a : right . but it 's it 's uh so . yeah . so there are clearly are spectral effects . since you 're getting all this indirect energy , then a lot of it does have have uh reduced high frequencies . but um the other thing is the temporal courses of things really are changed , and and uh we want to show that , in some obvious way . the reason i put the wave forms in there was because uh they they do look quite different . uh . and so i thought `` oh , this is good . `` but i i just uh after after uh they were put in there i did n't really look at them anymore , cuz i just they were different . so i want something that has a is a more interesting explanation for why they 're different . um . grad c : oh . so maybe we can just substitute one of these wave forms and um then do some kind of zoom in on the spectrogram on an interesting area . professor a : something like that . yeah . grad c : uh - huh . professor a : the other thing that we had in there that i did n't like was that um the most obvious characteristic of the difference uh when you listen to it is that there 's a second voice , and the the the the the uh cuts that we have there actually do n't correspond to the full wave form . it 's just the first i think there was something where he was having some trouble getting so much in , or . i i forget the reason behind it . but it it 's um it 's the first six seconds or something of it and it 's in the seventh or eighth second or something where @ @ the second voice comes in . so we we would like to actually see the voice coming in , too , i think , since that 's the most obvious thing when you listen to it . grad c : mm - hmm . professor a : so . um . phd f : uh , yeah . yeah . i brought some i do n't know if some figures here . well . i start we started to work on spectral subtraction . and um the preliminary results were very bad . professor a : uh - huh . phd f : so the thing that we did is just to add spectral subtraction before this , the wall uh process , which contains lda on - line normalization . and it hurts uh a lot . professor a : uh - huh . phd f : and so we started to look at at um things like this , which is , well , it 's yeah . so you have the c - zero parameters for one uh italian utterance . phd d : you can @ @ . phd f : and i plotted this for two channels . channel zero is the close mic microphone , and channel one is the distant microphone . and it 's perfectly synchronized , so . and the sentence contain only one word , which is `` due `` and it ca n't clearly be seen . where where is it ? professor a : uh - huh . phd f : where is the word ? phd b : this is this is , oh , a plot of c - zero , the energy . phd f : this is a plot of c - zero , uh when we do n't use spectral substraction , and when there is no on - line normalization . professor a : mm - hmm . phd f : so . there is just some filtering with the lda and and some downsampling , upsampling . phd b : c - zero is the close talking ? uh the close channel ? phd f : yeah . yeah . phd b : and s channel one is the phd f : yeah . so c - zero is very clean , actually . phd b : yeah . phd f : uh then when we apply mean normalization it looks like the second figure , though it is not . which is good . well , the noise part is around zero professor a : mm - hmm . phd f : and and then the third figure is what happens when we apply mean normalization and variance normalization . so . what we can clearly see is that on the speech portion the two channel come becomes very close , but also what happens on the noisy portion is that the variance of the noise is professor a : mm - hmm . phd b : this is still being a plot of c - zero ? ok . phd f : yeah . this is still c - zero . phd b : can i ask um what does variance normalization do ? w what is the effect of that ? professor a : normalizes the variance . phd f : so it it yeah . phd b : i mean phd f : it normalized th the standard deviation . phd b : y yeah . phd f : so it phd b : no , i understand that , phd f : you you get an estimate of the standard deviation . phd b : but i mean phd f : that 's um phd b : no , i understand what it is , but i mean , what does it what 's what is phd f : yeah but . phd b : uh professor a : what 's the rationale ? phd b : we yeah . yeah . why why do it ? professor a : well , i mean , because everything uh if you have a system based on gaussians , everything is based on means and variances . phd b : yeah . professor a : so if there 's an overall reason you know , it 's like uh if you were doing uh image processing and in some of the pictures you were looking at , uh there was a lot of light uh and and in some , there was low light , phd b : mm - hmm . professor a : you know , you would want to adjust for that in order to compare things . phd b : mm - hmm . professor a : and the variance is just sort of like the next moment , you know ? so uh what if um one set of pictures was taken uh so that throughout the course it was went through daylight and night uh um um ten times , another time it went thr i mean i is , you know , how how much how much vari phd b : oh , ok . professor a : or no . i guess a better example would be how much of the light was coming in from outside rather than artificial light . so if it was a lot if more was coming from outside , then there 'd be the bigger effect of the of the of the change in the so every mean every all all of the the parameters that you have , especially the variances , are going to be affected by the overall variance . phd b : oh , ok . uh - huh . professor a : and so , in principle , you if you remove that source , then , you know , you can phd b : i see . ok . so would the major effect is that you 're gon na get is by normalizing the means , professor a : that 's the first order but thing , phd b : but it may help first - order effects . professor a : but then the second order is is the variances phd b : and it may help to do the variance . ok . professor a : because , again , if you if you 're trying to distinguish between e and b if it just so happens that the e 's were a more you know , were recorded when when the energy was was was larger or something , phd b : mm - hmm . mm - hmm . mm - hmm . professor a : or the variation in it was larger , uh than with the b 's , then this will be give you some some bias . so the it 's removing these sources of variability in the data that have nothing to do with the linguistic component . phd b : gotcha . ok . sorry to interrupt . professor a : but the the uh but let me as ask ask you something . phd f : yep . and it and this professor a : i is if if you have a good voice activity detector , is n't is n't it gon na pull that out ? phd f : yeah . sure . if they are good . yeah . well what it it shows is that , yeah , perhaps a good voice activity detector is is good before on - line normalization and that 's what uh we 've already observed . but uh , yeah , voice activity detection is not an easy thing neither . phd b : but after you do this , after you do the variance normalization i mean . phd f : mm - hmm . phd b : i do n't know , it seems like this would be a lot easier than this signal to work with . phd f : yeah . so . what i notice is that , while i prefer to look at the second figure than at the third one , well , because you clearly see where speech is . professor a : yeah . phd b : yeah . phd f : but the problem is that on the speech portion , channel zero and channel one are more different than when you use variance normalization where channel zero and channel one become closer . professor a : right . phd b : but for the purposes of finding the speech phd f : and yeah , but here phd b : you 're more interested in the difference between the speech and the nonspeech , phd f : yeah . phd b : right ? phd f : yeah . so i think , yeah . for i th i think that it perhaps it shows that uh the parameters that the voice activity detector should use uh have to use should be different than the parameter that have to be used for speech recognition . professor a : yeah . so basically you want to reduce this effect . phd f : well , y professor a : so you can do that by doing the voi voice activity detection . you also could do it by spect uh spectral subtraction before the variance normalization , right ? phd f : yeah , but it 's not clear , yeah . professor a : so uh phd f : we so . well . it 's just to professor a : yeah . phd f : the the number that at that are here are recognition experiments on italian hm and mm with these two kinds of parameters . and , well , it 's better with variance normalization . professor a : yeah . yeah . so it does get better even though it looks ugly . phd f : uh professor a : ok . but does this have the voice activity detection in it ? phd f : yeah . phd b : where 's th phd f : but the fact is that the voice activity detector does n't work on channel one . so . yeah . professor a : uh - huh . phd b : where at what stage is the voice activity detector applied ? is it applied here or a after the variance normalization ? professor a : spectral subtraction , i guess . phd b : or phd f : it 's applied before variance normalization . so it 's a good thing , because i guess voice activity detection on this should could be worse . phd b : yeah . is it applied all the way back here ? phd f : it 's applied the um on , yeah , something like this , phd b : maybe that 's why it does n't work for channel one . phd f : yeah . perhaps , yeah . professor a : can i phd f : so we could perhaps do just mean normalization before vad . phd b : mm - hmm . professor a : mm - hmm . can i ask a , i mean a sort of top - level question , which is um `` if if most of what the ogi folk are working with is trying to integrate this other other uh spectral subtraction , why are we worrying about it ? `` phd f : mm - hmm . about ? spectral subtraction ? professor a : yeah . phd f : it 's just uh well it 's another they are trying to u to use the um the ericsson and we 're trying to use something something else . and . yeah , and also to understand what happens because uh fff well . when we do spectral subtraction , actually , i think that this is the the two last figures . professor a : yeah . phd f : um . it seems that after spectral subtraction , speech is more emerging now uh than than before . professor a : mm - hmm . phd b : speech is more what ? phd f : well , the difference between the energy of the speech and the energy of the n spectral subtrac subtracted noise portion is is larger . professor a : mm - hmm . phd f : well , if you compare the first figure to this one actually the scale is not the same , but if you look at the the numbers um you clearly see that the difference between the c - zero of the speech and c - zero of the noise portion is larger . uh but what happens is that after spectral subtraction , you also increase the variance of this of c - zero . professor a : mm - hmm . phd f : and so if you apply variance normalization on this , it completely sc screw everything . well . professor a : mm - hmm . phd f : um . uh . yeah . so yeah . and what they did at ogi is just uh they do n't use on - line normalization , for the moment , on spectral subtraction and i think yeah . i think as soon as they will try on - line normalization there will be a problem . so yeah , we 're working on the same thing but i think uh with different different system and professor a : right . i mean , i the intellectually it 's interesting to work on things th uh one way or the other phd f : mm - hmm . professor a : but i 'm i 'm just wondering if um on the list of things that there are to do , if there are things that we wo n't do because we 've got two groups doing the same thing . phd f : mm - hmm . professor a : um . that 's phd f : mm - hmm . professor a : um . just just asking . uh . i mean , it 's phd f : yeah , well , phd b : there also could be i mean . i can maybe see a reason f for both working on it too if um you know , if if if you work on something else and and you 're waiting for them to give you spectral subtraction i mean it 's hard to know whether the effects that you get from the other experiments you do will carry over once you then bring in their spectral subtraction module . so it 's it 's almost like everything 's held up waiting for this one thing . i do n't know if that 's true or not , but i could see how professor a : i do n't know . phd b : maybe that 's what you were thinking . professor a : i do n't know . i mean , we still evidently have a latency reduction plan which which is n't quite what you 'd like it to be . that that seems like one prominent thing . and then uh were n't issues of of having a a second stream or something ? that was was it there was this business that , you know , we we could use up the full forty - eight hundred bits , and phd f : yeah . but i think they ' i think we want to work on this . they also want to work on this , so . uh . yeah . we we will try msg , but um , yeah . and they are t i think they want to work on the second stream also , but more with some kind of multi - band or , well , what they call trap or generalized trap . professor a : mm - hmm . phd f : um . so . professor a : ok . do you remember when the next meeting is supposed to be ? the next uh phd f : it 's uh in june . professor a : in june . ok . phd f : yeah . professor a : yeah . um . yeah , the other thing is that you saw that that mail about uh the vad v a ds performing quite differently ? that that uh so um . this there was this experiment of uh `` what if we just take the baseline ? `` set uh of features , just mel cepstra , and you inc incorporate the different v a and it looks like the the french vad is actually uh better significantly better . phd b : improves the baseline ? professor a : yeah . yeah . phd f : yeah but i do n't know which vad they use . uh . if the use the small vad i th i think it 's on i think it 's easy to do better because it does n't work at all . so . i i do n't know which which one . it 's pratibha that that did this experiment . phd d : yeah . phd f : um . we should ask which vad she used . phd d : i do n't @ @ . he actually , i think that he say with the good vad of from ogi and with the alcatel vad . and the experiment was sometime better , sometime worse . phd f : yeah but i it 's uh i think you were talking about the other mail that used vad on the reference features . yeah . professor a : and on that one , uh the french one is was better . phd d : i do n't remember . professor a : it was just better . phd d : mm - hmm . professor a : i mean it was enough better that that it would uh account for a fair amount of the difference between our performance , actually . phd f : mm - hmm . phd d : mm - hmm . professor a : so . uh . so if they have a better one , we should use it . i mean . you know ? it 's you ca n't work on everything . phd f : yeah . professor a : uh . uh . yeah . phd f : yeah , so we should find out if it 's really better . i mean if it the compared to the small or the big network . phd d : mm - hmm . professor a : yeah . phd f : and perhaps we can easily improve if if we put like mean normalization before the before the vad . because as as you 've mentioned . professor a : yeah . h hynek will be back in town uh the week after next , back back in the country . so . and start start organizing uh more visits and connections and so forth , phd f : mm - hmm . professor a : and uh working towards june . phd f : yeah . phd d : also is stephane was thinking that maybe it was useful to f to think about uh voiced - unvoiced phd f : mm - hmm . phd d : to work uh here in voiced - unvoiced detection . phd f : yeah . yeah . phd d : and we are looking in the uh signal . phd f : yeah , my feeling is that um actually when we look at all the proposals , ev everybody is still using some kind of spectral envelope professor a : right . phd f : and um it 's professor a : no use of pitch uh basically . yeah . phd f : yeah , well , not pitch , but to look at the um fine at the at the high re high resolution spectrum . professor a : yeah . well , it phd f : so . we do n't necessarily want to find the the pitch of the of the sound but uh cuz i have a feeling that when we look when we look at the just at the envelope there is no way you can tell if it 's voiced and unvoiced , if there is some it 's it 's easy in clean speech because voiced sound are more low frequency and . so there would be more , professor a : yeah . phd f : uh there is the first formant , which is the larger and then voiced sound are more high frequencies cuz it 's frication and professor a : right . phd f : but , yeah . when you have noise there is no um if if you have a low frequency noise it could be taken for for voiced speech and . professor a : yeah , you can make these mistakes , but but phd b : is n't there some other phd f : so i think that it it would be good yeah , yeah , well , go go on . phd b : uh , i was just gon na say is n't there are n't are n't there lots of ideas for doing voice activity , or speech - nonspeech rather , { comment } um by looking at um , you know , uh i guess harmonics or looking across time professor a : well , i think he was talking about the voiced - unvoiced , though , right ? so , not the speech - nonspeech . phd b : yeah . well even with e professor a : yeah . phd b : uh w ah you know , uh even with the voiced - non voiced - unvoiced um i thought that you or somebody was talking about professor a : well . uh yeah . b we should let him finish what he w he was gon na say , and phd b : so go ahead . phd f : um yeah , so yeah , i think if we try to develop a second stream well , there would be one stream that is the envelope and the second , it could be interesting to have that 's something that 's more related to the fine structure of the spectrum . and . yeah , so i do n't know . we were thinking about like using ideas from from larry saul , have a good voice detector , have a good , well , voiced - speech detector , that 's working on on the fft and uh larry saul could be an idea . we were are thinking about just kind of uh taking the spectrum and computing the variance of of the high resolution spectrum and things like this . professor a : so u s u ok . so so many tell you something about that . uh we had a guy here some years ago who did some work on um making use of voicing information uh to help in reducing the noise . phd f : yeah ? professor a : so what he was doing is basically y you you do estimate the pitch . phd f : mm - hmm . professor a : and um you from that you you estimate or you estimate fine harmonic structure , whichev ei either way , it 's more or less the same . but uh the thing is that um you then can get rid of things that are not i if there is strong harmonic structure , you can throw away stuff that 's that 's non - harmonic . phd f : mm - hmm . mm - hmm . professor a : and that that is another way of getting rid of part of the noise phd f : yeah . professor a : so um that 's something that is sort of finer , phd f : yeah . professor a : brings in a little more information than just spectral subtraction . um . phd f : mm - hmm . professor a : and he had some i mean , he did that sort of in combination with rasta . it was kind of like rasta was taking care of convolutional stuff and he was phd f : mm - hmm . professor a : and and got some some decent results doing that . so that that 's another another way . but yeah , there 's there 's phd f : yeah . mmm . professor a : right . there 's all these cues . we 've actually back when chuck was here we did some voiced - unvoiced uh classification using a bunch of these , phd f : but professor a : and and uh works ok . obviously it 's not perfect but um phd f : mm - hmm . professor a : but the thing is that you ca n't given the constraints of this task , we ca n't , in a very nice way , feed forward to the recognizer the information the probabilistic information that you might get about whether it 's voiced or unvoiced , where w we ca n't you know affect the the uh distributions or anything . phd f : mm - hmm . professor a : but we what we uh i guess we could yeah . phd b : did n't the head dude send around that message ? yeah , i think you sent us all a copy of the message , where he was saying that i i 'm not sure , exactly , what the gist of what he was saying , but something having to do with the voice activity detector and that it will that people should n't put their own in or something . it was gon na be a professor a : that but ok . so that 's voice activity detector as opposed to voicing detector . phd f : they did n't . professor a : so we 're talking about something a little different . phd b : oh , i 'm sorry . professor a : right ? phd b : i i missed that . professor a : i guess what you could do , maybe this would be w useful , if if you have if you view the second stream , yeah , before you before you do klt 's and so forth , if you do view it as probabilities , and if it 's an independent so , if it 's if it 's uh not so much envelope - based by fine - structure - based , uh looking at harmonicity or something like that , um if you get a probability from that information and then multiply it by you know , multiply by all the voiced outputs and all the unvoiced outputs , you know , then use that as the phd f : mm - hmm . professor a : uh take the log of that or uh pre pre uh pre - nonlinearity , phd f : yeah . i if professor a : uh and do the klt on the on on that , phd f : yeah . professor a : then that would that would i guess be uh a reasonable use of independent information . so maybe that 's what you meant . and then that would be phd f : yeah , well , i was not thinking this yeah , this could be an yeah so you mean have some kind of probability for the v the voicing professor a : r right . so you have a second neural net . phd f : and then use a tandem system professor a : it could be pretty small . yeah . if you have a tandem system and then you have some kind of it can be pretty small net phd f : mm - hmm . professor a : we used we d did some of this stuff . uh i i did , some years ago , phd f : yeah . professor a : and the and and you use the thing is to use information primarily that 's different as you say , it 's more fine - structure - based than than envelope - based phd f : mm - hmm . professor a : uh so then it you you you can pretty much guarantee it 's stuff that you 're not looking at very well with the other one , and uh then you only use for this one distinction . phd f : alright . professor a : and and so now you 've got a probability of the cases , and you 've got uh the probability of the finer uh categories on the other side . you multiply them where appropriate and uh um phd f : i see , yeah . mm - hmm . professor a : if they really are from independent information sources then they should have different kinds of errors phd f : mm - hmm . professor a : and roughly independent errors , and it 's a good choice for phd f : mm - hmm . mm - hmm . yeah . professor a : uh . yeah , that 's a good idea . phd f : yeah . because , yeah , well , spectral subtraction is good and we could u we could use the fine structure to to have a better estimate of the noise but still there is this issue with spectral subtraction that it seems to increase the variance of of of professor a : yeah . phd f : um well it 's this musical noise which is annoying if you d you do some kind of on - line normalization after . professor a : right . phd f : so . um . yeah . well . spectral subtraction and on - line normalization do n't seem to to go together very well . i professor a : or if you do a spectral subtraction do some spectral subtraction first and then do some on - line normalization then do some more spectral subtraction i mean , maybe maybe you can do it layers or something so it does n't does n't hurt too much or something . phd f : ah , yeah . professor a : but it but uh , anyway i think i was sort of arguing against myself there by giving that example phd f : yeah . professor a : uh i mean cuz i was already sort of suggesting that we should be careful about not spending too much time on exactly what they 're doing in fact if you get if you go into uh a uh harmonics - related thing it 's definitely going to be different than what they 're doing and uh uh phd f : mm - hmm . professor a : should have some interesting properties in noise . um . i know that when have people have done um sort of the obvious thing of taking uh your feature vector and adding in some variables which are pitch related or uh that it has n't my impression it has n't particularly helped . uh . has not . phd f : it it i has not , professor a : yeah . phd f : yeah . professor a : but i think uh that 's that 's a question for this uh you know extending the feature vector versus having different streams . phd f : oh . was it nois noisy condition ? the example that you you just professor a : and and it may not have been noisy conditions . phd f : yeah . professor a : yeah . i i do n't remember the example but it was it was on some darpa data and some years ago and so it probably was n't , actually phd f : mm - hmm . mm - hmm . yeah . but we were thinking , we discussed with barry about this , and perhaps thinking we were thinking about some kind of sheet cheating experiment where we would use timit professor a : uh - huh . phd f : and see if giving the d uh , this voicing bit would help in in terms of uh frame classification . professor a : why do n't you why do n't you just do it with aurora ? just any i in in each in each frame phd f : yeah , but but b but we can not do the cheating , this cheating thing . grad e : we 're professor a : uh grad e : we need labels . professor a : why not ? phd f : well . cuz we do n't have well , for italian perhaps we have , but we do n't have this labeling for aurora . we just have a labeling with word models professor a : i see . phd f : but not for phonemes . phd d : not for foreigners . grad e : we do n't have frame frame level transcriptions . phd d : right . phd f : um . yeah . professor a : but you could i mean you can you can align so that it 's not perfect , but if you if you know what was said and phd b : but the problem is that their models are all word level models . so there 's no phone models that you get alignments for . phd f : mm - hmm . phd b : you so you could find out where the word boundaries are but that 's about it . professor a : yeah . i see . grad e : s but we could use uh the the noisy version that timit , which you know , is similar to the the noises found in the ti - digits um portion of aurora . phd f : yeah . noise , yeah . yeah , that 's right , yep . mmm . professor a : yeah . phd f : well , i guess i guess we can we can say that it will help , but i do n't know . if this voicing bit does n't help , uh , i think we do n't have to to work more about this because uh . it 's just to know if it how much i it will help professor a : yeah . phd f : and to have an idea of how much we can gain . professor a : right . i mean in experiments that we did a long time ago and different ta it was probably resource management or something , um , i think you were getting something like still eight or nine percent error on the voicing , as i recall . and um , so um grad e : another person 's voice . professor a : what that said is that , sort of , left to its own devices , like without the a strong language model and so forth , that you would you would make significant number of errors just with your uh probabilistic machinery in deciding phd b : it also professor a : one oh phd b : yeah , the though i think uh there was one problem with that in that , you know , we used canonical mapping so our truth may not have really been true to the acoustics . professor a : uh - huh . yeah . well back twenty years ago when i did this voiced - unvoiced stuff , we were getting more like ninety - seven or ninety - eight percent correct in voicing . but that was speaker - dependent actually . we were doing training on a particular announcer phd f : mm - hmm . professor a : and and getting a very good handle on the features . phd f : mm - hmm . professor a : and we did this complex feature selection thing where we looked at all the different possible features one could have for voicing and and and uh and exhaustively searched all size subsets and and uh for for that particular speaker and you 'd find you know the five or six features which really did well on them . phd f : mm - hmm . professor a : and then doing doing all of that we could get down to two or three percent error . but that , again , was speaker - dependent with lots of feature selection phd f : mm - hmm . professor a : and a very complex sort of thing . so i would i would believe that uh it was quite likely that um looking at envelope only , that we 'd be significantly worse than that . phd f : mm - hmm . and the all the the speechcorders ? what 's the idea behind ? cuz they they have to oh , they do n't even have to detect voiced spe speech ? professor a : the modern ones do n't do a a simple switch . phd f : they just work on the code book professor a : they work on the code book excitation . phd f : and find out the best excitation . professor a : yeah they do analysis - by - synthesis . they try they they try every every possible excitation they have in their code book and find the one that matches best . phd f : yeah . mmm . alright . yeah . so it would not help . professor a : yeah . uh . o k . phd b : can i just mention one other interesting thing ? professor a : yeah . phd b : um . one of the ideas that we had come up with last week for things to try to improve the system um . actually i i s we did n't i guess i wrote this in after the meeting b but the thought i had was um looking at the language model that 's used in the htk recognizer , which is basically just a big loop , grad e : mm - hmm . phd b : right ? so you it goes `` digit `` phd d : mm - hmm . phd b : and then that can be either go to silence or go to another digit , which that model would allow for the production of infinitely long sequences of digits , right ? professor a : right . phd b : so . i thought `` well i 'm gon na just look at the what actual digit strings do occur in the training data . `` professor a : right . phd b : and the interesting thing was it turns out that there are no sequences of two - long or three - long digit strings in any of the aurora training data . so it 's either one , four , five , six , uh up to eleven , and then it skips and then there 's some at sixteen . professor a : but what about the testing data ? phd b : um . i do n't know . i did n't look at the test data yet . professor a : yeah . i mean if there 's some testing data that has has has two or three phd b : so . yeah . but i just thought that was a little odd , that there were no two or three long sorry . so i i just for the heck of it , i made a little grammar which um , you know , had it 's separate path for each length digit string you could get . so there was a one - long path and there was a four - long and a five - long professor a : mm - hmm . phd b : and i tried that and it got way worse . there were lots of deletions . professor a : mm - hmm . phd b : so it was you know , i i did n't have any weights of these paths or i did n't have anything like that . professor a : mm - hmm . phd b : and i played with tweaking the word transition penalties a bunch , but i could n't go anywhere . but um . i thought `` well if i only allow `` yeah , i guess i should have looked at to see how often there was a mistake where a two - long or a three - long path was actually put out as a hypothesis . um . but . so to do that right you 'd probably want to have allow for them all but then have weightings and things . so . i just thought that was a interesting thing about the data . professor a : ok . so we 're gon na read some more digit strings i guess ? phd b : yeah . you want to go ahead , morgan ?"
}